POSTERS

TACL paper number: 485 (Poster)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/485
Title:  A sense-topic model for WSI with unsupervised data enrichment
Authors: Jing Wang, Mohit Bansal, Kevin Gimpel, Brian D. Ziebart, Clement T. Yu
Contact email: jwang69@uic.edu
AE: Hwee Tou Ng
Abstract: Word sense induction (WSI) seeks to automatically discover the senses of a word in a corpus via unsupervised methods. We propose a sense-topic model for WSI, which treats sense and topic as two separate latent variables to be inferred jointly. Topics are informed by the entire document, while senses are informed by the local context surrounding the ambiguous word. We also discuss unsupervised ways of enriching the original corpus in order to improve model performance, including using neural word embeddings and external corpora to expand the context of each data instance. We demonstrate significant improvements over the previous state-of-the-art, achieving the best results reported to date on the SemEval-2013 WSI task.

TACL paper number: 403 (Poster)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/

Title:  SPRITE: Generalizing Topic Models with Structured Priors
AE: Janyce Wiebe
Authors: Michael J. Paul, Mark Dredze
Contact email: mpaul39@gmail.com
Abstract: We introduce SPRITE, a family of topic models that incorporates structure into model priors as a function of underlying components. The structured priors can be constrained to model topic hierarchies, factorizations, correlations, and supervision, allowing SPRITE to be tailored to particular settings. We demonstrate this flexibility by constructing a SPRITE-based model to jointly infer topic hierarchies and author perspective, which we apply to corpora of political debates and online reviews. We show that the model learns intuitive topics, outperforming several other topic models at predictive tasks.

TACL Paper number: 429 (Poster)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/429
Title: Learning Strictly Local Subsequential Functions
Authors: Jane Chandlee, Remi Eyraud, Jeffrey Heinz
Conatact email:  janemc@udel.edu
AE: Alexander Clark, 
Abstract: We define two proper subclasses of subsequential functions based on the concept of Strict Locality (McNaughton and Papert, 1971; Rogers and Pullum, 2011; Rogers et al., 2013) for formal languages. They are called Input and Output Strictly Local (ISL and OSL). We provide an automata-theoretic characterization of the ISL class and theorems establishing how the classes are related to each other and to Strictly Local languages. We give evidence that local phonological and morphological processes belong to these classes. Finally we provide a learning algorithm which provably identifies the class of ISL functions in the limit from positive data in polynomial time and data. We demonstrate this learning result on appropriately synthesized artificial corpora. We leave a similar learning result for OSL functions for future work and suggest future directions for addressing non-local phonological processes.

POSTER BY AUTHOR REQUEST
TACL paper number: 255 (Poster)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/255
Title: Dense Event Ordering with a Multi-Pass Architecture
Authors: Nathanael Chambers, Taylor Cassidy, Bill McDowell, Steven Bethard
Contact email: nchamber@usna.edu
AE: Ellen Riloff
Abstract: The past 10 years of event ordering research has focused on learning partial orderings over document events and time expressions. The most popular corpus, the TimeBank, contains a small subset of the possible ordering graph. Many evaluations follow suit by only testing certain pairs of events (e.g., only main verbs of neighboring sentences). This has led most research to focus on specific learners for partial labelings. This paper attempts to nudge the discussion from identifying some relations to all relations. We present new experiments on strongly connected event graphs that contain âˆ¼10 times more relations per document than the TimeBank. We also describe a shift away from the single learner to a sieve-based architecture that naturally blends multiple learners into a precision-ranked cascade of sieves. Each sieve adds labels to the event graph one at a time, and earlier sieves inform later ones through transitive closure. This paper thus describes innovations in both approach and task. We experiment on the densest event graphs to date and show a 14% gain over state-of-the-art.

TACL paper number: 452 (Poster)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/452
Title:  Reasoning about Quantities in Natural Language
Authors: Subhro Roy, Tim Vieira, Dan Roth
Contact email:  sroy9@illinois.edu
AE: Lillian Lee (taking over for Johan)
b/b/b-> a/a
Abstract: Little work from the Natural Language Processing community has targeted the role of quantities in Natural Language Understanding. This paper takes some key steps towards facilitating reasoning about quantities expressed in natural language. We investigate two different tasks of numerical reasoning. First, we consider Quantity Entailment, a new task formulated to understand the role of quantities in general textual inference tasks. Second, we consider the problem of automatically understanding and solving elementary school math word problems. In order to address these quantitative reasoning problems we first develop a computational approach which we show to successfully recognize and normalize textual expressions of quantities. We then use these capabilities to further develop algorithms to assist reasoning in the context
of the aforementioned tasks.

TACL paper number: 371 (Poster)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/371
Title: Unsupervised Discovery of Biographical Structure from Text
Authors: David Bamman and Noah A. Smith
Contact email:  dbamman@cs.cmu.edu
AE: Jian Su
a/a/a
Abstract: We present a method for discovering abstract event classes in biographies, based on a probabilistic latent-variable model. Taking as input timestamped text, we exploit latent correlations among events to learn a set of event classes (such as "Born", "Graduates high school", and "Becomes citizen"), along with the typical times in a person's life when those events occur. In a quantitative evaluation at the task of predicting a person's age for a given event, we find that our generative model outperforms a strong linear regression baseline, along with simpler variants of the model that ablate some features. The abstract event classes that we learn allow us to perform a large-scale analysis of 242,970 Wikipedia biographies. Though it is known that women are greatly underrepresented on Wikipedia -- not only as editors (Wikipedia, 2011) but also as subjects of articles (Reagle and Rhue, 2011) -- we find that there is a bias in their characterization as well, with biographies of women containing significantly more emphasis on events of marriage and divorce than biographies of men.

TACL paper number 385 (Poster)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/385
Title: 2-Slave Dual Decomposition for Generalized High Order CRFs
Authors: Xian Qian and Yang Liu
Contact email:  qx@hlt.utdallas.edu
AE: Kristina Toutanova
b/b, improve the writing; a/a/a
Abstract: We show that the decoding problem in generalized Higher Order Conditional Random Fields (CRFs) can be decomposed into two parts: one is a tree labeling problem that can be solved in linear time using dynamic programming; the other is a supermodular quadratic pseudo-Boolean maximization problem, which can be solved in cubic time using a minimum cut algorithm. We use dual decomposition to force their agreement. Experimental results on Twitter named entity recognition and sentence dependency tagging tasks show that our method outperforms spanning tree based dual decomposition

TACL paper number: 381 (Poster)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/381
Title: Locally Non-Linear Learning for Statistical Machine Translation via Discretization and Structured Regularization
Authors: Jonathan H. Clark, Chris Dyer, Alon Lavie
Contact author:  jon.h.clark@gmail.com
AE: Bob Moore
a/c -> a/a
Abstract: Linear models, which support efficient learning and inference, are the workhorses of statistical machine translation; however, linear decision rules are less attractive from a modeling perspective. In this work, we introduce a technique for learning arbitrary, rule-local, nonlinear feature transforms that improve model expressivity, but do not sacrifice the efficient inference and learning associated with linear models. To demonstrate the value of our technique, we discard the customary log transform of lexical probabilities and drop the phrasal translation probability in favor of raw counts. We observe that our algorithm learns a variation of a log transform that leads to better translation quality compared to the explicit log transform. We conclude that non-linear responses play an important role in SMT, an observation that we hope will inform the efforts of feature engineers.

TACL paper number: 472 (Poster)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/472
Title: Learning Constraints for Information Structure Analysis of Scientific Documents
Authors: Yufan Guo, Roi Reichart, Anna Korhonen
Contact email: yufan.guo@gmail.com
AE: Masaaki Nagata
a/a/b->a
Abstract: Inferring the information structure of scientific documents is useful for many NLP applications. Existing approaches to this task require substantial human effort. We propose a framework for constraint learning that reduces human involvement considerably. Our model uses topic models to identify latent topics and their key linguistic features in input documents, induces constraints from this information and maps sentences to their dominant information structure categories through a constrained unsupervised model. When the induced constraints are combined with a fully unsupervised model, the resulting model challenges existing lightly supervised feature-based models as well as unsupervised models that use manually constructed declarative knowledge. Our results demonstrate that useful declarative knowledge can be learned from data with very limited human involvement.

ORAL

TACL paper number: 276  (Talk)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/276
Title: A Bayesian Model of Grounded Color Semantics
Authors: Brian McMahan and Matthew Stone
Contact email:  brian.c.mcmahan@gmail.com
AE: Lillian Lee (originally Jason Eisner)
a/b/d -> a/a
Abstract: Natural language meanings allow speakers to encode important real-world distinctions, but corpora of grounded language use also reveal that speakers categorize the world in different ways and describe situations with different terminology. To learn meanings from data, we therefore need to link underlying representations of meaning to models of speaker judgment and speaker choice.  This paper describes a new approach to this problem: we model variability through uncertainty in categorization boundaries and distributions over preferred vocabulary. We apply the approach to a large data set of color descriptions, where statistical evaluation documents its accuracy. The results are available as a Lexicon of Uncertain Color Standards (LUX), which supports future efforts in grounded language understanding and generation by probabilistically mapping 829 English color descriptions to potentially context-sensitive regions in HSV color space.
Area: Language and Vision

TACL paper number: 398 (Talk)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/398
Title: Large-scale Semantic Parsing without Question-Answer Pairs
Authors: Siva Reddy, Mirella Lapata, Mark Steedman
Contact author:  siva.reddy@ed.ac.uk
AE: Noah Smith
b/b/b-> a/a/a
Abstract: In this paper we introduce a novel semantic parsing approach to query Freebase in natural language without requiring manual annotations or question-answer pairs. Our key insight is to represent natural language via semantic graphs whose topology shares many commonalities with Freebase. Given this representation, we conceptualize semantic parsing as a graph matching problem. Our model converts sentences to semantic graphs using CCG and subsequently grounds them to Freebase guided by denotations as a form of weak supervision. Evaluation experiments on a subset of the Free917 and WebQuestions benchmark datasets show our semantic parser improves over the state of the art.
Area: Semantics

TACL paper number: 384 (Talk)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/384
Title: Building a State-of-the-Art Grammatical Error Correction System
Authors: Alla Rozovskaya and Dan Roth
Contact author:  rozovska@illinois.edu
AE: Alexander Koller
b/c -> a/a/b
Abstract: This paper identifies and examines the key principles underlying building a state-of-the-art grammatical error correction system. We do this by analyzing the Illinois system that placed first among seventeen teams in the recent CoNLL-2013 shared task on grammatical error correction.
The system focuses on five different types of errors common among non-native English writers. We describe four design principles that are relevant for correcting all of these errors, analyze the system along these dimensions, and show how each of these dimensions contributes to the performance.
Area: NLP-enabled Technology

TACL paper number: 412 (Talk)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/412
Title: A Joint Model for Entity Analysis: Coreference, Typing, and Linking
Authors: Greg Durrett and Dan Klein
Contact author:  gdurrett@eecs.berkeley.edu
AE: Jason Eisner
a/a/a
Abstract: We present a joint model of three core tasks in the entity analysis stack: coreference resolution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia entities). Our model is formally a structured conditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.
Area: Discourse and Pragmatics, or Information Extraction and Question Answering


TACL paper number: 414 (Talk)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/414
Title: Predicting the Difficulty of Language Proficiency Tests
Authors: Lisa Beinborn, Torsten Zesch, Iryna Gurevych,
contact email:  beinborn@ukp.informatik.tu-darmstadt.de
AE: CCB
b/b -> a/a/b
Abstract: Language proficiency tests are used to evaluate and compare the progress  of language learners. We present an approach for automatic difficulty prediction of C-tests that performs on par with human experts. On the basis of detailed analysis of newly collected data, we develop a model for C-test difficulty introducing four dimensions: solution difficulty, candidate ambiguity, inter-gap dependency, and paragraph difficulty. We show that cues from all four dimensions contribute to  C-test difficulty. 
Area: NLP-enabled Technology, or Linguistic and Psycholinguistic Aspects of CL

TACL paper number: 488 (Talk)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/488
Title: "Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment"
Authors: Yonatan Belinkov, Tao Lei, Regina Barzilay, Amir Globerson
contact email:  belinkov@mit.edu
AE: Ryan McDonald
c/c/b -> 
Abstract: Prepositional phrase (PP) attachment disambiguation is a known challenge in syntactic parsing. The lexical sparsity associated with PP attachments motivates research in word representations that can capture pertinent syntactic and semantic features of the word. One promising solution is to use word vectors induced from large amounts of raw text. However, state-of-the-art systems that employ such representations yield modest gains in PP attachment accuracy.  In this paper, we show that word vector representations can yield significant PP attachment performance gains. This is achieved via a non-linear architecture that is discriminatively trained to maximize PP attachment accuracy. The architecture is initialized with word vectors trained from unlabeled data, and relearns those to maximize attachment accuracy. We obtain additional performance gains with alternative representations such as dependency-based word vectors. When tested on both English and Arabic datasets, our method outperforms both a strong SVM classifier and state-of-the-art parsers. For instance, we achieve 82.6% PP attachment accuracy on Arabic, while the Turbo and Charniak self-trained parsers obtain 76.7% and 80.8% respectively.
Area: Tagging and Chunking and Syntax and Parsing

TACL paper number: 555 (Talk)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/555
Title: Gappy Pattern Matching on GPUs for On-Demand Extraction of Hierarchical Translation Grammars
Authors: Hua He, Jimmy Lin, Adam Lopez
Contact email: alopez@inf.ed.ac.uk
b/b/b ->b/a/a AE: David Chiang
Abstract: Grammars for machine translation can be materialized on demand by finding source phrases in an indexed parallel corpus and extracting their translations. This approach is limited in practical applications by the computational expense of online lookup and extraction. For phrase-based models, recent work has shown that on-demand grammar extraction can be greatly accelerated by parallelization on general purpose graphics processing units (GPUs), but these algorithms do not work for hierarchical models, which require matching patterns that contain gaps. We address this limitation by presenting a novel GPU algorithm for on-demand hierarchical grammar extraction that is at least an order of magnitude faster than a comparable CPU algorithm when processing large batches of sentences. In terms of end-to-end translation, with decoding on the CPU, we increase throughput by roughly two thirds on a standard MT evaluation dataset. The GPU necessary to achieve these improvements increases the cost of a server by about a third. We believe that GPU-based extraction of hierarchical grammars is an attractive proposition, particularly for MT applications that demand high throughput.
Area: Machine Translation

Action editor: David Chiang

TACL paper number: 457 (Talk)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/457
Title: A Large Scale Evaluation of Distributional Semantic Models: Parameters, Interactions and Model Selection
Authors: Gabriella Lapesa, Stefan Evert 
Contact email:  g.lapesa@gmail.com
AE: Janyce Wiebe
b/b/c -> b/a/a
Abstract: This paper presents the results of a large-scale evaluation study of window-based Distributional Semantic Models on a wide variety of tasks. Our study combines a broad coverage of model parameters with a model selection methodology that is robust to overfitting and able to capture parameter interactions. We show that our strategy allows us to identify parameter configurations that achieve good performance across different datasets and tasks.
Area: Semantics

TACL paper number: 494 (Talk)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/494
Title:    Entity disambiguation with web links
Authors:  Andrew Chisholm, Ben Hachey
contact email:  andy.chisholm.89@gmail.com
AE: Ryan McDonald
c/a/b
Abstract: Entity disambiguation with Wikipedia relies on structured information from redirect pages, article text, inter-article links, and categories. We explore whether web links can replace a curated encyclopaedia, obtaining entity prior, name, context, and coherence models from a corpus of web pages with links to Wikipedia. Experiments compare web link models to Wikipedia models on well-known CoNLL and TAC data sets. Results show that using 34 million web links approaches Wikipedia performance. Combining web link and Wikipedia models produces the best-known disambiguation accuracy of 88.7 on standard newswire test data.
Area: Information Extraction and Question Answering

TACL paper number: 498 (Talk)
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/498
Title: Extracting Lexically Divergent Paraphrases from Twitter
Authors: Wei Xu, Alan Ritter, Chris Callison-Burch, William B. Dolan, Yangfeng Ji
Contact email:  xwe@cis.upenn.edu
AE: Sharon Goldwater
c/b/a -> c (baseline needed), 
Abstract:   We present MultiP (Multi-instance Learning Paraphrase Model), a new model suited to identify paraphrases within the short messages on Twitter. We jointly model paraphrase relations between word and sentence pairs and assume only sentence-level annotations during learning. Using this principled latent variable model alone, we achieve the performance competitive with a state-of-the-art method which combines a latent space model with a feature-based supervised classifier. Our model also captures lexically divergent paraphrases that differ from yet complement previous methods; combining our model with previous work significantly outperforms the state-of-the-art. In addition, we present a novel annotation methodology that has allowed us to crowdsource a paraphrase corpus from Twitter. We make this new dataset available to the research community.
Area: NLP for Web and Social Media and Social Sciences

Contact authors: 
|      p 255 | nchamber@usna.edu                       | 
|      o 276 | brian.c.mcmahan@gmail.com               | 
|      p 371 | dbamman@cs.cmu.edu                      | 
|      p 381 | jon.h.clark@gmail.com                   | 
|      o 384 | rozovska@illinois.edu                   | 
|      p 385 | qx@hlt.utdallas.edu                     | 
|      o 398 | siva.reddy@ed.ac.uk                     | 
|      p 403 | mpaul39@gmail.com                       | 
|      o 412 | gdurrett@eecs.berkeley.edu              |  (changed from poster to oral)
|      o 414 | beinborn@ukp.informatik.tu-darmstadt.de | 
|      p 429 | janemc@udel.edu                         | 
|      p 452 | sroy9@illinois.edu                      | 
|      o 457 | g.lapesa@gmail.com                      | 
|      p 472 | yufan.guo@gmail.com                     | 
|      p 485 | jwang69@uic.edu                         | 
|      o 488 | belinkov@mit.edu                        | 
|      o 494 | andy.chisholm.89@gmail.com              | 
|      o 498 | xwe@cis.upenn.edu                       |     (changed from poster to oral)
|      o 555 | alopez@inf.ed.ac.uk               


