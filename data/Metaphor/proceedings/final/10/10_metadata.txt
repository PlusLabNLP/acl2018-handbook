SubmissionNumber#=%=#10
FinalPaperTitle#=%=#Different Texts, Same Metaphors: Unigrams and Beyond
ShortPaperTitle#=%=#Different Texts, Same Metaphors: Unigrams and Beyond
NumberOfPages#=%=#7
CopyrightSigned#=%=#Beata Beigman Klebanov
JobTitle#==#
Organization#==#Educational Testing Service, Princeton, USA
Abstract#==#Current approaches to supervised learning of metaphor tend to use sophisticated
features and restrict their attention to constructions and contexts where these
features apply. In this paper, we describe the development of a supervised
learning system to classify all content words in a running text as either being
used metaphorically or not. We start by examining the performance of a simple
unigram baseline that achieves surprisingly good results for some of the
datasets. We then show how the recall of the system can be improved over this
strong baseline.
Author{1}{Firstname}#=%=#Beata
Author{1}{Lastname}#=%=#Beigman Klebanov
Author{1}{Email}#=%=#bbeigmanklebanov@ets.org
Author{1}{Affiliation}#=%=#ETS
Author{2}{Firstname}#=%=#Ben
Author{2}{Lastname}#=%=#Leong
Author{2}{Email}#=%=#cleong@ets.org
Author{2}{Affiliation}#=%=#Educational Testing Service
Author{3}{Firstname}#=%=#Michael
Author{3}{Lastname}#=%=#Heilman
Author{3}{Email}#=%=#mheilman@ets.org
Author{3}{Affiliation}#=%=#Educational Testing Service
Author{4}{Firstname}#=%=#Michael
Author{4}{Lastname}#=%=#Flor
Author{4}{Email}#=%=#mflor@ets.org
Author{4}{Affiliation}#=%=#Educational Testing Service

==========