%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[dvipdfmx,11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage[noend]{algpseudocode}
\usepackage[]{xcolor}
\usepackage{amsmath,amssymb}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{latexsym}
\usepackage{url}
\renewcommand{\UrlFont}{\ttfamily\small}


\abovedisplayskip=-1cm
\abovedisplayshortskip=1cm
\belowdisplayskip=10cm
\belowdisplayshortskip=0.0cm
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Meeting  the 2020 Duolingo Challenge on a Shoestring}

\author{Tadashi Nomoto\\
  National Institute of Japanese Literature \\ 10-3 Midori Tachikawa  190-0014 Japan \\
  \texttt{nomoto@acm.org} \\}
\date{}
\begin{document}
\maketitle


\begin{abstract}
What is given below is a brief description of the two systems, called gFCONV and c-VAE, which we built in a response to the 2020 Duolingo Challenge.  Both are neural models that aim at disrupting a sentence representation the encoder generates with an eye on increasing the diversity of sentences that emerge out of the process.  Importantly, we decided not to turn to external sources for extra ammunition, curious  to know how far we can go while confining ourselves to the data released by Duolingo \cite{staple20}. 
gFCONV works by  taking over a pre-trained  sequence model, intercepting the output its encoder produces on its way to the decoder. c-VAE is a conditional variational auto-encoder, seeking the diversity by blurring the representation that the encoder derives. Experiments on a corpus constructed out of the public dataset from Duolingo, containing some 4 million pairs of sentences,  found that gFCONV is a consistent winner over c-VAE though both suffered heavily  from a low recall. 
\end{abstract}

\section{Introduction}

A major driver for  our participating in the challenge was the curiosity to  see  whether recent approaches to sentence encoding with the  variational auto-encoder (VAE) have any relevance to the generation of diverse sentences.  \cite{bowman-etal-2016-generating} were the first to explore the use of VAE in language generation. The work demonstrated that VAE provides a continuous code space for sentences, where any randomly picked data point in the space can be decoded to yield a coherent sentence, which is significant given that  the conventional RNNs do not provide such a capability. The problem with VAE however, is  that it has no mechanism to ensure that the meaning of the source sentence is passed over to the output, which often causes  a sentence to be altered, or deformed beyond recognition. While VAE is a popular  approach people turn to  as a way to diversify sentences the model generates, no definitive answer has been found on how to control or tame what it spews out.   A typical solution is to fuse a VAE code with the output of a regular sentence encoder,  in order to encourage the decoder to output a sentence that retains some semantic features present in the source sentence \cite{gupta:2017}.  Also noteworthy is a recent work  by  \cite {guu-etal-2018-generating}, who building on an idea similar to VAE, talk about modeling the distribution of cosine similarities between word vectors for the  input and target.   \cite{DBLP:journals/corr/LiGBGD15} is something of an odd ball in the pursuit of the diversity in sentence generation. The authors argued that we could achieve the diversity by discouraging the decoder to select candidates that are similar to the input.  A clear advantage they have over others is that their scheme does not involve any learning and is straightforward to implement.  

\par The idea that one can view a latent representation as a sample drawn from some probabilistic distribution inspired people to explore its potential in a wide range of  tasks and domains. \cite{DBLP:journals/corr/MiaoYB15}, while working on document modeling, suggested that we  use VAE as a way to get a compact representation for a document.
\cite{Fang2019ImplicitDL}  argued for using a sample based distribution over  Gaussian distribution for a latent code to better express the holistic property of the source sentence. 
\par In this work, we focus on  two approaches, both based on VAE: one that attempts to achieve the diversity by   generalizing the sentence representation produced by the encoder; and another which randomly perturbs the encoder's output during the sentence generation.  We report  here  their respective performance on a test corpus we carved out of the official training data.   For the final submission, we went along  with the latter approach.
\section{Translation as Paraphrase}
 \begin{figure}
\includegraphics[height=3.4cm, trim={17.5cm 14cm 0cm 12cm},clip]{meeting_challenge.pdf}
\caption{Translation as Paraphrase \label{fig:model}}
\end{figure}
Our effort revolved around  two questions: (1)  how best to incorporate likelihood scores of target translations that were provided as part of the training data, and (2) how {\em not} to rely on an external resource while building a solution. We wanted to know how far we can go using only the data made available to us at the competition, and nothing more. Our answer to the first question takes advantage of the fact that a set of translations associated with each  English prompt  are considered an equivalence class in the sense that if we take any pair from the set, we can substitute one for the other without significantly affecting its  meaning. We may take the likelihood that a human picks a particular sentence (call it X)  as a good translation for some prompt (P) as the probability of its being a paraphrase of some other sentence (say Y) from a group of possible translations of which X is part. The intuition here is that  if X is more typical as a translation of P,  it is  more likely to serve as a paraphrase of whatever other way we may have to express  P in the target language. Following this idea, we created training data by randomly sampling a pair of sentences (both in the same language) that appear as alternate translations for a given prompt in accordance with their popular rating.  For each prompt, we sampled 2,000 pairs of translations (which may include pairs consisting of identical sentences), resulting in  4,601,000 training instances (which amount to 2,300 prompts plus those provided in the development and test set) \cite{staple20}.\footnote{ For this year's challenge, we worked only on the  English-Japanese track. We included both test and development sets as part of training data, as a way to prevent the algorithm from stumbling upon unknown tokens in the test set.  We don't see this as much of a problem because each prompt in development and test sets carries no more than one translation, i.e.  a training pair we get  from the development and test set  has a same sentence for both source and target. We made use of MeCab for tokenizing sentences in Japanese.} 
\begin{figure}
\includegraphics[height=5.7cm, trim={19cm 8cm 20cm 10cm},clip]{arch.pdf}
\caption{gFCONV\label{fig:arch}}
\end{figure}
We set aside 100 prompts as a private development set and another 100 for testing.  We included in each training instance an English prompt as well as its translation in order to prevent paraphrases the algorithm generates  from diverging from the meaning of  the prompt (Table~\ref{tbl:train}).
\begin{table*}[t]

\caption{Training Instances. The source part of each input  consists of two sections: the first section contains a sentence in an original language, followed by a translation in a target language, demarcated by a separator `@@@@.`  \label{tbl:train}}
\begin{tabular}{|c|p{.8\textwidth}|} \hline
\multirow{5}{*}{\sc source}&  
that apple is very big . @@@@ その 林檎 は 非常 に でかい です 。\\
& i like to work . @@@@ ボク は 仕事 は 好き です 。\\
&he drinks milk . @@@@ 彼 は 牛乳 を 飲み ます 。\\
%& we made pasta with fish last week . @@@@ 僕達 は 先週 魚 を 使っ て パスタ を 作っ た 。\\
& what are her strengths ? @@@@ 彼女 の 長所 は なん でしょ う ？\\
& what has she done ? @@@@ 彼女 は 何 を やり 終わっ た ん です か ？ \\ \hline\hline
%who is going to believe me ? @@@@ 誰 が 私 を 信じる つもり です か ？ \\
%& he did not answer me . @@@@ 彼 は 私 に 答え なかっ た 。  \\
%& it is a sad chapter . @@@@ それ は 悲しい 章 です 。  \\
%& the paint is orange . @@@@ 塗料 は オレンジ 色 です 。  \\
%&you are not a victim . @@@@ あなた は 被害 者 で は あり ませ ん 。  \\ \hline\hline
\multirow{5}{*}{\sc target }&その りんご が とても でかい 。\\
&私 は 働く の が 好き 。\\
& かれ は ぎゅうにゅうを 飲み ます 。\\
%&先週 僕 たち は 魚 を 使っ た パスタ を 作っ た 。\\
&何 が 彼女 の 強い 所 な の ？\\
&何 を 彼女 は 終わっ た の ？\\ \hline


%\multirow{5}{*}{\sc target }&誰 が 私 を 信じる つもり です か ？   \\
%&彼 は 私 に 答え なかっ た 。   \\
%&それ は 悲しい 章 です 。   \\
%&塗料 は オレンジ 色 です 。   \\
%&あなた は 被害 者 で は あり ませ ん 。   \\ \hline
\end{tabular}
\end{table*}
\definecolor{darkmidnightblue}{rgb}{0.0, 0.2, 0.4}
%\definecolor{blue}{rgb}{0.2, 0.2, 0.6}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\definecolor{mypink1}{rgb}{0.858, 0.188, 0.478}

\par Figure~\ref{fig:model} shows a schematic picture of how our approach works. We feed into the system a prompt and its translation which we assume to be given (via AWS, for example). Out comes its paraphrases (or translations in varied styles). The model we built is essentially one based on  Fairseq's convolution to sequence architecture  of the type called `fconv\_iwslt\_de\_en' (call it FCONV) which features 4 convolutional layers for the encoder and 3 for the decoder.\footnote{\url{https://github.com/pytorch/fairseq}}   
The embedding dimension for the input and output token was set to 256.   We did not use pre-trained embeddings for either of the languages we dealt with. Neither did we make any architectural change to FCONV.  We simply trained it as it was given.  A departure comes in the testing phase.  Following \cite {guu-etal-2018-generating}, we applied a Gaussian noise on the output of the encoder as it was sent to the decoder (Fig.~\ref{fig:arch}). 
\begin{equation}\label{eqn:gfconv}
u = E(x) + \epsilon,\;\;  \epsilon \sim {\cal N} (0, k)
\end{equation}
where $x$ is an input and $E(x)$ is an output from applying an encoder $E$ on $x$.  $u$ denotes an input to a decoder. 
 A larger noise means a greater disruption in the latent representation coming from the encoder, which we hoped would   lead to an increase in the diversity of sentences being generated.  We randomly sampled a noise from a normal distribution with the mean set to 0 and the variance ranging from 0 to 0.6. In what follows, we refer to the scheme as gFCONV.
%
\par We also looked at a conditional variational auto-encoder (c-VAE), a close cousin of gFCONV for the sake of comparison.  While both aim at building a latent representation that embraces the notion of uncertainty,  c-VAE 
differs from the variance based approach in that it seeks to find a probabilistic distribution that defines a range of representations that the encoder churns out. In terms of formulae, this comes to the following (also see Fig.~\ref{fig:svae}
for a visual intuition). 
 \begin{figure}
\includegraphics[height=3.2cm, trim={18.5cm 18cm 17cm 8cm},clip]{svae.pdf}
\caption{Conditional  VAE  \label{fig:svae}}
\end{figure}
\begin{equation} \label{eqn:cvae}
u = E(x)  + r * z,
\end{equation}
Here $z=\mu + \epsilon * \upsilon \text{ with } \epsilon \sim \text{Unif}[0,1)$. $\mu$ and $\upsilon$ are a mean and variance,  defined as $\mu = g(x)$, and $\upsilon = f(x)$, respectively. $x$ is  an input, $g$ and $f$ are some arbitrary  functions over $x$. $E(x)$ again denotes the output of an encoder. $\mu$ and $\upsilon$  are learnable parameters, which means that they need to be trained to have them work. It is worth noting that  gFCONV has no extra `learnable' parameters.  $r$ is a hyper-parameter to be set manually,  which determines the degree of contribution of $z$ to  a latent representation of $x$. We combine $E(x)$  and a representation sampled from a Gaussian distribution to build a final encoder output.   Our decision to condition VAE on $E(x)$ is motivated by a frequent observation in the past literature that VAE is poor at preserving the meaning of the source sentence, often transforming it beyond recognition. Conditioning VAE on the input is a popular trick to discourage the algorithm from  straying too far away from the source.
\begin{table*}[h]
\caption{Variance vs. Performance. `P' denotes precision,  `R' recall, and '$k$' variance.  Numbers in red represent the baseline and those in blue the best performing system where we have a minimum divergence between  Micro and Macro F1. \label{tbl:variance}}
\center\begin{tabular}{|c|c|ccc|ccc|}  \cline{3-8}
\multicolumn{1}{c}{}&&\multicolumn{3}{|c}{\sc unweighted} & \multicolumn{3}{|c|}{\sc weighted}\\ \cline{1-8}
$k$ &P&R&Micro F1&Macro F1&R&Micro F1&Macro F1\\ \hline
\red{0.00}& 39.16& \textcolor{red}{3.83}& \textcolor{red}{6.97}& \textcolor{red}{11.37}& 13.62& 20.21& 16.35\\
0.05& 33.24& 5.09& 8.83& 12.77& 15.60& 21.23& 17.13\\
0.10& 28.10& 6.18& 10.14& 13.46& 17.47& 21.54& 17.38\\
0.15& 23.41& 7.45& 11.30& 13.66& 19.34& 21.18& 16.92\\
0.20& 20.08& 8.62& 12.06& 13.73& 21.10& 20.58& 16.48\\
\blue{0.25}& 16.93& \blue{9.57}& \blue{12.22}& \blue{13.18}& 22.37& 19.27& 15.40\\
0.30& 14.17& 10.24& 11.89& 12.27& 23.36& 17.64& 14.13\\
0.35& 12.02& 10.69& 11.31& 11.22& 24.16& 16.05& 12.74\\
0.40& 10.92& 11.49& 11.20& 10.70& 24.70& 15.14& 12.03\\
0.45& 9.15& 11.23& 10.08& 9.67& 24.38& 13.30& 10.73\\
0.50& 8.21& 10.81& 9.33& 8.72& 23.24& 12.14& 9.72\\
0.55& 7.24& 9.99& 8.40& 7.90& 22.84& 11.00& 8.78\\
0.60& 6.92& 9.30& 7.94& 7.54& 22.25& 10.56& 8.38\\ \cline{1-8}
\end{tabular}
\end{table*}
%
\begin{table*}
\caption{Conditional VAE \label{tbl:vae}}
\center\begin{tabular}{|c|c|ccc|ccc|}  \cline{3-8}
\multicolumn{1}{c}{}&&\multicolumn{3}{|c}{\sc unweighted} & \multicolumn{3}{|c|}{\sc weighted}\\ \cline{1-8}
$r$ &P&R&Micro F1&Macro F1&R&Micro F1&Macro F1\\ \hline
%Scale&P&R&Micro F1&Macro F1&Weighted Recall&Weighted Micro F1&Weighted Macro F1\\
\red{0.0}& \red{39.16}& \textcolor{red}{3.83}& \textcolor{red}{6.97}& \textcolor{red}{11.37}& {13.62}& {20.21}& {16.35}\\ 
0.1& 24.06& 7.11& 10.97& 14.84& 19.41& 21.49& 18.08\\
0.2& 22.55& 7.98& 11.78& 14.81& 20.16& 21.29& 17.87\\
0.3& 18.28& 7.72& 10.86& 14.28& 20.09& 19.15& 17.10\\
0.4& 17.09& 7.58& 10.50& 13.12& 20.21& 18.52& 15.69\\
\blue{0.5}& 15.68& \blue{7.70}& \blue{10.33}& \blue{12.73}& 20.08& 17.61& 15.31\\ \hline
\end{tabular} 
\end{table*}
%
\par Implementation-wise, c-VAE was based on FCONV, from which we also built gFCONV. We kept all the hyper-parameter settings intact, e.g. the number of layers, the size and the number of filters, etc.  We did not apply any scheduled annealing weight to the KL term in the loss function. 
%

For gFCONV, we varied the variance parameter $k$ (Eqn.~\ref{eqn:gfconv}) from 0.00 to 0.60 in increments of 0.05. For each value of  $k$, we  ran gFCONV on the test set 100 times, letting  the model output 80 alternative translations for each prompt (Setting $k$ to 0 reduces gFCONV to a vanilla FCONV).  This  had resulted in  a pool of 8,000 candidates for a given prompt under  a particular value of $k$. 
Out of which we retained only those that had a non zero similarity to gold translations by AWS.\footnote{i.e. those found in the `test.en\_ja.aws\_baseline.pred.txt' in the `staple-2020-test-blind' directory.}  We measured the similarity using LASER,\footnote{\url{https://github.com/facebookresearch/LASER}} along with  pre-trained word embeddings  from FastText,\footnote{\url{ https://fasttext.cc/docs/en/crawl-vectors.html}} which LASER requires. We were interested to know   how variance  affected the  performance, in particular how it contributed to improving the diversity. 
 \section{Results and Discussion \label{sec:results}}
%
\par Results are provided in Table~\ref{tbl:variance}.  The numbers shown were produced  using the official scorer.  In the following discussion, we concentrate on unweighted scores as our interest here is in knowing how much we improved the raw recall under the current setup. Note that weighted scores do not shed light on the true diversity of sentences we have garnered.
\par Looking at Table~\ref{tbl:variance}, we see  gFCONV gaining on  a vanilla FCONV, whose performance is represented by  the numbers at $k=0.00$.    At $k=0.25$, we see the raw recall jumping from 3.83 to 9.57, Micro F1 from 6.97 to 12.22, and Macro F1 from 11.37 to 13.18. Compare the difference between Micro and Macro F1 at $k=0.00$ and that we have at $k=0.25$. The difference for the latter is much smaller. This suggests that under  gFCONV, the performance is more stable across  test items compared to the vanilla FCONV.  A large divergence at $k=0.00$ indicates wild ups and downs  in performance, suggesting that the model is doing beautifully well on some but failing miserably on others. In contrast to Micro F1, Macro F1 is blind to how many candidate translations  there are for each prompt, so may not give us an accurate picture of how the model is doing on each prompt.
%
\par  As with gFCONV,  we ran c-VAE on the test set 100 times, obtaining 100 distinct pools of candidate translations for each prompt.\footnote{
We generated 8,000 hypotheses for  each prompt under a particular value of  $r$, 80 at each round. 
} We report  in Table~\ref{tbl:vae}, figures that represent performance on all the results combined in the manner we described for gFCONV.  %\footnote{See the first paragraph of Section~\ref{sec:results}.}  
We varied $r$ (in Eqn.~\ref{eqn:cvae}) from 0.1 to 0.5 in 0.1 increments. We observe that c-VAE is somewhat behind gFCONV (in terms of divergence between Micro and Macro F1), though performing well over the baseline (numbers in red). A large gap between (unweighted) Micro and Macro F1 again shows that the model suffers from a fluctuating  performance, swinging wildly from one test item to another.  
The final submission for the official evaluation was prepared using gFCONV at $k=0.10$, under the pseudonym `darkside,' with the official results shown in Table~\ref{tbl:results}.\footnote{We did not submit the version at 0.25 which turned out to be the best, due to  its late discovery, which came well past the deadline.  }
\begin{table}
\caption{Official Results. By W. Recall and W. F1,  we mean Weighted Recall and F1. \label{tbl:results}}
\begin{tabular}{||ccccc||} \hline
Phase &Rank & Precision & W. Recall & W. F1 \\ \hline
DEV & 6/6 & 	0.369 & 0.183 & 0.181 \\
TEST & 6/6 & 0.349& 	0.212& 	0.194\\ \hline
\end{tabular}
\end{table}


\section{Conclusions}
We discussed two approaches as a way to tackle the Duolingo  Challenge. One is  gFCONV, which  takes over a pre-trained  sequence model, intercepts and perturbs the output its encoder produces on its way to the decoder.  Another is c-VAE,  a conditional variational auto-encoder, which seeks the diversity by blurring the representation that the encoder derives.
%which works by injecting  random perturbations into the encoder output, sampled from the Gaussian distribution, and another which attempts to learn a probabilistic representation of a source sentence through VAE. 
Either approach, it was found,  outperformed the vanilla FCONV.  We also noted a large discrepancy between Micro and Macro F1, suggesting that the models'  performance is not even and fluctuates wildly from item to item.  Moreover, there were some test prompts for which the models were not able to find any translations. We recognize that this is an  area we need to scrutinize to further improve the performance. In the long run, it would be interesting to see if we can  bring to the task  recent developments  in VAE such as  \cite{bouchacourt2018multi-level}.


\bibliographystyle{acl_natbib}
\bibliography{ling1-u}

\end{document}
