SubmissionNumber#=%=#9
FinalPaperTitle#=%=#Increasing Lexical Diversity in Plug and Play Language Models
ShortPaperTitle#=%=#
NumberOfPages#=%=#
CopyrightSigned#=%=#
JobTitle#==#
Organization#==#
Abstract#==#Transformer-based models have made great progress in open-domain natural language generation. These models are commonly fine-tuned for controlled text generation, e.g., for generating domain-specific text. Dathathri et al. (2019) recently proposed the Plug and Play Language Model (PPLM) which allows standard transformer-based models to perform controlled text generation with the help of simple topic bag-of-words classifier.
However, it turns out that text generated in this fashion does not use all of the words in the bag-of-words with equal frequency. Some words from the topic are strongly over-represented in the generated text, while others rarely or never get generated. In this work, we analyze the lexical diversity of texts generated by PPLM for four different topic models and show that the generated topic word frequencies are disproportionately skewed toward common words. We then propose and compare three different methods for encouraging PPLM to generate a greater diversity of topic words. We show that these approaches are effective in alleviating the imbalance issue. This is a submission to the extended abstract track.
Author{1}{Firstname}#=%=#Soham
Author{1}{Lastname}#=%=#Parikh
Author{1}{Username}#=%=#prawncream
Author{1}{Email}#=%=#sohamp@seas.upenn.edu
Author{1}{Affiliation}#=%=#University of Pennsylvania
Author{2}{Firstname}#=%=#Daphne
Author{2}{Lastname}#=%=#Ippolito
Author{2}{Username}#=%=#daphnei
Author{2}{Email}#=%=#daphnei@seas.upenn.edu
Author{2}{Affiliation}#=%=#University of Pennsylvania
Author{3}{Firstname}#=%=#Satyarth
Author{3}{Lastname}#=%=#Vaidya
Author{3}{Username}#=%=#satyarthv
Author{3}{Email}#=%=#satyarth@seas.upenn.edu
Author{3}{Affiliation}#=%=#University of Pennsylvania

==========