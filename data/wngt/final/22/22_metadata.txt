SubmissionNumber#=%=#22
FinalPaperTitle#=%=#Transformers without Tears: Improving the Normalization of Self-Attention
ShortPaperTitle#=%=#
NumberOfPages#=%=#
CopyrightSigned#=%=#Julian Salazar
JobTitle#==#
Organization#==#Amazon AWS AI, USA
Abstract#==#We evaluate three simple, normalization-centric changes to improve Transformer training. First, we show that pre-norm residual connections (PreNorm) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose L2 normalization with a single scale parameter (ScaleNorm) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FixNorm). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT '15 EnglishVietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (WMT '14 English-German), ScaleNorm and FixNorm remain competitive but PreNorm degrades performance. Preprocessing scripts and code are released at https://github.com/tnq177/transformers_without_tears.

This is a submission to the cross-submission track, presented at IWSLT 2019.
Author{1}{Firstname}#=%=#Toan Q.
Author{1}{Lastname}#=%=#Nguyen
Author{1}{Username}#=%=#tnguye28
Author{1}{Email}#=%=#tnguye28@nd.edu
Author{1}{Affiliation}#=%=#University of Notre Dame
Author{2}{Firstname}#=%=#Julian
Author{2}{Lastname}#=%=#Salazar
Author{2}{Username}#=%=#julianslzr
Author{2}{Email}#=%=#accounts+softconf@julianslzr.com
Author{2}{Affiliation}#=%=#Amazon AWS AI

==========