SubmissionNumber#=%=#46
FinalPaperTitle#=%=#{T}ext{B}rewer: {A}n {O}pen-{S}ource {K}nowledge {D}istillation {T}oolkit for {N}atural {L}anguage {P}rocessing
ShortPaperTitle#=%=#
NumberOfPages#=%=#8
CopyrightSigned#=%=#Ziqing Yang
JobTitle#==#
Organization#==#iFLYTEK Research, Beijing, China
Abstract#==#In this paper, we introduce TextBrewer, an open-source knowledge distillation toolkit designed for natural language processing. It works with different neural network models and supports various kinds of supervised learning tasks, such as text classification, reading comprehension, sequence labeling. TextBrewer provides a simple and uniform workflow that enables quick setting up of distillation experiments with highly flexible configurations. It offers a set of predefined distillation methods and can be extended with custom code. As a case study, we use TextBrewer to distill BERT on several typical NLP tasks. With simple configurations, we achieve results that are comparable with or even higher than the public distilled BERT models with similar numbers of parameters.
Author{1}{Firstname}#=%=#Ziqing
Author{1}{Lastname}#=%=#Yang
Author{1}{Username}#=%=#yangziqing
Author{1}{Email}#=%=#ziqingyang@gmail.com
Author{1}{Affiliation}#=%=#iFLYTEK Research
Author{2}{Firstname}#=%=#Yiming
Author{2}{Lastname}#=%=#Cui
Author{2}{Username}#=%=#ymcui
Author{2}{Email}#=%=#conandiy@gmail.com
Author{2}{Affiliation}#=%=#Harbin Institute of Technology
Author{3}{Firstname}#=%=#Zhipeng
Author{3}{Lastname}#=%=#Chen
Author{3}{Username}#=%=#zpchen
Author{3}{Email}#=%=#zpchen@iflytek.com
Author{3}{Affiliation}#=%=#iFLYTEK Research
Author{4}{Firstname}#=%=#Wanxiang
Author{4}{Lastname}#=%=#Che
Author{4}{Username}#=%=#wanxiang
Author{4}{Email}#=%=#wanxiang@gmail.com
Author{4}{Affiliation}#=%=#Harbin Institute of Technology
Author{5}{Firstname}#=%=#Ting
Author{5}{Lastname}#=%=#Liu
Author{5}{Username}#=%=#tliu72
Author{5}{Email}#=%=#tliu72@qq.com
Author{5}{Affiliation}#=%=#Harbin Institute of Technology
Author{6}{Firstname}#=%=#Shijin
Author{6}{Lastname}#=%=#Wang
Author{6}{Username}#=%=#sjwang3
Author{6}{Email}#=%=#sjwang3@iflytek.com
Author{6}{Affiliation}#=%=#iFLYTEK Research
Author{7}{Firstname}#=%=#Guoping
Author{7}{Lastname}#=%=#Hu
Author{7}{Username}#=%=#gphu
Author{7}{Email}#=%=#gphu@iflytek.com
Author{7}{Affiliation}#=%=#iFlyTEK Research

==========