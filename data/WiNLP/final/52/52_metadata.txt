SubmissionNumber#=%=#52
FinalPaperTitle#=%=#Understanding the Impact of Experiment Design for Evaluating Dialogue System Output
ShortPaperTitle#=%=#
NumberOfPages#=%=#1
CopyrightSigned#=%=#
JobTitle#==#
Organization#==#
Abstract#==#Evaluation of output from natural language generation (NLG) systems is typically conducted via crowdsourced human judgments. To understand the impact how experiment design might affect the quality and consistency of such human judgments, we designed a between-subjects study with four experiment conditions. Through our systematic study with 40 crowdsourced workers in each task, we find that using continuous scales achieves more consistent ratings than Likert scale or ranking-based experiment design. Additionally, we find that factors such as no prior experience of participating in similar studies of rating dialogue system output
Author{1}{Firstname}#=%=#Sashank
Author{1}{Lastname}#=%=#Santhanam
Author{1}{Username}#=%=#sashank06
Author{1}{Email}#=%=#ssantha1@uncc.edu
Author{1}{Affiliation}#=%=#University of North Carolina at Charlotte
Author{2}{Firstname}#=%=#Samira
Author{2}{Lastname}#=%=#Shaikh
Author{2}{Username}#=%=#samirashaikh
Author{2}{Email}#=%=#sshaikh2@uncc.edu
Author{2}{Affiliation}#=%=#UNC - Charlotte

==========