SubmissionNumber#=%=#2
FinalPaperTitle#=%=#Improving Autoregressive NMT with Non-Autoregressive Model
ShortPaperTitle#=%=#
NumberOfPages#=%=#6
CopyrightSigned#=%=#Long Zhou
JobTitle#==#
Organization#==#No. 95, Zhong Guan Cun East Road, Beijing 100190, China
Abstract#==#Autoregressive neural machine translation (NMT) models are often used to teach non-autoregressive models via knowledge distillation. However, there are few studies on improving the quality of autoregressive translation (AT) using non-autoregressive translation (NAT). In this work, we propose a novel Encoder-NAD-AD framework for NMT, aiming at boosting AT with global information produced by NAT model. Specifically, under the semantic guidance of source-side context captured by the encoder, the non-autoregressive decoder (NAD) first learns to generate target-side hidden state sequence in parallel. Then the autoregressive decoder (AD) performs translation from left to right, conditioned on source-side and target-side hidden states.  Since AD has global information generated by low-latency NAD, it is more likely to produce a better translation with less time delay. Experiments on WMT14 En-De, WMT16 En-Ro, and IWSLT14 De-En translation tasks demonstrate that our framework achieves significant improvements with only 8\% speed degeneration over the autoregressive NMT.
Author{1}{Firstname}#=%=#Long
Author{1}{Lastname}#=%=#Zhou
Author{1}{Username}#=%=#long.zhou
Author{1}{Email}#=%=#long.zhou@nlpr.ia.ac.cn
Author{1}{Affiliation}#=%=#Institute of Automation, Chinese Academy of Sciences
Author{2}{Firstname}#=%=#Jiajun
Author{2}{Lastname}#=%=#Zhang
Author{2}{Username}#=%=#jjzhang
Author{2}{Email}#=%=#jiajun.zhang@nlpr.ia.ac.cn
Author{2}{Affiliation}#=%=#Institute of Automation Chinese Academy of Sciences
Author{3}{Firstname}#=%=#Chengqing
Author{3}{Lastname}#=%=#Zong
Author{3}{Username}#=%=#zong
Author{3}{Email}#=%=#cqzong@nlpr.ia.ac.cn
Author{3}{Affiliation}#=%=#Institute of Automation, Chinese Academy of Sciences

==========