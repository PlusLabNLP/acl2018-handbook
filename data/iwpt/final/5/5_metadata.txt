SubmissionNumber#=%=#5
FinalPaperTitle#=%=#Distilling Neural Networks for Greener and Faster Dependency Parsing
ShortPaperTitle#=%=#
NumberOfPages#=%=#12
CopyrightSigned#=%=#Mark Anderson
JobTitle#==#
Organization#==#Universidade da Coruña, Campus Elviña, s/n, 15071 A Coruña, España
Abstract#==#The carbon footprint of natural language processing research has been increasing in recent years due to its reliance on large and inefficient neural network implementations. Distillation is a network compression technique which attempts to impart knowledge from a large model to a smaller one. We use teacher-student distillation to improve the efficiency of the Biaffine dependency parser which obtains state-of-the-art performance with respect to accuracy and parsing speed (Dozat and Manning, 2017). When distilling to 20% of the original model's trainable parameters, we only observe an average decrease of ∼1 point for both UAS and LAS across a number of diverse Universal Dependency treebanks while being 2.30x (1.19x) faster than the baseline model on CPU (GPU) at inference time. We also observe a small increase in performance when compressing to 80% for some treebanks. Finally, through distillation we attain a parser which is not only faster but also more accurate than the fastest modern parser on the Penn Treebank.
Author{1}{Firstname}#=%=#Mark
Author{1}{Lastname}#=%=#Anderson
Author{1}{Username}#=%=#markda
Author{1}{Email}#=%=#mark.anderson.nlp@gmail.com
Author{1}{Affiliation}#=%=#Universidade da Coruña
Author{2}{Firstname}#=%=#Carlos
Author{2}{Lastname}#=%=#Gómez-Rodríguez
Author{2}{Username}#=%=#cgomezr
Author{2}{Email}#=%=#cgomezr@udc.es
Author{2}{Affiliation}#=%=#Universidade da Coruña

==========