SubmissionNumber#=%=#8
FinalPaperTitle#=%=#Randomized Significance Tests in Machine Translation
ShortPaperTitle#=%=#Randomized Significance Tests in Machine Translation
NumberOfPages#=%=#9
CopyrightSigned#=%=#Yvette Graham
JobTitle#==#
Organization#==#Department of Computing and Information Systems,
University of Melbourne,
Parkville 3010,
Victoria,
Australia
Abstract#==#Randomized methods of significance testing enable estimation of the probability
that an increase in score has occurred simply by chance. In this paper, we
examine the accuracy of three randomized methods of significance testing in the
context of machine translation: paired bootstrap resampling, bootstrap
resampling and approximate randomization. We carry out a large-scale human
evaluation of shared task systems for two language pairs to provide a gold
standard for tests. Results show very little difference in accuracy across the
three methods of significance testing. Notably, accuracy of all test/metric
combinations for evaluation of English-to-Spanish are so low that there is
not enough evidence to conclude they are any better than a random baseline.
Author{1}{Firstname}#=%=#Yvette
Author{1}{Lastname}#=%=#Graham
Author{1}{Email}#=%=#yvette.graham@unimelb.edu.au
Author{1}{Affiliation}#=%=#The University of Melbourne
Author{2}{Firstname}#=%=#Nitika
Author{2}{Lastname}#=%=#Mathur
Author{2}{Email}#=%=#nmathur@student.unimelb.edu.au
Author{2}{Affiliation}#=%=#The University of Melbourne
Author{3}{Firstname}#=%=#Timothy
Author{3}{Lastname}#=%=#Baldwin
Author{3}{Email}#=%=#tb@ldwin.net
Author{3}{Affiliation}#=%=#The University of Melbourne

==========