SubmissionNumber#=%=#66
FinalPaperTitle#=%=#Linear Mixture Models for Robust Machine Translation
ShortPaperTitle#=%=#Linear Mixture Models for Robust Machine Translation
NumberOfPages#=%=#11
CopyrightSigned#=%=#NA
JobTitle#==#
Organization#==#
Abstract#==#As larger and more diverse parallel texts become available, how can we leverage
heterogeneous data to train robust machine translation systems that achieve
good translation quality on various test domains? This challenge has been
addressed so far by re-purposing techniques developed for domain adaptation,
such as linear mixture models which combine estimates learned on homogeneous
sub-domains. However, learning from large heterogeneous corpora is quite
different from standard adaptation tasks with clear domain distinctions.  In
this paper, we show that linear mixture models can reliably improve translation
quality in very heterogeneous training conditions, even if the mixtures do not
use any domain knowledge and attempt to learn generic models rather than adapt
them to the target domain. This surprising finding opens new perspectives for
using mixture models in machine translation beyond clear cut domain adaptation
tasks.
Author{1}{Firstname}#=%=#Marine
Author{1}{Lastname}#=%=#Carpuat
Author{1}{Email}#=%=#marine.carpuat@cnrc-nrc.gc.ca
Author{1}{Affiliation}#=%=#National Research Council
Author{2}{Firstname}#=%=#Cyril
Author{2}{Lastname}#=%=#Goutte
Author{2}{Email}#=%=#cyril.goutte@gmail.com
Author{2}{Affiliation}#=%=#National Research Council Canada
Author{3}{Firstname}#=%=#George
Author{3}{Lastname}#=%=#Foster
Author{3}{Email}#=%=#george.foster@cnrc-nrc.gc.ca
Author{3}{Affiliation}#=%=#NRC

==========