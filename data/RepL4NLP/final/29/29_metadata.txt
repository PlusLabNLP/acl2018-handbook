SubmissionNumber#=%=#29
FinalPaperTitle#=%=#Exploring the Limits of Simple Learners in Knowledge Distillation for Document Classification with DocBERT
ShortPaperTitle#=%=#
NumberOfPages#=%=#6
CopyrightSigned#=%=#Ashutosh Adhikari
JobTitle#==#
Organization#==#University of Waterloo
Abstract#==#Fine-tuned variants of BERT are able to achieve state-of-the-art accuracy on many natural language processing tasks, although at significant computational costs.
In this paper, we verify BERT's effectiveness for document classification and investigate the extent to which BERT-level effectiveness can be obtained by different baselines, combined with knowledge distillation---a popular model compression method.
The results show that BERT-level effectiveness can be achieved by a single-layer LSTM with at least $40\times$ fewer FLOPS and only ${\sim}3\%$ parameters.
More importantly, this study analyzes the limits of knowledge distillation as we distill BERT's knowledge all the way down to linear models---a relevant baseline for the task.
We report substantial improvement in effectiveness for even the simplest models, as they capture the knowledge learnt by BERT.
Author{1}{Firstname}#=%=#Ashutosh
Author{1}{Lastname}#=%=#Adhikari
Author{1}{Username}#=%=#ashutosh1996
Author{1}{Email}#=%=#adadhika@uwaterloo.ca
Author{1}{Affiliation}#=%=#University of Waterloo
Author{2}{Firstname}#=%=#Achyudh
Author{2}{Lastname}#=%=#Ram
Author{2}{Username}#=%=#achyudh
Author{2}{Email}#=%=#arkeshav@uwaterloo.ca
Author{2}{Affiliation}#=%=#Student
Author{3}{Firstname}#=%=#Raphael
Author{3}{Lastname}#=%=#Tang
Author{3}{Username}#=%=#rtang123
Author{3}{Email}#=%=#r33tang@uwaterloo.ca
Author{3}{Affiliation}#=%=#University of Waterloo
Author{4}{Firstname}#=%=#William L.
Author{4}{Lastname}#=%=#Hamilton
Author{4}{Username}#=%=#williamleif
Author{4}{Email}#=%=#wlh@cs.mcgill.ca
Author{4}{Affiliation}#=%=#McGill University
Author{5}{Firstname}#=%=#Jimmy
Author{5}{Lastname}#=%=#Lin
Author{5}{Username}#=%=#jimmylin
Author{5}{Email}#=%=#jimmylin@uwaterloo.ca
Author{5}{Affiliation}#=%=#University of Waterloo

==========