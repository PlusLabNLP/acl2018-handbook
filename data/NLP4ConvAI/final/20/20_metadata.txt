SubmissionNumber#=%=#20
FinalPaperTitle#=%=#Accelerating Natural Language Understanding in Task-Oriented Dialog
ShortPaperTitle#=%=#
NumberOfPages#=%=#8
CopyrightSigned#=%=#Ojas Ahuja
JobTitle#==#
Organization#==#UT Austin
Abstract#==#Task-oriented dialog models typically leverage complex neural architectures and large-scale, pre-trained Transformers to achieve state-of-the-art performance on popular natural language understanding benchmarks. However, these models frequently have in excess of tens of millions of parameters, making them impossible to deploy on-device where resource-efficiency is a major concern. In this work, we show that a simple convolutional model compressed with structured pruning achieves largely comparable results to BERT on ATIS and Snips, with under 100K parameters. Moreover, we perform acceleration experiments on CPUs, where we observe our multi-task model predicts intents and slots nearly 63x faster than even DistilBERT.
Author{1}{Firstname}#=%=#Ojas
Author{1}{Lastname}#=%=#Ahuja
Author{1}{Username}#=%=#ahuja
Author{1}{Email}#=%=#ojas@utexas.edu
Author{1}{Affiliation}#=%=#University of Texas at Austin
Author{2}{Firstname}#=%=#Shrey
Author{2}{Lastname}#=%=#Desai
Author{2}{Username}#=%=#shreydesai
Author{2}{Email}#=%=#shreydesai@me.com
Author{2}{Affiliation}#=%=#University of Texas at Austin

==========