SubmissionNumber#=%=#24
FinalPaperTitle#=%=#CopyBERT: A Unified Approach to Question Generation with Self-Attention
ShortPaperTitle#=%=#
NumberOfPages#=%=#7
CopyrightSigned#=%=#Stalin Varanasi
JobTitle#==#
Organization#==#Stalin Varanasi, DFKI Saarbrucken, Germany
Abstract#==#Contextualized word embeddings provide better initialization for neural networks that deal
with various natural language understanding (NLU) tasks including Question Answering
(QA) and more recently, Question Generation(QG). Apart from providing meaningful word representations, pre-trained transformer models (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) also provide self-attentions which encode syntactic information that can be probed for dependency parsing (Hewitt and Manning, 2019) and POStagging (Coenen et al., 2019). In this paper, we show that the information from selfattentions of BERT are useful for language modeling of questions conditioned on paragraph and answer phrases. To control the attention span, we use semi-diagonal mask and utilize a shared model for encoding and decoding, unlike sequence-to-sequence. We further employ copy-mechanism over self-attentions
to acheive state-of-the-art results for Question Generation on SQuAD v1.1 (Rajpurkar et al.,
2016).
Author{1}{Firstname}#=%=#Stalin
Author{1}{Lastname}#=%=#Varanasi
Author{1}{Username}#=%=#stalin
Author{1}{Email}#=%=#varanasi.stalin@gmail.com
Author{1}{Affiliation}#=%=#DFKI
Author{2}{Firstname}#=%=#Saadullah
Author{2}{Lastname}#=%=#Amin
Author{2}{Username}#=%=#suamin
Author{2}{Email}#=%=#saadullahamin@gmail.com
Author{2}{Affiliation}#=%=#DFKI
Author{3}{Firstname}#=%=#Guenter
Author{3}{Lastname}#=%=#Neumann
Author{3}{Username}#=%=#neumann
Author{3}{Email}#=%=#neumann@dfki.de
Author{3}{Affiliation}#=%=#DFKI & Saarland University

==========