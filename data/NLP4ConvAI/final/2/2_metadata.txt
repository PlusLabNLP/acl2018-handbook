SubmissionNumber#=%=#2
FinalPaperTitle#=%=#On Incorporating Structural Information to improve Dialogue Response Generation
ShortPaperTitle#=%=#
NumberOfPages#=%=#14
CopyrightSigned#=%=#Nikita Moghe
JobTitle#==#
Organization#==#University of Edinburgh
Abstract#==#We consider the task of generating dialogue responses from background knowledge comprising of domain specific resources. Specifically, given a conversation around a movie, the task is to generate the next response based on background knowledge about the movie such as the plot, review, Reddit comments etc. This requires capturing structural, sequential and semantic information from the conversation context and the background resources. We propose a new architecture that uses the ability of BERT to capture deep contextualized representations in conjunction with explicit structure and sequence information. More specifically, we use (i) Graph Convolutional Networks (GCNs) to capture structural information, (ii) LSTMs to capture sequential information and (iii) BERT for the deep contextualized representations that capture semantic information. We analyze the proposed architecture extensively. To this end, we propose a plug-and-play Semantics-Sequences-Structures (SSS) framework which allows us to effectively combine such linguistic information. Through a series of experiments we make some interesting observations. First, we observe that the popular adaptation of the GCN model for NLP tasks where structural information (GCNs) was added on top of sequential information (LSTMs) performs poorly on our task. This leads us to explore interesting ways of combining semantic and structural information to improve the performance. Second, we observe that while BERT already outperforms other deep contextualized representations such as ELMo, it still benefits from the additional structural information explicitly added using GCNs. This is a bit surprising given the recent claims that BERT already captures structural information. Lastly, the proposed SSS framework gives an improvement of 7.95% on BLUE score over the baseline.
Author{1}{Firstname}#=%=#Nikita
Author{1}{Lastname}#=%=#Moghe
Author{1}{Username}#=%=#nikitam
Author{1}{Email}#=%=#nikitamoghe29@gmail.com
Author{1}{Affiliation}#=%=#University of Edinburgh
Author{2}{Firstname}#=%=#Priyesh
Author{2}{Lastname}#=%=#Vijayan
Author{2}{Username}#=%=#priyeshv
Author{2}{Email}#=%=#priyesh.vijayan@mail.mcgill.ca
Author{2}{Affiliation}#=%=#McGill University + MILA
Author{3}{Firstname}#=%=#Balaraman
Author{3}{Lastname}#=%=#Ravindran
Author{3}{Username}#=%=#ravib
Author{3}{Email}#=%=#ravi@cse.iitm.ac.in
Author{3}{Affiliation}#=%=#Indian Institute of Technology Madras
Author{4}{Firstname}#=%=#Mitesh M.
Author{4}{Lastname}#=%=#Khapra
Author{4}{Username}#=%=#miteshk
Author{4}{Email}#=%=#miteshk@cse.iitm.ac.in
Author{4}{Affiliation}#=%=#Indian Institute of Technology Madras

==========