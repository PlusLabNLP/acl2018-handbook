SubmissionNumber#=%=#19
FinalPaperTitle#=%=#DLGNet: A Transformer-based Model for Dialogue Response Generation
ShortPaperTitle#=%=#
NumberOfPages#=%=#9
CopyrightSigned#=%=#Oluwatobi Olabiyi
JobTitle#==#Senior Research Scientist
Organization#==#Capital One Conversation Research, Vienna VA
Abstract#==#Neural dialogue models, despite their successes, still suffer from lack of relevance, diversity, and in many cases coherence in their generated responses. On the other hand, transformer-based models such as GPT-2 have demonstrated an excellent ability to capture long-range structures in language modeling tasks.  In this paper, we present DLGNet, a transformer-based model for dialogue modeling. We specifically examine the use of DLGNet for multi-turn dialogue response generation. In our experiments, we evaluate DLGNet on the open-domain Movie Triples dataset and the closed-domain Ubuntu Dialogue dataset. DLGNet models, although trained with only the maximum likelihood objective, achieve significant improvements over state-of-the-art multi-turn dialogue models. They also produce best performance to date on the two datasets based on several metrics, including BLEU, ROUGE, and distinct n-gram. Our analysis shows that the performance improvement is mostly due to the combination of (1) the long-range transformer architecture with (2) the injection of random informative paddings. Other contributing factors include the joint modeling of dialogue context and response, and the 100\% tokenization coverage from the byte pair encoding (BPE).
Author{1}{Firstname}#=%=#Olabiyi
Author{1}{Lastname}#=%=#Oluwatobi
Author{1}{Username}#=%=#engr3os
Author{1}{Email}#=%=#oluwatobi.olabiyi@capitalone.com
Author{1}{Affiliation}#=%=#Capital One
Author{2}{Firstname}#=%=#Erik
Author{2}{Lastname}#=%=#Mueller
Author{2}{Username}#=%=#erikmueller
Author{2}{Email}#=%=#erik.mueller@capitalone.com
Author{2}{Affiliation}#=%=#Capital One

==========