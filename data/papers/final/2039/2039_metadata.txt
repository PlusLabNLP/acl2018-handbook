SubmissionNumber#=%=#2039
FinalPaperTitle#=%=#Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?
ShortPaperTitle#=%=#
NumberOfPages#=%=#17
CopyrightSigned#=%=#Yada Pruksachatkun
JobTitle#==#
Organization#==#Center for Data Science at New York University, 60 5th Avenue, New York, NY 10011
Abstract#==#While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model  with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.
Author{1}{Firstname}#=%=#Yada
Author{1}{Lastname}#=%=#Pruksachatkun
Author{1}{Username}#=%=#yadapruk
Author{1}{Email}#=%=#yp913@nyu.edu
Author{1}{Affiliation}#=%=#NYU
Author{2}{Firstname}#=%=#Jason
Author{2}{Lastname}#=%=#Phang
Author{2}{Username}#=%=#zphang
Author{2}{Email}#=%=#jasonphang@nyu.edu
Author{2}{Affiliation}#=%=#New York University
Author{3}{Firstname}#=%=#Haokun
Author{3}{Lastname}#=%=#Liu
Author{3}{Username}#=%=#haokunliu
Author{3}{Email}#=%=#hl3236@nyu.edu
Author{3}{Affiliation}#=%=#New York University
Author{4}{Firstname}#=%=#Phu Mon
Author{4}{Lastname}#=%=#Htut
Author{4}{Username}#=%=#pmh330
Author{4}{Email}#=%=#pmh330@nyu.edu
Author{4}{Affiliation}#=%=#New York University
Author{5}{Firstname}#=%=#Xiaoyi
Author{5}{Lastname}#=%=#Zhang
Author{5}{Username}#=%=#elle0804
Author{5}{Email}#=%=#xz2448@nyu.edu
Author{5}{Affiliation}#=%=#New York University
Author{6}{Firstname}#=%=#Richard Yuanzhe
Author{6}{Lastname}#=%=#Pang
Author{6}{Username}#=%=#yzpang
Author{6}{Email}#=%=#yzpang@nyu.edu
Author{6}{Affiliation}#=%=#New York University
Author{7}{Firstname}#=%=#Clara
Author{7}{Lastname}#=%=#Vania
Author{7}{Username}#=%=#cvania
Author{7}{Email}#=%=#c.vania@nyu.edu
Author{7}{Affiliation}#=%=#New York University
Author{8}{Firstname}#=%=#Katharina
Author{8}{Lastname}#=%=#Kann
Author{8}{Username}#=%=#kann
Author{8}{Email}#=%=#katharina.kann@colorado.edu
Author{8}{Affiliation}#=%=#University of Colorado Boulder
Author{9}{Firstname}#=%=#Samuel R.
Author{9}{Lastname}#=%=#Bowman
Author{9}{Username}#=%=#sbowman
Author{9}{Email}#=%=#bowman@nyu.edu
Author{9}{Affiliation}#=%=#New York University

==========