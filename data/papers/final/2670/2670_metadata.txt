SubmissionNumber#=%=#2670
FinalPaperTitle#=%=#Are we Estimating or Guesstimating Translation Quality?
ShortPaperTitle#=%=#
NumberOfPages#=%=#6
CopyrightSigned#=%=#Shuo Sun
JobTitle#==#
Organization#==#Johns Hopkins University
Abstract#==#Recent advances in pre-trained multilingual language models lead to state-of-the-art results on the task of quality estimation (QE) for machine translation. A carefully engineered ensemble of such models won the QE shared task at WMT19. Our in-depth analysis, however, shows that the success of using pre-trained language models for QE is over-estimated due to three issues we observed in current QE datasets:  (i) The distributions of quality scores are imbalanced and skewed towards good quality scores; (iii) QE models can perform well on these datasets while looking at only source or translated sentences; (iii) They contain statistical artifacts that correlate well with human-annotated QE labels.
Our findings suggest that although QE models might capture fluency of translated sentences and complexity of source sentences, they cannot model adequacy of translations effectively.
Author{1}{Firstname}#=%=#Shuo
Author{1}{Lastname}#=%=#Sun
Author{1}{Username}#=%=#ssfei81
Author{1}{Email}#=%=#ssun32@jhu.edu
Author{1}{Affiliation}#=%=#Johns Hopkins University
Author{2}{Firstname}#=%=#Francisco
Author{2}{Lastname}#=%=#Guzm√°n
Author{2}{Username}#=%=#fguzman
Author{2}{Email}#=%=#fguzman@fb.com
Author{2}{Affiliation}#=%=#Facebook
Author{3}{Firstname}#=%=#Lucia
Author{3}{Lastname}#=%=#Specia
Author{3}{Username}#=%=#l.specia
Author{3}{Email}#=%=#l.specia@imperial.ac.uk
Author{3}{Affiliation}#=%=#Imperial College London

==========