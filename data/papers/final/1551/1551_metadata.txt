SubmissionNumber#=%=#1551
FinalPaperTitle#=%=#A Simple and Effective Unified Encoder for Document-Level Machine Translation
ShortPaperTitle#=%=#
NumberOfPages#=%=#7
CopyrightSigned#=%=#Shuming Ma
JobTitle#==#
Organization#==#Microsoft Research Asia
Abstract#==#Most of the existing models for document-level machine translation adopt dual-encoder structures. The representation of the source sentences and the document-level contexts\footnote{In this work, document-level contexts denote the surrounding sentences of the current source sentence.} are modeled with two separate encoders. Although these models can make use of the document-level contexts, they do not fully model the interaction between the contexts and the source sentences, and can not directly adapt to the recent pre-training models (e.g., BERT) which encodes multiple sentences with a single encoder. In this work, we propose a simple and effective unified encoder that can outperform the baseline models of dual-encoder models in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of our proposed model.
Author{1}{Firstname}#=%=#Shuming
Author{1}{Lastname}#=%=#Ma
Author{1}{Username}#=%=#shumingma
Author{1}{Email}#=%=#shumma@microsoft.com
Author{1}{Affiliation}#=%=#Microsoft Research Asia
Author{2}{Firstname}#=%=#Dongdong
Author{2}{Lastname}#=%=#Zhang
Author{2}{Username}#=%=#zdd
Author{2}{Email}#=%=#dozhang@microsoft.com
Author{2}{Affiliation}#=%=#Microsoft Research Asia
Author{3}{Firstname}#=%=#Ming
Author{3}{Lastname}#=%=#Zhou
Author{3}{Username}#=%=#mingzhou
Author{3}{Email}#=%=#mingzhou@microsoft.com
Author{3}{Affiliation}#=%=#microsoft research asia

==========