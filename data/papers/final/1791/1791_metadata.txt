SubmissionNumber#=%=#1791
FinalPaperTitle#=%=#TVQA+: Spatio-Temporal Grounding for Video Question Answering
ShortPaperTitle#=%=#
NumberOfPages#=%=#15
CopyrightSigned#=%=#Jie Lei
JobTitle#==#
Organization#==#UNC Chapel Hill
Abstract#==#We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations.
Author{1}{Firstname}#=%=#Jie
Author{1}{Lastname}#=%=#Lei
Author{1}{Username}#=%=#jie
Author{1}{Email}#=%=#jielei@cs.unc.edu
Author{1}{Affiliation}#=%=#University of North Carolina at Chapel Hill
Author{2}{Firstname}#=%=#Licheng
Author{2}{Lastname}#=%=#Yu
Author{2}{Username}#=%=#licheng
Author{2}{Email}#=%=#licheng@cs.unc.edu
Author{2}{Affiliation}#=%=#University of North Carolina at Chapel Hill
Author{3}{Firstname}#=%=#Tamara
Author{3}{Lastname}#=%=#Berg
Author{3}{Username}#=%=#berg.tamara
Author{3}{Email}#=%=#berg.tamara@gmail.com
Author{3}{Affiliation}#=%=#University of North Carolina Chapel Hill
Author{4}{Firstname}#=%=#Mohit
Author{4}{Lastname}#=%=#Bansal
Author{4}{Username}#=%=#mbansal
Author{4}{Email}#=%=#mbansal@cs.unc.edu
Author{4}{Affiliation}#=%=#University of North Carolina at Chapel Hill

==========