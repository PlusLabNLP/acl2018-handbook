SubmissionNumber#=%=#2876
FinalPaperTitle#=%=#TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data
ShortPaperTitle#=%=#
NumberOfPages#=%=#14
CopyrightSigned#=%=#Pengcheng Yin
JobTitle#==#
Organization#==#Carnegie Mellon University
Abstract#==#Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TaBERT is trained on a large corpus of 26 million tables and their English  contexts. In experiments, neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider.
Author{1}{Firstname}#=%=#Pengcheng
Author{1}{Lastname}#=%=#Yin
Author{1}{Username}#=%=#handesy
Author{1}{Email}#=%=#pcyin@cs.cmu.edu
Author{1}{Affiliation}#=%=#Carnegie Mellon University
Author{2}{Firstname}#=%=#Graham
Author{2}{Lastname}#=%=#Neubig
Author{2}{Username}#=%=#gneubig
Author{2}{Email}#=%=#gneubig@cs.cmu.edu
Author{2}{Affiliation}#=%=#Carnegie Mellon University
Author{3}{Firstname}#=%=#Wen-tau
Author{3}{Lastname}#=%=#Yih
Author{3}{Username}#=%=#scottyih
Author{3}{Email}#=%=#scottyih.review@gmail.com
Author{3}{Affiliation}#=%=#Facebook AI Research
Author{4}{Firstname}#=%=#Sebastian
Author{4}{Lastname}#=%=#Riedel
Author{4}{Email}#=%=#S.Riedel@cs.ucl.ac.uk
Author{4}{Affiliation}#=%=#Facebook AI Research

==========