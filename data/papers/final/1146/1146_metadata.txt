SubmissionNumber#=%=#1146
FinalPaperTitle#=%=#Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Chulun Zhou
JobTitle#==#
Organization#==#Xiamen University
Abstract#==#Unsupervised style transfer aims to change the style of an input sentence while preserving its original content without using parallel training data. In current dominant approaches, owing to the lack of fine-grained control on the influence from the target style, they are unable to yield desirable output sentences. In this paper, we propose a novel attentional sequence-to-sequence (Seq2seq) model that dynamically exploits the relevance of each output word to the target style for unsupervised style transfer. Specifically, we first pretrain a style classifier, where the relevance of each input word to the original style can be quantified via layer-wise relevance propagation. In a denoising auto-encoding manner, we train an attentional Seq2seq model to reconstruct input sentences and repredict word-level previously-quantified style relevance simultaneously. In this way, this model is endowed with the ability to automatically predict the style relevance of each output word. Then, we equip the decoder of this model with a neural style component to exploit the predicted wordlevel style relevance for better style transfer. Particularly, we fine-tune this model using a carefully-designed objective function involving style transfer, style relevance consistency, content preservation and fluency modeling loss terms. Experimental results show that our proposed model achieves state-of-the-art performance in terms of both transfer accuracy and content preservation.
Author{1}{Firstname}#=%=#Chulun
Author{1}{Lastname}#=%=#Zhou
Author{1}{Username}#=%=#zhouchulun
Author{1}{Email}#=%=#clzhou@stu.xmu.edu.cn
Author{1}{Affiliation}#=%=#Xiamen University
Author{2}{Firstname}#=%=#Liangyu
Author{2}{Lastname}#=%=#Chen
Author{2}{Username}#=%=#chenliangyu07
Author{2}{Email}#=%=#chenliangyu@baidu.com
Author{2}{Affiliation}#=%=#Baidu
Author{3}{Firstname}#=%=#Jiachen
Author{3}{Lastname}#=%=#Liu
Author{3}{Username}#=%=#hugepanda88
Author{3}{Email}#=%=#liujiachen@baidu.com
Author{3}{Affiliation}#=%=#Baidu
Author{4}{Firstname}#=%=#Xinyan
Author{4}{Lastname}#=%=#Xiao
Author{4}{Username}#=%=#xxy
Author{4}{Email}#=%=#xiaoxinyan@baidu.com
Author{4}{Affiliation}#=%=#Baidu Inc.
Author{5}{Firstname}#=%=#Jinsong
Author{5}{Lastname}#=%=#Su
Author{5}{Username}#=%=#jssu
Author{5}{Email}#=%=#jssu@xmu.edu.cn
Author{5}{Affiliation}#=%=#Xiamen university
Author{6}{Firstname}#=%=#Sheng
Author{6}{Lastname}#=%=#Guo
Author{6}{Username}#=%=#guoshengcs
Author{6}{Email}#=%=#guosheng@baidu.com
Author{6}{Affiliation}#=%=#Baidu
Author{7}{Firstname}#=%=#Hua
Author{7}{Lastname}#=%=#Wu
Author{7}{Username}#=%=#wu_hua
Author{7}{Email}#=%=#wu_hua@baidu.com
Author{7}{Affiliation}#=%=#Baidu

==========