SubmissionNumber#=%=#1915
FinalPaperTitle#=%=#Learning a Multi-Domain Curriculum for Neural Machine Translation
ShortPaperTitle#=%=#
NumberOfPages#=%=#13
CopyrightSigned#=%=#Wei Wang
JobTitle#==#
Organization#==#Google, Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043
Abstract#==#Most data selection research in machine translation focuses on improving a single domain. We perform data selection for multiple domains at once. This is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches. Both the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-of-domain. In large-scale experiments, the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training.
Author{1}{Firstname}#=%=#Wei
Author{1}{Lastname}#=%=#Wang
Author{1}{Username}#=%=#wangwe
Author{1}{Email}#=%=#wangwe@google.com
Author{1}{Affiliation}#=%=#Google Research
Author{2}{Firstname}#=%=#Ye
Author{2}{Lastname}#=%=#Tian
Author{2}{Username}#=%=#ytian
Author{2}{Email}#=%=#ytian@google.com
Author{2}{Affiliation}#=%=#Google
Author{3}{Firstname}#=%=#Jiquan
Author{3}{Lastname}#=%=#Ngiam
Author{3}{Username}#=%=#jngiam
Author{3}{Email}#=%=#jngiam@google.com
Author{3}{Affiliation}#=%=#Google
Author{4}{Firstname}#=%=#Yinfei
Author{4}{Lastname}#=%=#Yang
Author{4}{Username}#=%=#yinfeiy
Author{4}{Email}#=%=#yangyin7@gmail.com
Author{4}{Affiliation}#=%=#Google
Author{5}{Firstname}#=%=#Isaac
Author{5}{Lastname}#=%=#Caswell
Author{5}{Username}#=%=#icaswell
Author{5}{Email}#=%=#icaswell@google.com
Author{5}{Affiliation}#=%=#Google Research
Author{6}{Firstname}#=%=#Zarana
Author{6}{Lastname}#=%=#Parekh
Author{6}{Username}#=%=#zarana
Author{6}{Email}#=%=#zarana@google.com
Author{6}{Affiliation}#=%=#Google Research

==========