SubmissionNumber#=%=#2000
FinalPaperTitle#=%=#Probing for Referential Information in Language Models
ShortPaperTitle#=%=#
NumberOfPages#=%=#13
CopyrightSigned#=%=#Ionut Teodor Sorodoc
JobTitle#==#
Organization#==#Universitat Pompeu Fabra, Barcelona, Spain
Abstract#==#Language models keep track of complex information about the preceding context -- including, e.g., syntactic relations in a sentence. We investigate whether they also capture information beneficial for resolving pronominal anaphora in English.
We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus.
The Transformer outperforms the LSTM in all analyses.
Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world.
However, we find traces of the latter aspect, too.
Author{1}{Firstname}#=%=#Ionut-Teodor
Author{1}{Lastname}#=%=#Sorodoc
Author{1}{Username}#=%=#sorodoc
Author{1}{Email}#=%=#ionut.sorodoc@gmail.com
Author{1}{Affiliation}#=%=#Universitat Pompeu Fabra
Author{2}{Firstname}#=%=#Kristina
Author{2}{Lastname}#=%=#Gulordava
Author{2}{Username}#=%=#xsway
Author{2}{Email}#=%=#kgulordava@gmail.com
Author{2}{Affiliation}#=%=#Shift Technology
Author{3}{Firstname}#=%=#Gemma
Author{3}{Lastname}#=%=#Boleda
Author{3}{Username}#=%=#gboleda
Author{3}{Email}#=%=#gemma.boleda@upf.edu
Author{3}{Affiliation}#=%=#ICREA / Universitat Pompeu Fabra

==========