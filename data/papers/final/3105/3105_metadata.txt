SubmissionNumber#=%=#3105
FinalPaperTitle#=%=#INSET: Sentence Infilling with INter-SEntential Transformer
ShortPaperTitle#=%=#
NumberOfPages#=%=#14
CopyrightSigned#=%=#Yichen Huang
JobTitle#==#
Organization#==#Massachusetts Institute of Technology
Abstract#==#Missing sentence generation (or sentence in-filling) fosters a wide range of applications in natural language generation, such as document auto-completion and meeting note expansion. This task asks the model to generate intermediate missing sentences that can syntactically and semantically bridge the surrounding context. Solving the sentence infilling task requires techniques in natural language processing ranging from understanding to discourse-level planning to generation. In this paper, we propose a framework to decouple the challenge and address these three aspects respectively, leveraging the power of existing large-scale pre-trained models such as BERT and GPT-2. We empirically demonstrate the effectiveness of our model in learning a sentence representation for generation and further generating a missing sentence that fits the context.
Author{1}{Firstname}#=%=#Yichen
Author{1}{Lastname}#=%=#Huang
Author{1}{Username}#=%=#huangyc07
Author{1}{Email}#=%=#yichuang@mit.edu
Author{1}{Affiliation}#=%=#Massachusetts Institute of Technology
Author{2}{Firstname}#=%=#Yizhe
Author{2}{Lastname}#=%=#Zhang
Author{2}{Username}#=%=#yizhe
Author{2}{Email}#=%=#yizhe.zhang@microsoft.com
Author{2}{Affiliation}#=%=#Microsoft
Author{3}{Firstname}#=%=#Oussama
Author{3}{Lastname}#=%=#Elachqar
Author{3}{Username}#=%=#ouelachq
Author{3}{Email}#=%=#ouelachq@microsoft.com
Author{3}{Affiliation}#=%=#Microsoft
Author{4}{Firstname}#=%=#Yu
Author{4}{Lastname}#=%=#Cheng
Author{4}{Username}#=%=#ych.thu
Author{4}{Email}#=%=#yu.cheng@microsoft.com
Author{4}{Affiliation}#=%=#Microsoft

==========