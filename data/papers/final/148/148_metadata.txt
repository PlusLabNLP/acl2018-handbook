SubmissionNumber#=%=#148
FinalPaperTitle#=%=#HAT: Hardware-Aware Transformers for Efficient Natural Language Processing
ShortPaperTitle#=%=#
NumberOfPages#=%=#14
CopyrightSigned#=%=#Hanrui Wang
JobTitle#==#
Organization#==#Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, USA
Abstract#==#Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT’14 translation task on Raspberry Pi-4, HAT can achieve 3× speedup, 3.7× smaller size over baseline Transformer; 2.7× speedup, 3.6× smaller size over Evolved Transformer with 12,041× less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.
Author{1}{Firstname}#=%=#Hanrui
Author{1}{Lastname}#=%=#Wang
Author{1}{Username}#=%=#hanruiwang
Author{1}{Email}#=%=#hanrui@mit.edu
Author{1}{Affiliation}#=%=#Massachusetts Institute of Technology
Author{2}{Firstname}#=%=#Zhanghao
Author{2}{Lastname}#=%=#Wu
Author{2}{Username}#=%=#michaelvll
Author{2}{Email}#=%=#zhwu@mit.edu
Author{2}{Affiliation}#=%=#Massachusetts Institute of Technology
Author{3}{Firstname}#=%=#Zhijian
Author{3}{Lastname}#=%=#Liu
Author{3}{Email}#=%=#zhijian@mit.edu
Author{3}{Affiliation}#=%=#MIT
Author{4}{Firstname}#=%=#Han
Author{4}{Lastname}#=%=#Cai
Author{4}{Email}#=%=#hancai@mit.edu
Author{4}{Affiliation}#=%=#MIT
Author{5}{Firstname}#=%=#Ligeng
Author{5}{Lastname}#=%=#Zhu
Author{5}{Username}#=%=#ligeng
Author{5}{Email}#=%=#ligeng@mit.edu
Author{5}{Affiliation}#=%=#Massachusetts Institute of Technology
Author{6}{Firstname}#=%=#Chuang
Author{6}{Lastname}#=%=#Gan
Author{6}{Username}#=%=#ganchuang
Author{6}{Email}#=%=#ganchuang1990@gmail.com
Author{6}{Affiliation}#=%=#MIT-IBM Watson AI Lab
Author{7}{Firstname}#=%=#Song
Author{7}{Lastname}#=%=#Han
Author{7}{Username}#=%=#songhan
Author{7}{Email}#=%=#songhan@mit.edu
Author{7}{Affiliation}#=%=#MIT

==========