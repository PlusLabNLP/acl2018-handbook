SubmissionNumber#=%=#3218
FinalPaperTitle#=%=#Finding Universal Grammatical Relations in Multilingual BERT
ShortPaperTitle#=%=#
NumberOfPages#=%=#14
CopyrightSigned#=%=#Ethan Andrew Chi
JobTitle#==#
Organization#==#Stanford University. 450 Serra Mall, Stanford, CA 94305
Abstract#==#Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks' internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.
Author{1}{Firstname}#=%=#Ethan A.
Author{1}{Lastname}#=%=#Chi
Author{1}{Username}#=%=#ethanchi
Author{1}{Email}#=%=#ethanchi@cs.stanford.edu
Author{1}{Affiliation}#=%=#Stanford University
Author{2}{Firstname}#=%=#John
Author{2}{Lastname}#=%=#Hewitt
Author{2}{Username}#=%=#johnhewitt
Author{2}{Email}#=%=#johnhew@stanford.edu
Author{2}{Affiliation}#=%=#Stanford University
Author{3}{Firstname}#=%=#Christopher D.
Author{3}{Lastname}#=%=#Manning
Author{3}{Username}#=%=#manning
Author{3}{Email}#=%=#manning@cs.stanford.edu
Author{3}{Affiliation}#=%=#Stanford University

==========