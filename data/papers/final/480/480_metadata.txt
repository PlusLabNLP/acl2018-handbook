SubmissionNumber#=%=#480
FinalPaperTitle#=%=#PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable
ShortPaperTitle#=%=#
NumberOfPages#=%=#12
CopyrightSigned#=%=#Siqi BAO
JobTitle#==#
Organization#==#Baidu Inc., China
Abstract#==#Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.
Author{1}{Firstname}#=%=#Siqi
Author{1}{Lastname}#=%=#Bao
Author{1}{Username}#=%=#sbao
Author{1}{Email}#=%=#baosiqi@baidu.com
Author{1}{Affiliation}#=%=#Baidu
Author{2}{Firstname}#=%=#Huang
Author{2}{Lastname}#=%=#He
Author{2}{Username}#=%=#sserdoubleh
Author{2}{Email}#=%=#hehuang@baidu.com
Author{2}{Affiliation}#=%=#Baidu
Author{3}{Firstname}#=%=#Fan
Author{3}{Lastname}#=%=#Wang
Author{3}{Username}#=%=#worldeditor
Author{3}{Email}#=%=#fanwang.px@gmail.com
Author{3}{Affiliation}#=%=#Baidu Inc.
Author{4}{Firstname}#=%=#Hua
Author{4}{Lastname}#=%=#Wu
Author{4}{Username}#=%=#wu_hua
Author{4}{Email}#=%=#wu_hua@baidu.com
Author{4}{Affiliation}#=%=#Baidu
Author{5}{Firstname}#=%=#Haifeng
Author{5}{Lastname}#=%=#Wang
Author{5}{Username}#=%=#wanghaifeng
Author{5}{Email}#=%=#wanghaifeng@baidu.com
Author{5}{Affiliation}#=%=#Baidu

==========