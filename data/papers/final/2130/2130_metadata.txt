SubmissionNumber#=%=#2130
FinalPaperTitle#=%=#Enabling Language Models to Fill in the Blanks
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Chris Donahue
JobTitle#==#
Organization#==#353 Jane Stanford Way Stanford, CA 94305
Abstract#==#We present a simple approach for \emph{text infilling}, the task of predicting missing spans of text at any position in a document. While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling---a special case of infilling where text is predicted at the end of a document. In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling. To this end, we train (or fine tune) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked. We show that this approach, which we call \emph{infilling by language modeling}, can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics. Furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machine-generated in the domain of short stories.
Author{1}{Firstname}#=%=#Chris
Author{1}{Lastname}#=%=#Donahue
Author{1}{Username}#=%=#cdonahue
Author{1}{Email}#=%=#cdonahue@cs.stanford.edu
Author{1}{Affiliation}#=%=#Stanford
Author{2}{Firstname}#=%=#Mina
Author{2}{Lastname}#=%=#Lee
Author{2}{Username}#=%=#minalee
Author{2}{Email}#=%=#minalee@cs.stanford.edu
Author{2}{Affiliation}#=%=#Stanford University
Author{3}{Firstname}#=%=#Percy
Author{3}{Lastname}#=%=#Liang
Author{3}{Username}#=%=#pliang
Author{3}{Email}#=%=#pliang@cs.stanford.edu
Author{3}{Affiliation}#=%=#Stanford University

==========