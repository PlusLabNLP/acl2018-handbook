SubmissionNumber#=%=#2902
FinalPaperTitle#=%=#FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization
ShortPaperTitle#=%=#
NumberOfPages#=%=#16
CopyrightSigned#=%=#Esin Durmus
JobTitle#==#PhD candidate
Organization#==#Cornell University
Abstract#==#Neural abstractive summarization models are prone to generate content inconsistent with
the source document, i.e. unfaithful. Existing automatic metrics do not capture such
mistakes effectively. We tackle the problem
of evaluating faithfulness of a generated summary given its source document. We first
collected human annotations of faithfulness
for outputs from numerous models on two
datasets. We find that current models exhibit
a trade-off between abstractiveness and faithfulness: outputs with less word overlap with
the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances
in reading comprehension. Given question-answer pairs generated from the summary, a
QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among
metrics based on word overlap, embedding
similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive
summaries.
Author{1}{Firstname}#=%=#Esin
Author{1}{Lastname}#=%=#Durmus
Author{1}{Username}#=%=#esdurmus
Author{1}{Email}#=%=#ed459@cornell.edu
Author{1}{Affiliation}#=%=#Cornell University
Author{2}{Firstname}#=%=#He
Author{2}{Lastname}#=%=#He
Author{2}{Username}#=%=#hhe
Author{2}{Email}#=%=#hhe.xiy@gmail.com
Author{2}{Affiliation}#=%=#New York University
Author{3}{Firstname}#=%=#Mona
Author{3}{Lastname}#=%=#Diab
Author{3}{Username}#=%=#mdiab
Author{3}{Email}#=%=#mtdiab@gmail.com
Author{3}{Affiliation}#=%=#Facebook AI

==========