SubmissionNumber#=%=#1691
FinalPaperTitle#=%=#Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Alexandre Tamborrino
JobTitle#==#
Organization#==#Samsung Strategy and Innovation Center, 18 Rue du 4 septembre, 75002 Paris, FRANCE
Abstract#==#Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks. Most of the existing approaches rely on a randomly initialized classifier on top of such networks. We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task. In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase. We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets. By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches. Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g  x10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.
Author{1}{Firstname}#=%=#Alexandre
Author{1}{Lastname}#=%=#Tamborrino
Author{1}{Username}#=%=#a.tamborrino
Author{1}{Email}#=%=#a.tamborrino@samsung.com
Author{1}{Affiliation}#=%=#Samsung
Author{2}{Firstname}#=%=#Nicola
Author{2}{Lastname}#=%=#Pellican√≤
Author{2}{Username}#=%=#nicks_91
Author{2}{Email}#=%=#n.pellicano@samsung.com
Author{2}{Affiliation}#=%=#Samsung
Author{3}{Firstname}#=%=#Baptiste
Author{3}{Lastname}#=%=#Pannier
Author{3}{Username}#=%=#b.pannier
Author{3}{Email}#=%=#b.pannier@samsung.com
Author{3}{Affiliation}#=%=#Samsung
Author{4}{Firstname}#=%=#Pascal
Author{4}{Lastname}#=%=#Voitot
Author{4}{Username}#=%=#p.voitot
Author{4}{Email}#=%=#p.voitot@samsung.com
Author{4}{Affiliation}#=%=#Samsung
Author{5}{Firstname}#=%=#Louise
Author{5}{Lastname}#=%=#Naudin
Author{5}{Username}#=%=#naudinlo
Author{5}{Email}#=%=#l.naudin@samsung.com
Author{5}{Affiliation}#=%=#Samsung

==========