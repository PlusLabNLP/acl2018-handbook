SubmissionNumber#=%=#2309
FinalPaperTitle#=%=#BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Mike Lewis
JobTitle#==#
Organization#==#Facebook, 1 Hacker Way, Menlo Park, 94025
Abstract#==#We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.
Author{1}{Firstname}#=%=#Mike
Author{1}{Lastname}#=%=#Lewis
Author{1}{Username}#=%=#mikelewis0
Author{1}{Email}#=%=#mikelewis0@gmail.com
Author{1}{Affiliation}#=%=#Facebook AI Research
Author{2}{Firstname}#=%=#Yinhan
Author{2}{Lastname}#=%=#Liu
Author{2}{Username}#=%=#yinhanliu
Author{2}{Email}#=%=#yinhanliu@fb.com
Author{2}{Affiliation}#=%=#Facebook AI Research
Author{3}{Firstname}#=%=#Naman
Author{3}{Lastname}#=%=#Goyal
Author{3}{Username}#=%=#ngoyal
Author{3}{Email}#=%=#naman.goyal21@gmail.com
Author{3}{Affiliation}#=%=#Facebook
Author{4}{Firstname}#=%=#Marjan
Author{4}{Lastname}#=%=#Ghazvininejad
Author{4}{Username}#=%=#mghazvininejad
Author{4}{Email}#=%=#ghazvini@fb.com
Author{4}{Affiliation}#=%=#Facebook AI Research
Author{5}{Firstname}#=%=#Abdelrahman
Author{5}{Lastname}#=%=#Mohamed
Author{5}{Username}#=%=#asamir
Author{5}{Email}#=%=#abdo@fb.com
Author{5}{Affiliation}#=%=#Facebook AI Research
Author{6}{Firstname}#=%=#Omer
Author{6}{Lastname}#=%=#Levy
Author{6}{Username}#=%=#omerlevy
Author{6}{Email}#=%=#omerlevy@gmail.com
Author{6}{Affiliation}#=%=#Tel Aviv University
Author{7}{Firstname}#=%=#Veselin
Author{7}{Lastname}#=%=#Stoyanov
Author{7}{Username}#=%=#ves
Author{7}{Email}#=%=#vesko.st@gmail.com
Author{7}{Affiliation}#=%=#Facebook
Author{8}{Firstname}#=%=#Luke
Author{8}{Lastname}#=%=#Zettlemoyer
Author{8}{Username}#=%=#lsz
Author{8}{Email}#=%=#lsz@cs.washington.edu
Author{8}{Affiliation}#=%=#University of Washington; Facebook

==========