SubmissionNumber#=%=#718
FinalPaperTitle#=%=#Syntactic Data Augmentation Increases Robustness to Inference Heuristics
ShortPaperTitle#=%=#
NumberOfPages#=%=#14
CopyrightSigned#=%=#Junghyun Min
JobTitle#==#
Organization#==#Department of Cognitive Science, Johns Hopkins University, 3400 N Charles St., Baltimore, MD 21218
Abstract#==#Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets. We hypothesize that this issue is not primarily caused by the pretrained model's limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage. We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the MNLI corpus. The best-performing augmentation method, subject/object inversion, improved BERT's accuracy on controlled examples that diagnose sensitivity to word order from $0.28$ to $0.73$, without affecting performance on the MNLI test set. This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations.
Author{1}{Firstname}#=%=#Junghyun
Author{1}{Lastname}#=%=#Min
Author{1}{Username}#=%=#aatlantise
Author{1}{Email}#=%=#jmin10@jhu.edu
Author{1}{Affiliation}#=%=#Johns Hopkins University
Author{2}{Firstname}#=%=#R. Thomas
Author{2}{Lastname}#=%=#McCoy
Author{2}{Username}#=%=#rtmccoy17
Author{2}{Email}#=%=#tom.mccoy@jhu.edu
Author{2}{Affiliation}#=%=#Johns Hopkins University
Author{3}{Firstname}#=%=#Dipanjan
Author{3}{Lastname}#=%=#Das
Author{3}{Username}#=%=#dipanjand
Author{3}{Email}#=%=#dipanjand@gmail.com
Author{3}{Affiliation}#=%=#Google AI Language
Author{4}{Firstname}#=%=#Emily
Author{4}{Lastname}#=%=#Pitler
Author{4}{Username}#=%=#epitler
Author{4}{Email}#=%=#EMILY.PITLER@GMAIL.COM
Author{4}{Affiliation}#=%=#Google, Inc.
Author{5}{Firstname}#=%=#Tal
Author{5}{Lastname}#=%=#Linzen
Author{5}{Username}#=%=#tallinzen
Author{5}{Email}#=%=#tal.linzen@jhu.edu
Author{5}{Affiliation}#=%=#Johns Hopkins University

==========