SubmissionNumber#=%=#3282
FinalPaperTitle#=%=#DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering
ShortPaperTitle#=%=#
NumberOfPages#=%=#11
CopyrightSigned#=%=#Qingqing Cao
JobTitle#==#
Organization#==#Stony Brook University
Abstract#==#Transformer-based QA models use input-wide self-attention -- i.e. across both the question and the input passage -- at all layers, causing them to be slow and memory-intensive. It turns out that we can get by without input-wide self-attention at all layers, especially in the lower layers. We introduce DeFormer, a decomposed transformer, which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers. This allows for question-independent processing of the input text representations, which in turn enables pre-computing passage representations reducing runtime compute drastically. Furthermore, because DeFormer is largely similar to the original model, we can initialize DeFormer with the pre-training weights of a standard transformer, and directly fine-tune on the target QA dataset. We show DeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and with simple distillation-based losses they incur only a 1% drop in accuracy. We open source the code at https://github.com/StonyBrookNLP/deformer.
Author{1}{Firstname}#=%=#Qingqing
Author{1}{Lastname}#=%=#Cao
Author{1}{Username}#=%=#csarron
Author{1}{Email}#=%=#qicao@cs.stonybrook.edu
Author{1}{Affiliation}#=%=#Stony Brook University
Author{2}{Firstname}#=%=#Harsh
Author{2}{Lastname}#=%=#Trivedi
Author{2}{Username}#=%=#harshtrivedi
Author{2}{Email}#=%=#hjtrivedi@cs.stonybrook.edu
Author{2}{Affiliation}#=%=#Stony Brook University
Author{3}{Firstname}#=%=#Aruna
Author{3}{Lastname}#=%=#Balasubramanian
Author{3}{Username}#=%=#arunab
Author{3}{Email}#=%=#arunab@cs.stonybrook.edu
Author{3}{Affiliation}#=%=#Stony Brook University
Author{4}{Firstname}#=%=#Niranjan
Author{4}{Lastname}#=%=#Balasubramanian
Author{4}{Username}#=%=#niranjan
Author{4}{Email}#=%=#niranjan@cs.stonybrook.edu
Author{4}{Affiliation}#=%=#Stony Brook University

==========