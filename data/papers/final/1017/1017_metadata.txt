SubmissionNumber#=%=#1017
FinalPaperTitle#=%=#Highway Transformer: Self-Gating Enhanced Self-Attentive Networks
ShortPaperTitle#=%=#
NumberOfPages#=%=#14
CopyrightSigned#=%=#Yekun Chai
JobTitle#==#
Organization#==#Institute of Automation, Chinese Academy of Sciences. Beijing, China
Abstract#==#Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.
Author{1}{Firstname}#=%=#Yekun
Author{1}{Lastname}#=%=#Chai
Author{1}{Username}#=%=#cyk
Author{1}{Email}#=%=#chaiyekun@gmail.com
Author{1}{Affiliation}#=%=#Institute of Automation, Chinese Academy of Sciences
Author{2}{Firstname}#=%=#Shuo
Author{2}{Lastname}#=%=#Jin
Author{2}{Username}#=%=#wjwssxt
Author{2}{Email}#=%=#shj42@pitt.edu
Author{2}{Affiliation}#=%=#University of Pittsburgh
Author{3}{Firstname}#=%=#Xinwen
Author{3}{Lastname}#=%=#Hou
Author{3}{Username}#=%=#xwhou
Author{3}{Email}#=%=#xwhou@nlpr.ia.ac.cn
Author{3}{Affiliation}#=%=#Institute of Automation, Chinese Academy of Sciences

==========