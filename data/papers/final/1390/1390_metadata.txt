SubmissionNumber#=%=#1390
FinalPaperTitle#=%=#Large Scale Multi-Actor Generative Dialog Modeling
ShortPaperTitle#=%=#
NumberOfPages#=%=#19
CopyrightSigned#=%=#Alex Boyd
JobTitle#==#
Organization#==#University of California, Irvine
Abstract#==#Non-goal oriented dialog agents (i.e. chatbots) aim to produce varying and engaging conversations with a user; however, they typically exhibit either inconsistent personality across conversations or the average personality of all users. This paper addresses these issues by controlling an agent's persona upon generation via conditioning on prior conversations of a target actor. In doing so, we are able to utilize more abstract patterns within a person's speech and better emulate them in generated responses. This work introduces the Generative Conversation Control model, an augmented and fine-tuned GPT-2 language model that conditions on past reference conversations to probabilistically model multi-turn conversations in the actor's persona. We introduce an accompanying data collection procedure to obtain 10.3M conversations from 6 months worth of Reddit comments. We demonstrate that scaling model sizes from 117M to 8.3B parameters yields an improvement from 23.14 to 13.14 perplexity on 1.7M held out Reddit conversations. Increasing model scale yielded similar improvements in human evaluations that measure preference of model samples to the held out target distribution in terms of realism (31\% increased to 37% preference), style matching (37% to 42%), grammar and content quality (29% to 42%), and conversation coherency (32% to 40%). We find that conditionally modeling past conversations improves perplexity by 0.47 in automatic evaluations. Through human trials we identify positive trends between conditional modeling and style matching and outline steps to further improve persona control.
Author{1}{Firstname}#=%=#Alex
Author{1}{Lastname}#=%=#Boyd
Author{1}{Username}#=%=#alexjboyd
Author{1}{Email}#=%=#alexjb@uci.edu
Author{1}{Affiliation}#=%=#NVIDIA, University of California Irvine
Author{2}{Firstname}#=%=#Raul
Author{2}{Lastname}#=%=#Puri
Author{2}{Username}#=%=#raulp
Author{2}{Email}#=%=#raulpuric@berkeley.edu
Author{2}{Affiliation}#=%=#NVIDIA
Author{3}{Firstname}#=%=#Mohammad
Author{3}{Lastname}#=%=#Shoeybi
Author{3}{Username}#=%=#shoeybi
Author{3}{Email}#=%=#mshoeybi@nvidia.com
Author{3}{Affiliation}#=%=#NVIDIA
Author{4}{Firstname}#=%=#Mostofa
Author{4}{Lastname}#=%=#Patwary
Author{4}{Username}#=%=#mpatwary
Author{4}{Email}#=%=#mpatwary@nvidia.com
Author{4}{Affiliation}#=%=#NVIDIA
Author{5}{Firstname}#=%=#Bryan
Author{5}{Lastname}#=%=#Catanzaro
Author{5}{Username}#=%=#bcatanzaro
Author{5}{Email}#=%=#bcatanzaro@acm.org
Author{5}{Affiliation}#=%=#NVIDIA

==========