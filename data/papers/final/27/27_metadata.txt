SubmissionNumber#=%=#27
FinalPaperTitle#=%=#Learning Source Phrase Representations for Neural Machine Translation
ShortPaperTitle#=%=#
NumberOfPages#=%=#11
CopyrightSigned#=%=#Hongfei Xu
JobTitle#==#
Organization#==#Saarland University / Saarland, Germany
Abstract#==#The Transformer translation model (Vaswani et al., 2017) based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation (NMT). Though intuitively the attentional network can connect distant words via shorter network paths than RNNs, empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies (Tang et al., 2018). Considering that modeling phrases instead of words has significantly improved the Statistical Machine Translation (SMT) approach through the use of larger translation blocks ("phrases") and its reordering ability, modeling NMT at phrase level is an intuitive proposal to help the model capture long-distance relationships. In this paper, we first propose an attentive phrase representation generation mechanism which is able to generate phrase representations from corresponding token representations. In addition, we incorporate the generated phrase representations into the Transformer translation model to enhance its ability to capture long-distance relationships. In our experiments, we obtain significant improvements on the WMT 14 English-German and English-French tasks on top of the strong Transformer baseline, which shows the effectiveness of our approach. Our approach helps Transformer Base models perform at the level of Transformer Big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps. The fact that phrase representations help even in the big setting further supports our conjecture that they make a valuable contribution to long-distance relations.
Author{1}{Firstname}#=%=#Hongfei
Author{1}{Lastname}#=%=#Xu
Author{1}{Username}#=%=#ano
Author{1}{Email}#=%=#hfxunlp@foxmail.com
Author{1}{Affiliation}#=%=#Saarland University
Author{2}{Firstname}#=%=#Josef
Author{2}{Lastname}#=%=#van Genabith
Author{2}{Username}#=%=#josefvangenabith
Author{2}{Email}#=%=#josef.van_genabith@dfki.de
Author{2}{Affiliation}#=%=#DFKI
Author{3}{Firstname}#=%=#Deyi
Author{3}{Lastname}#=%=#Xiong
Author{3}{Username}#=%=#dyxiong
Author{3}{Email}#=%=#dyxiong@tju.edu.cn
Author{3}{Affiliation}#=%=#Tianjin University
Author{4}{Firstname}#=%=#Qiuhui
Author{4}{Lastname}#=%=#Liu
Author{4}{Username}#=%=#qhliu
Author{4}{Email}#=%=#liuqhano@foxmail.com
Author{4}{Affiliation}#=%=#China Mobile Online Services
Author{5}{Firstname}#=%=#Jingyi
Author{5}{Lastname}#=%=#Zhang
Author{5}{Username}#=%=#zhangjingyi
Author{5}{Email}#=%=#zhangjingyizz@gmail.com
Author{5}{Affiliation}#=%=#DFKI

==========