SubmissionNumber#=%=#1106
FinalPaperTitle#=%=#Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition
ShortPaperTitle#=%=#
NumberOfPages#=%=#14
CopyrightSigned#=%=#Ryuichi Takanobu
JobTitle#==#
Organization#==#Tsinghua University, Beijing, China
Abstract#==#Many studies have applied reinforcement learning to train a dialog policy and show great promise these years. One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms. However, modeling a realistic user simulator is challenging. A rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator. To avoid explicitly building a user simulator beforehand, we propose Multi-Agent Dialog Policy Learning, which regards both the system and the user as the dialog agents. Two agents interact with each other and are jointly learned simultaneously. The method uses the actor-critic framework to facilitate pretraining and improve scalability. We also propose Hybrid Value Network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog. Results show that our method can successfully build a system policy and a user policy simultaneously, and two agents can achieve a high task success rate through conversational interaction.
Author{1}{Firstname}#=%=#Ryuichi
Author{1}{Lastname}#=%=#Takanobu
Author{1}{Username}#=%=#truthless
Author{1}{Email}#=%=#truthless11@gmail.com
Author{1}{Affiliation}#=%=#Tsinghua University
Author{2}{Firstname}#=%=#Runze
Author{2}{Lastname}#=%=#Liang
Author{2}{Username}#=%=#liangrz15
Author{2}{Email}#=%=#liangrz15@mails.tsinghua.edu.cn
Author{2}{Affiliation}#=%=#Tsinghua University
Author{3}{Firstname}#=%=#Minlie
Author{3}{Lastname}#=%=#Huang
Author{3}{Username}#=%=#aihuang
Author{3}{Email}#=%=#aihuang@tsinghua.edu.cn
Author{3}{Affiliation}#=%=#Tsinghua University

==========