SubmissionNumber#=%=#2114
FinalPaperTitle#=%=#Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly
ShortPaperTitle#=%=#
NumberOfPages#=%=#8
CopyrightSigned#=%=#Nora Kassner
JobTitle#==#
Organization#==#Nora Kassner,   Center for Information and Language Processing (CIS), LMU Munich, Germany
Abstract#==#Building on Petroni et al. 2019, we propose two new probing tasks analyzing factual knowledge stored in
Pretrained Language Models (PLMs).

(1) Negation. We find that PLMs do not distinguish between negated
(``Birds cannot [MASK]'') and non-negated (``Birds can [MASK]'') cloze questions.
(2) Mispriming. Inspired by priming methods in human psychology, we add
``misprimes'' to cloze questions (``Talk? Birds can [MASK]'').

We find that PLMs are easily distracted by misprimes.
These results suggest that PLMs still have a long way to go to
adequately learn human-like factual knowledge.
Author{1}{Firstname}#=%=#Nora
Author{1}{Lastname}#=%=#Kassner
Author{1}{Username}#=%=#kassner
Author{1}{Email}#=%=#kassner@cis.lmu.de
Author{1}{Affiliation}#=%=#Center for Information and Language Processing, LMU Munich
Author{2}{Firstname}#=%=#Hinrich
Author{2}{Lastname}#=%=#Sch√ºtze
Author{2}{Username}#=%=#cislmu
Author{2}{Email}#=%=#inquiries@cislmu.org
Author{2}{Affiliation}#=%=#Center for Information and Language Processing, University of Munich

==========