SubmissionNumber#=%=#454
FinalPaperTitle#=%=#MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices
ShortPaperTitle#=%=#
NumberOfPages#=%=#13
CopyrightSigned#=%=#Zhiqing Sun
JobTitle#==#
Organization#==#Carnegie Mellon University
Abstract#==#Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7  (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2  (1.5/2.1 higher than BERT_BASE).
Author{1}{Firstname}#=%=#Zhiqing
Author{1}{Lastname}#=%=#Sun
Author{1}{Username}#=%=#edwardsun0909
Author{1}{Email}#=%=#zhiqings@cs.cmu.edu
Author{1}{Affiliation}#=%=#Carnegie Mellon University
Author{2}{Firstname}#=%=#Hongkun
Author{2}{Lastname}#=%=#Yu
Author{2}{Username}#=%=#hongkuny
Author{2}{Email}#=%=#hongkuny@google.com
Author{2}{Affiliation}#=%=#Google
Author{3}{Firstname}#=%=#Xiaodan
Author{3}{Lastname}#=%=#Song
Author{3}{Username}#=%=#xiaodansong
Author{3}{Email}#=%=#xiaodansong@google.com
Author{3}{Affiliation}#=%=#Google Inc
Author{4}{Firstname}#=%=#Renjie
Author{4}{Lastname}#=%=#Liu
Author{4}{Username}#=%=#renjieliu
Author{4}{Email}#=%=#renjieliu@google.com
Author{4}{Affiliation}#=%=#Google
Author{5}{Firstname}#=%=#Yiming
Author{5}{Lastname}#=%=#Yang
Author{5}{Username}#=%=#yiming
Author{5}{Email}#=%=#yiming.yang2@gmail.com
Author{5}{Affiliation}#=%=#Carnegie Mellon University
Author{6}{Firstname}#=%=#Denny
Author{6}{Lastname}#=%=#Zhou
Author{6}{Username}#=%=#dennyzhou
Author{6}{Email}#=%=#dennyzhou@gmail.com
Author{6}{Affiliation}#=%=#Google

==========