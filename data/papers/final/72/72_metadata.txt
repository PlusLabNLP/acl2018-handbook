SubmissionNumber#=%=#72
FinalPaperTitle#=%=#Dice Loss for Data-imbalanced NLP Tasks
ShortPaperTitle#=%=#
NumberOfPages#=%=#12
CopyrightSigned#=%=#Xiaoya Li
JobTitle#==#
Organization#==#Xiaoya Li Shannon.AI
Abstract#==#Many NLP tasks such as tagging and machine reading comprehension are faced with the severe  data imbalance issue:  negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training. 
The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. 

In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is  based on the Sørensen–Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is  more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Theoretical analysis shows that this strategy narrows down the gap between the F1  score in evaluation and the dice  loss in training. 

With the proposed training objective, we observe  significant performance boost on a wide range of data imbalanced  NLP tasks. Notably, we are able to achieve  SOTA results  on CTB5,  CTB6 and UD1.4 for the part of speech tagging  task;  SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition  task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.
Author{1}{Firstname}#=%=#Xiaoya
Author{1}{Lastname}#=%=#Li
Author{1}{Username}#=%=#xiaoyli
Author{1}{Email}#=%=#xiaoya_li@shannonai.com
Author{1}{Affiliation}#=%=#Shannon.AI
Author{2}{Firstname}#=%=#Xiaofei
Author{2}{Lastname}#=%=#Sun
Author{2}{Username}#=%=#adoni1203
Author{2}{Email}#=%=#xiaofei_sun@shannonai.com
Author{2}{Affiliation}#=%=#Shannon.AI
Author{3}{Firstname}#=%=#Yuxian
Author{3}{Lastname}#=%=#Meng
Author{3}{Username}#=%=#yuxian
Author{3}{Email}#=%=#yuxian_meng@shannonai.com
Author{3}{Affiliation}#=%=#Shannon.AI
Author{4}{Firstname}#=%=#Junjun
Author{4}{Lastname}#=%=#Liang
Author{4}{Username}#=%=#liangjunjun
Author{4}{Email}#=%=#junjun_liang@shannonai.com
Author{4}{Affiliation}#=%=#ShannonAI
Author{5}{Firstname}#=%=#Fei
Author{5}{Lastname}#=%=#Wu
Author{5}{Username}#=%=#wufeizju
Author{5}{Email}#=%=#wufei@zju.edu.cn
Author{5}{Affiliation}#=%=#Zhejiang University
Author{6}{Firstname}#=%=#Jiwei
Author{6}{Lastname}#=%=#Li
Author{6}{Username}#=%=#bdlijiwei
Author{6}{Email}#=%=#jiwei_li@shannonai.com
Author{6}{Affiliation}#=%=#Shannon.AI

==========