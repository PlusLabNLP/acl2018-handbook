SubmissionNumber#=%=#3241
FinalPaperTitle#=%=#Heterogeneous Graph Transformer for Graph-to-Sequence Learning
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Shaowei Yao
JobTitle#==#
Organization#==#Peking University, Beijing, China
Abstract#==#The graph-to-sequence (Graph2Seq) learning aims to transduce graph-structured representations to word sequences for text generation. Recent studies propose various models to encode graph structure. However, most previous works ignore the indirect relations between distance nodes, or treat indirect relations and direct relations in the same way. In this paper, we propose the Heterogeneous Graph Transformer to independently model the different relations in the individual subgraphs of the original graph, including direct relations, indirect relations and multiple possible relations between nodes. Experimental results show that our model strongly outperforms the state of the art on all four standard benchmarks of AMR-to-text generation and syntax-based neural machine translation.
Author{1}{Firstname}#=%=#Shaowei
Author{1}{Lastname}#=%=#Yao
Author{1}{Username}#=%=#yaoshaowei
Author{1}{Email}#=%=#yaosw@pku.edu.cn
Author{1}{Affiliation}#=%=#Peking University
Author{2}{Firstname}#=%=#Tianming
Author{2}{Lastname}#=%=#Wang
Author{2}{Username}#=%=#soda
Author{2}{Email}#=%=#wangtm@pku.edu.cn
Author{2}{Affiliation}#=%=#Peking University
Author{3}{Firstname}#=%=#Xiaojun
Author{3}{Lastname}#=%=#Wan
Author{3}{Username}#=%=#wanxiaojun
Author{3}{Email}#=%=#wanxiaojun@pku.edu.cn
Author{3}{Affiliation}#=%=#Peking University

==========