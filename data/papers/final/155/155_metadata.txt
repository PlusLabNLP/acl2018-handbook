SubmissionNumber#=%=#155
FinalPaperTitle#=%=#Logical Natural Language Generation from Open-Domain Tables
ShortPaperTitle#=%=#
NumberOfPages#=%=#14
CopyrightSigned#=%=#Wenhu Chen
JobTitle#==#
Organization#==#University of California, Santa Barbara
Abstract#==#Neural natural language generation (NLG) models have recently shown remarkable progress in fluency and coherence. However, existing studies on neural NLG are primarily focused on surface-level realizations with limited emphasis on logical inference, an important aspect of human thinking and language. In this paper, we suggest a new NLG task where a model is tasked with generating natural language statements that can be \emph{logically entailed} by the facts in an open-domain semi-structured table. To facilitate the study of the proposed logical NLG problem, we use the existing TabFact dataset~\cite{chen2019tabfact} featured with a wide range of logical/symbolic inferences as our testbed, and propose new automatic metrics to evaluate the fidelity of generation models w.r.t.\ logical inference. The new task poses challenges to the existing monotonic generation frameworks due to the mismatch between sequence order and logical order. In our experiments, we comprehensively survey different generation architectures (LSTM, Transformer, Pre-Trained LM) trained with different algorithms  (RL, Adversarial Training, Coarse-to-Fine) on the dataset and made following observations: 1) Pre-Trained LM can significantly boost both the fluency and logical fidelity metrics, 2) RL and Adversarial Training are trading fluency for fidelity, 3) Coarse-to-Fine generation can help partially alleviate the fidelity issue while maintaining high language fluency. The code and data are available at \url{https://github.com/wenhuchen/LogicNLG}.
Author{1}{Firstname}#=%=#Wenhu
Author{1}{Lastname}#=%=#Chen
Author{1}{Username}#=%=#wenhu
Author{1}{Email}#=%=#wenhuchen@cs.ucsb.edu
Author{1}{Affiliation}#=%=#UCSB
Author{2}{Firstname}#=%=#Jianshu
Author{2}{Lastname}#=%=#Chen
Author{2}{Username}#=%=#chenjianshu
Author{2}{Email}#=%=#chenjianshu@gmail.com
Author{2}{Affiliation}#=%=#Tencent AI Lab
Author{3}{Firstname}#=%=#Yu
Author{3}{Lastname}#=%=#Su
Author{3}{Username}#=%=#suyu1989
Author{3}{Email}#=%=#qfsuyu@gmail.com
Author{3}{Affiliation}#=%=#The Ohio State University
Author{4}{Firstname}#=%=#Zhiyu
Author{4}{Lastname}#=%=#Chen
Author{4}{Username}#=%=#czyssrs
Author{4}{Email}#=%=#zhiyuchen@cs.ucsb.edu
Author{4}{Affiliation}#=%=#University of California, Santa Barbara
Author{5}{Firstname}#=%=#William Yang
Author{5}{Lastname}#=%=#Wang
Author{5}{Username}#=%=#wangwilliamyang
Author{5}{Email}#=%=#william@cs.ucsb.edu
Author{5}{Affiliation}#=%=#Unversity of California, Santa Barbara

==========