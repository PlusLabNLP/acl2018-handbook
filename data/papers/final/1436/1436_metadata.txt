SubmissionNumber#=%=#1436
FinalPaperTitle#=%=#Balancing Training for Multilingual Neural Machine Translation
ShortPaperTitle#=%=#
NumberOfPages#=%=#12
CopyrightSigned#=%=#Xinyi Wang
JobTitle#==#
Organization#==#Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213
Abstract#==#When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.
Author{1}{Firstname}#=%=#Xinyi
Author{1}{Lastname}#=%=#Wang
Author{1}{Username}#=%=#xinyiw1
Author{1}{Email}#=%=#xinyiw1@cs.cmu.edu
Author{1}{Affiliation}#=%=#Carnegie Mellon University
Author{2}{Firstname}#=%=#Yulia
Author{2}{Lastname}#=%=#Tsvetkov
Author{2}{Username}#=%=#yulia.tsvetkov
Author{2}{Email}#=%=#ytsvetko@cs.cmu.edu
Author{2}{Affiliation}#=%=#Carnegie Mellon University
Author{3}{Firstname}#=%=#Graham
Author{3}{Lastname}#=%=#Neubig
Author{3}{Username}#=%=#gneubig
Author{3}{Email}#=%=#gneubig@cs.cmu.edu
Author{3}{Affiliation}#=%=#Carnegie Mellon University

==========