SubmissionNumber#=%=#2293
FinalPaperTitle#=%=#Improving Disfluency Detection by Self-Training a Self-Attentive Model
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Paria Jamshid Lou
JobTitle#==#
Organization#==#Macquarie University, Sydney, Australia
Abstract#==#Self-attentive neural syntactic parsers using contextualized word embeddings (e.g. ELMo or BERT) currently produce state-of-the-art results in joint parsing and disfluency detection in speech transcripts. Since the contextualized word embeddings are pre-trained on a large amount of unlabeled data, using additional unlabeled data to train a neural model might seem redundant. However, we show that self-training --- a semi-supervised technique for incorporating unlabeled data --- sets a new state-of-the-art for the self-attentive parser on disfluency detection, demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized word representations. We also show that ensembling self-trained parsers provides further gains for disfluency detection.
Author{1}{Firstname}#=%=#Paria
Author{1}{Lastname}#=%=#Jamshid Lou
Author{1}{Username}#=%=#paria20
Author{1}{Email}#=%=#paria.jamshid-lou@students.mq.edu.au
Author{1}{Affiliation}#=%=#Macquarie University
Author{2}{Firstname}#=%=#Mark
Author{2}{Lastname}#=%=#Johnson
Author{2}{Username}#=%=#mark.johnson
Author{2}{Email}#=%=#Mark.MJ.Johnson@Oracle.com
Author{2}{Affiliation}#=%=#Oracle

==========