SubmissionNumber#=%=#2422
FinalPaperTitle#=%=#SimulSpeech: End-to-End Simultaneous Speech to Text Translation
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Yi Ren
JobTitle#==#
Organization#==#Zhejiang University
Abstract#==#In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-$k$ strategy for simultaneous translation. SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)). We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech. Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.
Author{1}{Firstname}#=%=#Yi
Author{1}{Lastname}#=%=#Ren
Author{1}{Username}#=%=#rayeren613
Author{1}{Email}#=%=#rayeren@zju.edu.cn
Author{1}{Affiliation}#=%=#Zhejiang University
Author{2}{Firstname}#=%=#Jinglin
Author{2}{Lastname}#=%=#Liu
Author{2}{Username}#=%=#moonintheriver
Author{2}{Email}#=%=#jinglinliu@zju.edu.cn
Author{2}{Affiliation}#=%=#Zhejiang University
Author{3}{Firstname}#=%=#Xu
Author{3}{Lastname}#=%=#Tan
Author{3}{Username}#=%=#xuta
Author{3}{Email}#=%=#xuta@microsoft.com
Author{3}{Affiliation}#=%=#Microsoft Research Asia
Author{4}{Firstname}#=%=#Chen
Author{4}{Lastname}#=%=#Zhang
Author{4}{Username}#=%=#chenzhang_zju
Author{4}{Email}#=%=#zc99@zju.edu.cn
Author{4}{Affiliation}#=%=#Zhejiang University
Author{5}{Firstname}#=%=#Tao
Author{5}{Lastname}#=%=#QIN
Author{5}{Username}#=%=#taoqin
Author{5}{Email}#=%=#taoqin@microsoft.com
Author{5}{Affiliation}#=%=#Microsoft Research Asia
Author{6}{Firstname}#=%=#Zhou
Author{6}{Lastname}#=%=#Zhao
Author{6}{Username}#=%=#cszhaozhou
Author{6}{Email}#=%=#zhaozhou@zju.edu.cn
Author{6}{Affiliation}#=%=#Zhejiang University
Author{7}{Firstname}#=%=#Tie-Yan
Author{7}{Lastname}#=%=#Liu
Author{7}{Username}#=%=#tyliu
Author{7}{Email}#=%=#tyliu@microsoft.com
Author{7}{Affiliation}#=%=#Microsoft Research Asia

==========