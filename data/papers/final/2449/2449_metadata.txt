SubmissionNumber#=%=#2449
FinalPaperTitle#=%=#Estimating Mutual Information Between Dense Word Embeddings
ShortPaperTitle#=%=#
NumberOfPages#=%=#11
CopyrightSigned#=%=#Vitalii Zhelezniak
JobTitle#==#
Organization#==#Babylon Health, 60 Sloane Avenue, London SW3 3DD, United Kingdom
Abstract#==#Word embedding-based similarity measures are currently among the top-performing methods on unsupervised semantic textual similarity (STS) tasks. Recent work has increasingly adopted a statistical view on these embeddings, with some of the top approaches being essentially various correlations (which include the famous cosine similarity). Another excellent candidate for a similarity measure is mutual information (MI), which can capture arbitrary dependencies between the variables and has a simple and intuitive expression. Unfortunately, its use in the context of dense word embeddings has so far been avoided due to difficulties with estimating MI for continuous data. In this work we go through a vast literature on estimating MI in such cases and single out the most promising methods, yielding a simple and elegant similarity measure for word embeddings. We show that mutual information is a viable alternative to correlations, gives an excellent signal that correlates well with human judgements of similarity and rivals existing state-of-the-art unsupervised methods.
Author{1}{Firstname}#=%=#Vitalii
Author{1}{Lastname}#=%=#Zhelezniak
Author{1}{Username}#=%=#vitzhe
Author{1}{Email}#=%=#vitali.zhelezniak@babylonhealth.com
Author{1}{Affiliation}#=%=#Babylon Health
Author{2}{Firstname}#=%=#Aleksandar
Author{2}{Lastname}#=%=#Savkov
Author{2}{Username}#=%=#asavkov
Author{2}{Email}#=%=#sasho.savkov@babylonhealth.com
Author{2}{Affiliation}#=%=#Babylon Partners Ltd.
Author{3}{Firstname}#=%=#Nils
Author{3}{Lastname}#=%=#Hammerla
Author{3}{Username}#=%=#nhammerla
Author{3}{Email}#=%=#nils.hammerla@babylonhealth.com
Author{3}{Affiliation}#=%=#Babylon Health

==========