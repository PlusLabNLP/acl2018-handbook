SubmissionNumber#=%=#3254
FinalPaperTitle#=%=#Sources of Transfer in Multilingual Named Entity Recognition
ShortPaperTitle#=%=#
NumberOfPages#=%=#12
CopyrightSigned#=%=#David Mueller
JobTitle#==#
Organization#==#Johns Hopkins University 3400 N. Charles Street Baltimore, MD 21218
Abstract#==#Named-entities are inherently multilingual, and annotations in any given
  language may be limited. This motivates us to consider \emph{polyglot}
  named-entity recognition (NER), where one model is trained using annotated
  data drawn from more than one language. However, a straightforward
  implementation of this simple idea does not always work in practice: naive
  training of NER models using annotated data drawn from multiple languages
  consistently underperforms models trained on monolingual data alone, despite
  having access to more training data. The starting point of this paper is a
  simple solution to this problem, in which polyglot models are
  \emph{fine-tuned} on monolingual data to consistently and significantly
  outperform their monolingual counterparts. To explain this phenomena, we
  explore the sources of multilingual transfer in polyglot NER models and
  examine the weight structure of polyglot models compared to their monolingual
  counterparts. We find that polyglot models efficiently share many parameters
  across languages and that fine-tuning may utilize a large number of those 
  parameters.
Author{1}{Firstname}#=%=#David
Author{1}{Lastname}#=%=#Mueller
Author{1}{Username}#=%=#dmueller
Author{1}{Email}#=%=#daveandym@gmail.com
Author{1}{Affiliation}#=%=#The Johns Hopkins University
Author{2}{Firstname}#=%=#Nicholas
Author{2}{Lastname}#=%=#Andrews
Author{2}{Username}#=%=#noa
Author{2}{Email}#=%=#noa@jhu.edu
Author{2}{Affiliation}#=%=#Johns Hopkins University
Author{3}{Firstname}#=%=#Mark
Author{3}{Lastname}#=%=#Dredze
Author{3}{Username}#=%=#mdredze
Author{3}{Email}#=%=#mdredze@cs.jhu.edu
Author{3}{Affiliation}#=%=#Johns Hopkins University

==========