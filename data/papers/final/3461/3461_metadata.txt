SubmissionNumber#=%=#3461
FinalPaperTitle#=%=#Tagged Back-translation Revisited: Why Does It Really Work?
ShortPaperTitle#=%=#
NumberOfPages#=%=#8
CopyrightSigned#=%=#Benjamin Marie
JobTitle#==#Researcher
Organization#==#National Institute of Information and Communications Technology, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan
Abstract#==#In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts. Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown.
Author{1}{Firstname}#=%=#Benjamin
Author{1}{Lastname}#=%=#Marie
Author{1}{Username}#=%=#benjamin.marie
Author{1}{Email}#=%=#bmarie@nict.go.jp
Author{1}{Affiliation}#=%=#NICT
Author{2}{Firstname}#=%=#Raphael
Author{2}{Lastname}#=%=#Rubino
Author{2}{Username}#=%=#rafa
Author{2}{Email}#=%=#raphael.rubino@gmail.com
Author{2}{Affiliation}#=%=#NICT
Author{3}{Firstname}#=%=#Atsushi
Author{3}{Lastname}#=%=#Fujita
Author{3}{Username}#=%=#fujita
Author{3}{Email}#=%=#fujita@paraphrasing.org
Author{3}{Affiliation}#=%=#National Institute of Information and Communications Technology

==========