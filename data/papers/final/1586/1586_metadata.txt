SubmissionNumber#=%=#1586
FinalPaperTitle#=%=#Character-Level Translation with Self-attention
ShortPaperTitle#=%=#
NumberOfPages#=%=#14
CopyrightSigned#=%=#Nikola Nikolov
JobTitle#==#
Organization#==#Institute of Neuroinformatics, UZH / ETH Zurich, Winterthurerstrasse 190, CH-8057 Zurich Switzerland
Abstract#==#We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese). Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.
Author{1}{Firstname}#=%=#Yingqiang
Author{1}{Lastname}#=%=#Gao
Author{1}{Username}#=%=#charizardacademy
Author{1}{Email}#=%=#yingqiang.gao@in.tum.de
Author{1}{Affiliation}#=%=#Technical University of Munich, Germnay
Author{2}{Firstname}#=%=#Nikola I.
Author{2}{Lastname}#=%=#Nikolov
Author{2}{Username}#=%=#niniko
Author{2}{Email}#=%=#niniko@ini.ethz.ch
Author{2}{Affiliation}#=%=#University of Zurich and ETH Zurich
Author{3}{Firstname}#=%=#Yuhuang
Author{3}{Lastname}#=%=#Hu
Author{3}{Username}#=%=#yuhuang.hu
Author{3}{Email}#=%=#yuhuang.hu@ini.uzh.ch
Author{3}{Affiliation}#=%=#University of Zurich and ETH Zurich
Author{4}{Firstname}#=%=#Richard H.R.
Author{4}{Lastname}#=%=#Hahnloser
Author{4}{Username}#=%=#rhahnloser
Author{4}{Email}#=%=#rich@ini.ethz.ch
Author{4}{Affiliation}#=%=#ETH Zurich

==========