SubmissionNumber#=%=#204
FinalPaperTitle#=%=#Bridging Anaphora Resolution as Question Answering
ShortPaperTitle#=%=#
NumberOfPages#=%=#11
CopyrightSigned#=%=#Yufang Hou
JobTitle#==#
Organization#==#IBM Research
Abstract#==#Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al.,
2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that
the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of “quasi-bridging” training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (Ro ̈siger, 2018)).
Author{1}{Firstname}#=%=#Yufang
Author{1}{Lastname}#=%=#Hou
Author{1}{Username}#=%=#yufang
Author{1}{Email}#=%=#bnuxiaofang@gmail.com
Author{1}{Affiliation}#=%=#IBM Research

==========