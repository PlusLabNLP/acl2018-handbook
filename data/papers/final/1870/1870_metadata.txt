SubmissionNumber#=%=#1870
FinalPaperTitle#=%=#Posterior Calibrated Training on Sentence Classification Tasks
ShortPaperTitle#=%=#
NumberOfPages#=%=#8
CopyrightSigned#=%=#Taehee Jung
JobTitle#==#
Organization#==#University of Pittsburgh
Abstract#==#Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability. In many settings however, the quality of posterior probability itself (e.g., 65% chance having diabetes), gives more reliable information than the final predicted class alone. When these methods are shown to be poorly calibrated, most fixes to date have relied on posterior calibration, which rescales the predicted probabilities but often has little impact on final classifications. Here we propose an end-to-end training procedure called posterior calibrated (PosCal) training that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.We show that PosCal not only helps reduce the calibration error but also improve task performance by penalizing drops in performance of both objectives. Our PosCal achieves about 2.5% of task performance gain and 16.1% of calibration error reduction on GLUE (Wang et al., 2018) compared to the baseline. We achieved the comparable task performance with 13.2% calibration error reduction on xSLUE (Kang and Hovy, 2019), but not outperforming the two-stage calibration baseline. PosCal training can be easily extendable to any types of classification tasks as a form of regularization term. Also, PosCal has the advantage that it incrementally tracks needed statistics for the calibration objective during the training process, making efficient use of large training sets.
Author{1}{Firstname}#=%=#Taehee
Author{1}{Lastname}#=%=#Jung
Author{1}{Username}#=%=#theejung
Author{1}{Email}#=%=#taj41@pitt.edu
Author{1}{Affiliation}#=%=#University of Pittsburgh
Author{2}{Firstname}#=%=#Dongyeop
Author{2}{Lastname}#=%=#Kang
Author{2}{Username}#=%=#dongyeok
Author{2}{Email}#=%=#dykang85@gmail.com
Author{2}{Affiliation}#=%=#University of California, Berkeley
Author{3}{Firstname}#=%=#Hua
Author{3}{Lastname}#=%=#Cheng
Author{3}{Username}#=%=#huacheng
Author{3}{Email}#=%=#hcheng@mmm.com
Author{3}{Affiliation}#=%=#3M
Author{4}{Firstname}#=%=#Lucas
Author{4}{Lastname}#=%=#Mentch
Author{4}{Username}#=%=#lkm31
Author{4}{Email}#=%=#lkm31@pitt.edu
Author{4}{Affiliation}#=%=#University of Pittsburgh
Author{5}{Firstname}#=%=#Thomas
Author{5}{Lastname}#=%=#Schaaf
Author{5}{Username}#=%=#tschaaf
Author{5}{Email}#=%=#tschaaf@reihl-schaaf.net
Author{5}{Affiliation}#=%=#3M | M*Modal

==========