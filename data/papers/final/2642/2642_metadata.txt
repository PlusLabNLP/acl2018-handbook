SubmissionNumber#=%=#2642
FinalPaperTitle#=%=#Towards Transparent and Explainable Attention Models
ShortPaperTitle#=%=#
NumberOfPages#=%=#11
CopyrightSigned#=%=#Akash Kumar Mohankumar
JobTitle#==#Dual Degree Student
Organization#==#Indian Institute of Technology Madras
Abstract#==#Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model's predictions. Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model's prediction. They can be considered a plausible explanation if they provide a human-understandable justification for the model's predictions. In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model's predictions. We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model's predictions. Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model's predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions. To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse. We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model's predictions (iii) correlate better with gradient-based attribution methods. Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model's predictions. Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention
Author{1}{Firstname}#=%=#Akash Kumar
Author{1}{Lastname}#=%=#Mohankumar
Author{1}{Username}#=%=#akashkm99
Author{1}{Email}#=%=#makashkumar99@gmail.com
Author{1}{Affiliation}#=%=#Indian Institute of Technology Madras
Author{2}{Firstname}#=%=#Preksha
Author{2}{Lastname}#=%=#Nema
Author{2}{Username}#=%=#preksha
Author{2}{Email}#=%=#cs15d201@smail.iitm.ac.in
Author{2}{Affiliation}#=%=#Indian Institute of Technology Madras
Author{3}{Firstname}#=%=#Sharan
Author{3}{Lastname}#=%=#Narasimhan
Author{3}{Username}#=%=#sharan.narasimhan
Author{3}{Email}#=%=#sharan.n21@gmail.com
Author{3}{Affiliation}#=%=#Indian Institute of Technology, Madras
Author{4}{Firstname}#=%=#Mitesh M.
Author{4}{Lastname}#=%=#Khapra
Author{4}{Username}#=%=#miteshk
Author{4}{Email}#=%=#miteshk@cse.iitm.ac.in
Author{4}{Affiliation}#=%=#Indian Institute of Technology Madras
Author{5}{Firstname}#=%=#Balaji Vasan
Author{5}{Lastname}#=%=#Srinivasan
Author{5}{Username}#=%=#balajivasan
Author{5}{Email}#=%=#balajivasan.srinivasan@gmail.com
Author{5}{Affiliation}#=%=#Adobe Research, India
Author{6}{Firstname}#=%=#Balaraman
Author{6}{Lastname}#=%=#Ravindran
Author{6}{Username}#=%=#ravib
Author{6}{Email}#=%=#ravi@cse.iitm.ac.in
Author{6}{Affiliation}#=%=#Indian Institute of Technology Madras

==========