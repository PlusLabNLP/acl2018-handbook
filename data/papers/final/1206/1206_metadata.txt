SubmissionNumber#=%=#1206
FinalPaperTitle#=%=#Multiscale Collaborative Deep Models for Neural Machine Translation
ShortPaperTitle#=%=#
NumberOfPages#=%=#13
CopyrightSigned#=%=#Xiangpeng Wei
JobTitle#==#PhD Candidate
Organization#==#Institute of Information Engineering, Chinese Academy of Sciences.  No. 89, Minzhuang Road, Haidian District, Beijing, China. 100093.
Abstract#==#Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2$\sim$+3.1 BLEU points. In addition, our deep MSC achieves a BLEU score of 30.56 on WMT14 English-to-German task that significantly outperforms state-of-the-art deep NMT models. We have included the source code in supplementary materials.
Author{1}{Firstname}#=%=#Xiangpeng
Author{1}{Lastname}#=%=#Wei
Author{1}{Username}#=%=#pemy_wei
Author{1}{Email}#=%=#weixiangpeng@iie.ac.cn
Author{1}{Affiliation}#=%=#Institute of Information Engineering, Chinese Academy of Sciences
Author{2}{Firstname}#=%=#Heng
Author{2}{Lastname}#=%=#Yu
Author{2}{Username}#=%=#sameyh
Author{2}{Email}#=%=#yuheng@ict.ac.cn
Author{2}{Affiliation}#=%=#Alibaba
Author{3}{Firstname}#=%=#Yue
Author{3}{Lastname}#=%=#Hu
Author{3}{Username}#=%=#huhuyue
Author{3}{Email}#=%=#huyue@iie.ac.cn
Author{3}{Affiliation}#=%=#Institute of Information Engineering, Chinese Academy of Sciences
Author{4}{Firstname}#=%=#Yue
Author{4}{Lastname}#=%=#Zhang
Author{4}{Username}#=%=#frcchang
Author{4}{Email}#=%=#yue.zhang@wias.org.cn
Author{4}{Affiliation}#=%=#Westlake University
Author{5}{Firstname}#=%=#Rongxiang
Author{5}{Lastname}#=%=#Weng
Author{5}{Username}#=%=#wengrx
Author{5}{Email}#=%=#wengrongxiang@gmail.com
Author{5}{Affiliation}#=%=#Alibaba DAMO Academy
Author{6}{Firstname}#=%=#Weihua
Author{6}{Lastname}#=%=#Luo
Author{6}{Username}#=%=#luoweihua
Author{6}{Email}#=%=#weihua.luowh@alibaba-inc.com
Author{6}{Affiliation}#=%=#Alibaba Group

==========