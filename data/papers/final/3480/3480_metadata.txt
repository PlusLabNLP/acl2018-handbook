SubmissionNumber#=%=#3480
FinalPaperTitle#=%=#Multi-Hypothesis Machine Translation Evaluation
ShortPaperTitle#=%=#
NumberOfPages#=%=#15
CopyrightSigned#=%=#Marina Fomicheva
JobTitle#==#
Organization#==#Department of Computer Science, University of Sheffield, Western Bank Sheffield S10 2TN UK
Abstract#==#Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing
problem. One of the main challenges is the fact that multiple outputs can be equally valid. Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references. The latter has been shown to significantly improve the performance of evaluation metrics. However, collecting multiple references is expensive and in practice a single reference is generally used. In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether. We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15%.
Author{1}{Firstname}#=%=#Marina
Author{1}{Lastname}#=%=#Fomicheva
Author{1}{Username}#=%=#marina.fomicheva
Author{1}{Email}#=%=#m.fomicheva@sheffield.ac.uk
Author{1}{Affiliation}#=%=#University of Sheffield
Author{2}{Firstname}#=%=#Lucia
Author{2}{Lastname}#=%=#Specia
Author{2}{Username}#=%=#l.specia
Author{2}{Email}#=%=#l.specia@imperial.ac.uk
Author{2}{Affiliation}#=%=#Imperial College London
Author{3}{Firstname}#=%=#Francisco
Author{3}{Lastname}#=%=#Guzm√°n
Author{3}{Username}#=%=#fguzman
Author{3}{Email}#=%=#fguzman@fb.com
Author{3}{Affiliation}#=%=#Facebook

==========