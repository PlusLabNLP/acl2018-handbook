SubmissionNumber#=%=#1033
FinalPaperTitle#=%=#Variational Neural Machine Translation with Normalizing Flows
ShortPaperTitle#=%=#
NumberOfPages#=%=#7
CopyrightSigned#=%=#Hendra Setiawan
JobTitle#==#
Organization#==#Apple
Abstract#==#Variational Neural Machine Translation (VNMT) is an attractive framework for modeling the generation of target translations, conditioned not only on the source sentence but also on some latent random variables. The latent variable modeling may introduce useful statistical dependencies that can improve translation accuracy. Unfortunately, learning informative latent variables is non-trivial, as the latent space can be prohibitively large, and the latent codes are prone to be ignored by many translation models at training time. Previous works impose strong assumptions on the distribution of the latent code and limit the choice of the NMT architecture. In this paper, we propose to apply the VNMT framework  to the state-of-the-art Transformer and introduce a more flexible approximate posterior based on normalizing flows. We demonstrate the efficacy of our proposal under both in-domain and out-of-domain conditions, significantly outperforming strong baselines.
Author{1}{Firstname}#=%=#Hendra
Author{1}{Lastname}#=%=#Setiawan
Author{1}{Username}#=%=#hendra.setiawan
Author{1}{Email}#=%=#hendra.setiawan@gmail.com
Author{1}{Affiliation}#=%=#Apple Inc.
Author{2}{Firstname}#=%=#Matthias
Author{2}{Lastname}#=%=#Sperber
Author{2}{Username}#=%=#matthias.sperber
Author{2}{Email}#=%=#sperber@apple.com
Author{2}{Affiliation}#=%=#Apple
Author{3}{Firstname}#=%=#Udhyakumar
Author{3}{Lastname}#=%=#Nallasamy
Author{3}{Username}#=%=#udhay.ece
Author{3}{Email}#=%=#udhay@apple.com
Author{3}{Affiliation}#=%=#Apple Inc
Author{4}{Firstname}#=%=#Matthias
Author{4}{Lastname}#=%=#Paulik
Author{4}{Username}#=%=#matthias.paulik
Author{4}{Email}#=%=#mpaulik@apple.com
Author{4}{Affiliation}#=%=#Apple

==========