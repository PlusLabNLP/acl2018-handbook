SubmissionNumber#=%=#744
FinalPaperTitle#=%=#Social Bias Frames: Reasoning about Social and Power Implications of Language
ShortPaperTitle#=%=#
NumberOfPages#=%=#14
CopyrightSigned#=%=#Maarten Sap
JobTitle#==#
Organization#==#Paul G. Allen School for Computer Science & Engineering, University of Washington, 185 E Stevens Way NE, Seattle, WA 98195
Abstract#==#Warning: this paper contains content that may be offensive or upsetting. 

Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people's judgments about others. For example, given a statement that "we shouldn't lower our standards to hire more women," most listeners will infer the implicature intended by the speaker - that "women (candidates) are less qualified." Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language.

We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups.

We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.
Author{1}{Firstname}#=%=#Maarten
Author{1}{Lastname}#=%=#Sap
Author{1}{Username}#=%=#maartensap93
Author{1}{Email}#=%=#msap@cs.washington.edu
Author{1}{Affiliation}#=%=#University of Washington
Author{2}{Firstname}#=%=#Saadia
Author{2}{Lastname}#=%=#Gabriel
Author{2}{Username}#=%=#skgabriel
Author{2}{Email}#=%=#skgabrie@cs.washington.edu
Author{2}{Affiliation}#=%=#University of Washington
Author{3}{Firstname}#=%=#Lianhui
Author{3}{Lastname}#=%=#Qin
Author{3}{Username}#=%=#lianhuiqin
Author{3}{Email}#=%=#lianhuiq@cs.washington.edu
Author{3}{Affiliation}#=%=#University of Washington
Author{4}{Firstname}#=%=#Dan
Author{4}{Lastname}#=%=#Jurafsky
Author{4}{Username}#=%=#jurafsky
Author{4}{Email}#=%=#jurafsky@stanford.edu
Author{4}{Affiliation}#=%=#Stanford University
Author{5}{Firstname}#=%=#Noah A.
Author{5}{Lastname}#=%=#Smith
Author{5}{Username}#=%=#nasmith
Author{5}{Email}#=%=#nasmith@cs.washington.edu
Author{5}{Affiliation}#=%=#University of Washington
Author{6}{Firstname}#=%=#Yejin
Author{6}{Lastname}#=%=#Choi
Author{6}{Username}#=%=#ychoi
Author{6}{Email}#=%=#yejin@cs.washington.edu
Author{6}{Affiliation}#=%=#University of Washington

==========