SubmissionNumber#=%=#576
FinalPaperTitle#=%=#Modeling Prompt Adherence in Student Essays
ShortPaperTitle#=%=#Modeling Prompt Adherence in Student Essays
NumberOfPages#=%=#10
CopyrightSigned#=%=#Isaac Persing
JobTitle#==#
Organization#==#Computer Science Department
The University of Texas at Dallas
800 W. Campbell Rd., MS EC31
Richardson, TX 75080, USA
Abstract#==#Recently, researchers have begun exploring methods of scoring student essays
with respect to particular dimensions of quality such as coherence, technical
errors, and prompt adherence. The work on modeling prompt adherence, however,
has been focused on whether individual sentences adhere to the prompt. We
present a new annotated corpus of essay-level prompt adherence scores and
propose a learning-based approach to scoring essays along the prompt adherence
dimension. Our approach makes use of novel features and significantly
outperforms a baseline prompt adherence scoring system yielding relative error
reductions of up to 21.4%.
Author{1}{Firstname}#=%=#Isaac
Author{1}{Lastname}#=%=#Persing
Author{1}{Email}#=%=#persingq@hlt.utdallas.edu
Author{1}{Affiliation}#=%=#University of Texas at Dallas
Author{2}{Firstname}#=%=#Vincent
Author{2}{Lastname}#=%=#Ng
Author{2}{Email}#=%=#vince@hlt.utdallas.edu
Author{2}{Affiliation}#=%=#University of Texas at Dallas

==========