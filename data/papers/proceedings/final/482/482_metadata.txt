SubmissionNumber#=%=#482
FinalPaperTitle#=%=#Unsupervised Dependency Parsing with Transferring Distribution via Parallel Guidance and Entropy Regularization
ShortPaperTitle#=%=#Unsupervised Dependency Parsing with Transferring Distribution via Parallel Guidance and Entropy Regularization
NumberOfPages#=%=#12
CopyrightSigned#=%=#Xuezhe Ma
JobTitle#==#
Organization#==#Department of Linguistics, University of Washington
Seattle, WA 98195, USA
Abstract#==#We present a novel approach for inducing unsupervised dependency parsers for
languages that have no labeled training data, but have translated text in a
resource-rich language. We train probabilistic parsing models for resource-poor
languages by transferring cross-lingual knowledge from resource-rich language
with entropy regularization. Our method can be used as a purely monolingual
dependency parser, requiring no human translations for the test data, thus
making it applicable to a wide range of resource-poor languages. We perform
experiments on three Data sets â€” Version 1.0 and version 2.0 of Google
Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across
ten languages. We obtain state-of-the art performance of all the three data
sets when compared with previously studied unsupervised and projected parsing
systems.
Author{1}{Firstname}#=%=#Xuezhe
Author{1}{Lastname}#=%=#Ma
Author{1}{Email}#=%=#xzma@uw.edu
Author{1}{Affiliation}#=%=#Department of Linguistics, University of Washington
Author{2}{Firstname}#=%=#Fei
Author{2}{Lastname}#=%=#Xia
Author{2}{Email}#=%=#fxia@uw.edu
Author{2}{Affiliation}#=%=#Department of Linguistics, University of Washington

==========