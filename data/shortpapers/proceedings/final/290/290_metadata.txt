SubmissionNumber#=%=#290
FinalPaperTitle#=%=#Experiments with crowdsourced re-annotation of a POS tagging data set
ShortPaperTitle#=%=#Experiments with crowdsourced re-annotation of a POS tagging data set
NumberOfPages#=%=#6
CopyrightSigned#=%=#Dirk Hovy
JobTitle#==#
Organization#==#
Abstract#==#Crowdsourcing lets us collect multiple annotations for an item from several
annotators. Typically, these are annotations for non-sequential classification
tasks. While there has been some work on crowdsourcing named entity
annotations, researchers have largely assumed that syntactic tasks such as
part-of-speech (POS) tagging cannot be crowdsourced. This paper shows that
workers can actually annotate sequential data almost as well as experts. Fur-
ther, we show that the models learned from crowdsourced annotations fare as
well as the models learned from expert annotations in downstream tasks.
Author{1}{Firstname}#=%=#Dirk
Author{1}{Lastname}#=%=#Hovy
Author{1}{Email}#=%=#mail@dirkhovy.com
Author{1}{Affiliation}#=%=#Center for Language Technology, University of Copenhagen
Author{2}{Firstname}#=%=#Barbara
Author{2}{Lastname}#=%=#Plank
Author{2}{Email}#=%=#bplank@gmail.com
Author{2}{Affiliation}#=%=#University of Copenhagen
Author{3}{Firstname}#=%=#Anders
Author{3}{Lastname}#=%=#SÃ¸gaard
Author{3}{Email}#=%=#soegaard@hum.ku.dk
Author{3}{Affiliation}#=%=#University of Copenhagen

==========