%
% File acl2014.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage[dvips]{color}
\usepackage{paralist} 
\usepackage{graphicx} 
\usepackage{xcolor} 

\newcommand{\changedea}[1]{\textcolor{blue}{ (ea) #1}} 

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{On WordNet Semantic Classes and Dependency Parsing}

\author{Kepa Bengoetxea\dag, Eneko Agirre\dag, Joakim Nivre\ddag, \\ 
{\bf Yue Zhang*, Koldo Gojenola\dag }\\
 \dag University of the Basque Country UPV/EHU / IXA NLP Group \\
  \ddag Uppsala University / Department of Linguistics and Philology \\
  $\ast$ Singapore University of Technology and Design \\
  {\tt \normalsize kepa.bengoetxea@ehu.es, e.agirre@ehu.es, } \\
  {\tt \normalsize joakim.nivre@lingfil.uu.se, yue\_zhang@sutd.edu.sg, } \\
  {\tt \normalsize koldo.gojenola@ehu.es} \\}
%\\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This paper presents experiments with WordNet semantic classes to improve dependency parsing. We study
the effect of semantic classes in three dependency parsers, using two types of constituency-to-dependency conversions of the English Penn Treebank. Overall, we can say that the improvements are small and not significant using automatic POS tags, contrary to previously published results using gold POS tags \cite{agirre-EtAl:2011:ACL-HLT2011}.
In addition, we explore parser combinations, showing that the semantically enhanced parsers yield a small significant gain only on the more semantically oriented LTH treebank conversion.
\end{abstract}

\section{Introduction}
This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English. Whether semantics improve parsing is one interesting research topic
both on parsing and lexical semantics. 
%Semantic information from
%both manually constructed repositories and automatically acquired
%statistics have been applied to parsing. 
Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such as WordNet or similar ontologies \cite{agirre-baldwin-martinez:2008:ACLMain,agirre-EtAl:2011:ACL-HLT2011,Sanae-Bond-Oepen-Takaaki}, and  systems using dynamic semantic clusters automatically acquired from corpora \cite{koo-carreras-collins:2008:ACLMain,suzuki-EtAl:2009:EMNLP}.


%THIS INTRODUCTION NEEDS TO BE REWRITTEN. WE CAN LEAVE THIS PART AT THE MOMENT. AFTER DOING AN ANALYSIS OF THE ERRORS AND IMPROVEMENTS, WE COULD ADD ONE OR TWO EXAMPLES WHERE SEMANTIC INFORMATION CAN HELP PARSING. TYPICALLY MOST WORKS CITE THE EXAMPLES OF THE MAN AND THE TELESCOPE OR SIMILAR. PERHAPS WE WILL FIND A NICER EXAMPLE THAT IS MORE RELATED TO THE DATA (IN OUR CASE WSJ)

%As the original Penn Treebank is annotated using constituency trees, there have been several approaches that convert constituents to dependencies, as Penn2Malt, LTH or Stanford dependencies. We will experiment with a standard coarse-grained transformation using the Penn2Malt tool (12 dependency tags), and a more fine-grained conversion using LTH, containing 42 more specialized dependency tags  (see subsection \ref{conversions}).

%YUE:
%-----------------------
%[paragraph 4] On the other
%hand, so far little work has been presented on using lexical semantics
%such as selectional preferences to improve parsing successfully. It
%has been observed that semantic information can help improve parsing,
%but not consistently [mckinley 2012 ...]
% Our conclusion is...
%-----------------------

Our main objective will be to determine whether static semantic knowledge can help parsing. We will apply different types of semantic
information to three dependency parsers. Specifically, we will test the following questions:

%\begin{itemize}
\begin{compactitem}
\item Does semantic information in WordNet help dependency parsing? \newcite{agirre-EtAl:2011:ACL-HLT2011} found improvements  in dependency parsing using  MaltParser on gold POS tags. In this work, we will investigate the effect of semantic information using  predicted POS tags.
\item Is the type of semantic information related to the type of parser? We will test   
 three different parsers representative of successful paradigms in dependency parsing. 
\item How does the semantic information relate to the style of dependency annotation? Most  experiments for English were evaluated on the Penn2Malt conversion of the constituency-based Penn Treebank. We will also examine the LTH conversion, with richer structure and an extended set of dependency labels.
% containing 42 more specialized dependency tags  (see subsection \ref{conversions}). %, that will serve to compare the contribution of semantic information to each type of representation.
\item How does WordNet compare to automatically obtained information? For the sake of comparison, we will also perform the experiments using syntactic/semantic clusters automatically acquired from corpora. 
%We will also try to establish if the gains obtained by using semantic repositories are complementary or orthogonal to automatically acquired semantic information.
\item Does parser combination benefit from semantic information? 
Different parsers can use semantic information in diverse ways. 
For example, while MaltParser can use the semantic information in local contexts, MST can incorporate them in global contexts. We will run parser combination experiments with and without semantic information, to determine whether it is useful in the combined parsers. 
\end{compactitem}
%\end{itemize}

After introducing related work in section \ref{related}, section \ref{framework}
 describes the treebank conversions, parsers and semantic features. Section \ref{results} presents
the results and section \ref{conclusions} draws the main conclusions.
%Section \ref{analysis} analyzes the results, 
%trying to characterize the improvements using each kind of semantic information,
%We present the main conclusions in section \ref{conclusions}.

\section{Related work}\label{related}
Broadly speaking, we can classify the attempts to add external knowledge to a parser in two sets: using large semantic repositories such as WordNet and approaches that use information automatically acquired from corpora. 
In the first group, \newcite{agirre-baldwin-martinez:2008:ACLMain} trained two state-of-the-art constituency-based statistical parsers \cite{Charniak:2000:MP:974305.974323,DBLP:journals/coling/Bikel04} on semantically-enriched input, substituting content words with their semantic 
classes, trying to overcome the limitations of lexicalized approaches to parsing \cite{Collins:2003:HSM:1105703.1105706} where related words, like \emph{scissors} and \emph{knife}, cannot be generalized.
%, and allowing the incorporation of lexical semantic information into the parser. 
%They tested the two parsers in both a full parsing and a PP attachment context. 
The results showed a signiÔ¨Åcant improvement, giving the first results over both WordNet and the Penn Treebank (PTB) to show that semantics helps parsing. Later, \newcite{agirre-EtAl:2011:ACL-HLT2011} successfully introduced WordNet classes in a dependency parser, obtaining improvements on the full PTB using gold POS tags, trying different combinations of   semantic classes. 
%In their experiments they used gold POS tags.
\newcite{MacKinlay-Dridan-McCarthy-Baldwin} investigate the addition of semantic
annotations in the form of word sense hypernyms,
% as an extra source of information
 in HPSG parse ranking,
% for the English Resource Grammar. 
%The annotations
%are coarse semantic categories 
%from a distributional thesaurus, assigned
%either heuristically or by a pre-trained tagger.
%They used two test corpora in different
%domains with various sources of training data.
reducing error rate in dependency F-score
by 1\%, while some methods
produce substantial decreases in performance.
\newcite{Sanae-Bond-Oepen-Takaaki}
% investigated the use of semantic information in parse selection, 
showed that fully disambiguated sense-based features smoothed
using ontological information are effective for parse selection. 
%Training and testing
%was undertaken using definition and example sentences taken from a Japanese dictionary
%corpus, manually annotated with senses. A model employing
%both syntactic and semantic information provides better parse selection accuracy than
%a model using only syntactic features.

%Looking at the experiments using automatically acquired information, 
On the second group, \newcite{koo-carreras-collins:2008:ACLMain} presented a semisupervised method for training dependency parsers, introducing features that incorporate word clusters automatically acquired from a large unannotated corpus. The clusters include strongly semantic associations like \{apple, pear\} or \{Apple, IBM\} and also syntactic clusters like \{of, in\}. They demonstrated its effectiveness in  dependency parsing experiments on the PTB and the Prague Dependency Treebank.
%, showing that the cluster-based features yield substantial gains in performance across a wide range of conditions. 
\newcite{suzuki-EtAl:2009:EMNLP}, \newcite{sagaegordon2009} and \newcite{candito-seddah:2010:SPMRL} also experiment with the same cluster method.
%\newcite{suzuki-EtAl:2009:EMNLP} also experiment with the same cluster method combined with a semi-supervised learning approach.
%\newcite{candito-seddah:2010:SPMRL} compared the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French. 
%The architectures are based on PCFGs with latent variables, graph-based dependency parsing and transition-based dependency parsing, respectively. They studied the influence of three types of lexical information: lemmas, morphological features, and word clusters. The results showed that word clusters had a positive effect primarily on the latent variable parser. 
%\newcite{sagaegordon2009} experimented  using syntactic clusters  grouping
%words according to their unlexicalized syntactic contexts. 
Recently, \newcite{tackstrom-mcdonald-uszkoreit:2012:NAACL-HLT} tested the incorporation of 
cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters.

%\newcite{ciaramita-attardi:2007:IWPT2007} report experiments showing that adding semantic features extracted by a named entity tagger (such as PERSON or MONEY) improved the accuracy of a dependency parser, yielding a 5.8\% relative error reduction on the full Penn Treebank.
%\section{Objectives}\label{objectives}



\section{Experimental Framework}\label{framework}
In this section we will briefly describe the PTB-based datasets (subsection \ref{conversions}), followed by the data-driven parsers used for the experiments  (subsection \ref{parsers}). Finally, we will describe the different types of semantic representation that were used. 



\subsection{Treebank conversions}\label{conversions}


\emph{Penn2Malt}\footnote{http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html} performs a simple and direct conversion from the constituency-based PTB to a dependency treebank. It obtains projective trees and has been used in several works, which allows us to  compare our results with related experiments \cite{koo-carreras-collins:2008:ACLMain,suzuki-EtAl:2009:EMNLP,koo-collins:2010:ACL}. We extracted dependencies using standard head rules \cite{Yamada-and-Matsumoto}, and a reduced set of 12 general dependency tags.%Overall, the Penn2Malt conversion obtains a reduced set of 12 general dependency tags. %, which does not account for several types of phenomena that are in fact encoded in the new Penn Treebank II version. For instance, Penn2Malt does not distinguish between temporal and locative adjuncts, but labels them all as verb modifiers.


\emph{LTH}\footnote{http://nlp.cs.lth.se/software/treebank\_converter} \cite{johansson2007a} presents a conversion better suited for semantic processing, with a richer structure and a more fine-grained set of dependency labels (42 different dependency labels), including links
to handle long-distance phenomena, giving a 6.17\% of  nonprojective sentences. The results from parsing the LTH output are lower than those for Penn2Malt conversions.
%such as wh-movement and topicalization. 

%2,459 out of 39,832 on the training set, that is, 6.17\% of the sentences.

%the LTH converter obtains a richer and more semantically oriented set of dependency arcs, distinguishing, for example, different types of arcs and long-distance phenomena. 



\subsection{Parsers}\label{parsers}
We have made use of three parsers representative of successful paradigms in dependency parsing. 
%MaltParser is a representative of deterministic greedy transition-based parsers, MST uses global graph-based parsing and ZPar applies a transition-based global parsing approach using a rich feature specification.


\emph{MaltParser} \cite{niv2007a} is a deterministic transition-based dependency parser 
%that has been successfully applied to typologically different languages and treebanks. 
%There are several variants of the base parser, and we will use one of its standard versions (version 1.4). 
that obtains a dependency tree in linear-time in a single pass over the input using a stack of partially analyzed items and the remaining input sequence,
%To determine the best action at each step, the parser 
by means of history-based feature models. 
%MaltParser easily allows the addition of semantic features to its input tabular format. 
We added two features that inspect the semantic feature at the top of the stack and the next input token.

%KEPA: CAN YOU WRITE SOMETHING ON THE MALT ALGORITHMS USED?

\emph{MST}\footnote{http://mstparser.sourceforge.net} represents global, exhaustive graph-based parsing \cite{mcdonald2005,mcdonald2006} that finds the highest scoring directed spanning tree in a graph. 
%To learn arc scores, it uses large-margin structured learning algorithms
%, which optimize the parameters of the model to maximize the score margin between the correct dependency graph and all incorrect dependency graphs for every sentence in a training set. 
The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, 
%and not just over single arc attachments,
 in contrast to the local but richer contexts used by transition-based parsers.
%We use the freely available version of MST\footnote{http://mstparser.sourceforge.net}. %There are different parsing algorithms, the non-projective Maximum Spanning Tree algorithm \cite{ChuandLiu1965,Edmonds1967} and the projective version using Eisner's algorithm \cite{Eisner1996}. 
The system can be trained using first or second order models. 
%The first order model calculates a score of the entire dependency graph using a global learning of a score on all arcs belonging to the graph, while the second order model includes pairs of adjacent arcs, using Eisner's \shortcite{Eisner1996} algorithm and postprocessing. 
The second order projective algorithm performed best on both conversions, and we used it in the rest of the evaluations. 
We modified the system in order to add  semantic features, combining them with wordforms and POS tags, on the parent and child nodes of each arc. % (see additional documentation on experimental settings).

\emph{ZPar}\footnote{www.sourceforge.net/projects/zpar} \cite{zhang-clark:2008,zhang-nivre:2011:ACL-HLT2011} performs transition-based dependency parsing with a stack of partial analysis and a queue of remaining inputs. In contrast to MaltParser (local model and greedy deterministic search) ZPar applies global discriminative learning and beam search. 
%Similar to MaltParser, the parser obtains a dependency tree by a linear time pass over the input sentence. However, at each step, the {\it K} best hypotheses are kept, and error propagation is reduced when $K>1$. 
%In addition, a global model accommodates more complex parsing models with non-local features. 
We extend the feature set of ZPar to include semantic features. Each set of semantic information is represented by two atomic feature templates, associated with the top of the stack and the head of the queue, respectively. 
ZPar was directly trained on the Penn2Malt conversion, while we applied the pseudo-projective transformation \cite{nil2007} on LTH, in order to deal with non-projective arcs.

\subsection{Semantic information}
Our aim was to experiment with different types of WordNet-related semantic information. For comparison with automatically acquired information, we will also experiment with bit clusters. 


\begin{table}[t]
\centering
\scriptsize
\begin{tabular}{ |c||c||c||l||l||} \hline 
& \multicolumn{1}{c||}{{\bf Base}} & \multicolumn{1}{c||}{{\bf WordNet}} & \multicolumn{1}{c||}{{\bf WordNet}} & \multicolumn{1}{c||}{{\bf Clusters }}\\
& {\bf line} & \multicolumn{1}{c||}{\bf SF} & \multicolumn{1}{c||}{\bf SS }& \\
%\hline
\hline \hline
Malt & 88.46 & 88.49 (+0.03) & 88.42 (-0.04)  & 88.59 (+0.13) \\ \hline 
MST & 90.55 & 90.70 (+0.15)    & 90.47 (-0.08)    & 90.88 (+0.33)\ddag   \\   \hline 
ZPar & 91.52  & 91.65 (+0.13)   & 91.70 (+0.18)\dag    & 91.74 (+0.22)  \\   \hline 
\end{tabular}
\\
\caption{\label{t1} LAS results with several parsing algorithms, Penn2Malt conversion (\dag: p \textless 0.05, \ddag: p \textless 0.005). In parenthesis, difference with baseline.}
 \end{table}


\emph{WordNet}. We will experiment with the semantic representations used in \newcite{agirre-baldwin-martinez:2008:ACLMain} and \newcite{agirre-EtAl:2011:ACL-HLT2011}, based on WordNet 2.1. WordNet is organized into sets of synonyms, called synsets (SS). Each synset in turn belongs to a unique semantic file (SF). There are a total of 45 SFs (1 for adverbs, 3 for adjectives, 15 for verbs, and 26 for nouns), based on syntactic and semantic categories. For example, noun SFs differentiate nouns denoting acts or actions, and nouns denoting animals, among others. We experiment with both full SSs and SFs as instances of fine-grained and coarse-grained semantic representation, respectively. As an example, \emph{knife} in its tool sense is in the EDGE TOOL USED AS A CUTTING INSTRUMENT singleton synset, and also in the ARTIFACT SF along with thousands of words including \emph{cutter}. These are the two extremes of semantic granularity in WordNet.
%As a hybrid representation, we also tested the effect of merging words with their corresponding SF (e.g. knife+ARTIFACT). This is a form of semantic specialisation rather than generalization, and allows the parser to discriminate between the different senses of each word, but not generalise across words. 
%For each of these two semantic representations, we experimented with semantic information for each of: (1) all open-class POSs (nouns, verbs, adjectives and adverbs), (2) nouns only, and (3) verbs only. There are thus a total of 6 combinations of representation type and target POS: SS (synset), SS\_N (noun synsets), SS\_V (verb synsets), SF (semantic file), SF\_N, and SF\_V. %, WSF (wordform+SF), WSF\_N (wordform+SF for nouns) and WSF\_V. 
For each semantic representation, we need to determine the semantics of each occurrence of a target word. \newcite{agirre-EtAl:2011:ACL-HLT2011} used i) gold-standard annotations from SemCor, a subset of the PTB, to give an upper bound performance of the semantic representation, ii) first sense, where all instances of a word were tagged with their most frequent sense, and iii) automatic sense ranking, predicting the most frequent sense for each word  \cite{mccarthy-EtAl:2004:ACL}.
As we will make use of the full PTB, we only have access to the first sense information. %(ENEKO: WordNet 2.1 or 1.7?).


\begin{table}[t]
\centering
\scriptsize
\begin{tabular}{ |c||c||l||l||l||} \hline 
& \multicolumn{1}{c||}{{\bf Base}} & \multicolumn{1}{c||}{{\bf WordNet}} & \multicolumn{1}{c||}{{\bf WordNet}} & \multicolumn{1}{c||}{{\bf Clusters }}\\
& {\bf line} & \multicolumn{1}{c||}{\bf SF} & \multicolumn{1}{c||}{\bf SS }& \\
%\hline
\hline \hline
Malt & 84.95 & 85.12 (+0.17) & 85.08 (+0.16)  & 85.13 (+0.18) \\ \hline 
MST & 85.06 & 85.35 (+0.29)\ddag    & 84.99 (-0.07)    & 86.18 (+1.12)\ddag   \\   \hline 
ZPar & 89.15  & 89.33 (+0.18)   & 89.19 (+0.04)    & 89.17 (+0.02)  \\   \hline 
\end{tabular}
\\
\caption{\label{t2} LAS results with several parsing algorithms in the LTH conversion (\dag: p \textless 0.05, \ddag: p \textless 0.005). In parenthesis, difference with baseline.}
 \end{table}


\emph{Clusters}.
\newcite{koo-carreras-collins:2008:ACLMain} describe a semi-supervised approach that makes use of cluster
features induced from unlabeled data, providing significant performance improvements 
for supervised dependency parsers 
on  the Penn Treebank
for English and the Prague Dependency Treebank for Czech. The process defines a hierarchical clustering of the words, which can be
represented as a binary tree where each node is associated to a bit-string, from the more general (root of the tree) to the more specific (leaves). Using prefixes of various lengths, it can
produce clusterings of different granularities. It can be seen as a representation of syntactic-semantic information acquired from corpora. They use short strings of 4-6 bits to represent parts of speech and the full strings for wordforms.

%KOLDO: I WILL EXPLAIN THE DIFFERENT VARIANTS USED BY CARRERAS ET AL: 4/5 BIT STRINGS AS SUBSTITUTES OF POS, AND ALL THE BITS FOR REPRESENTING WORDS

%\cite{candito-seddah:2010:SPMRL}  


% \subsubsection{Named Entities and erences}
 %We made use of the Stanford CoreNLP\footnote{http://nlp.stanford.edu/software/corenlp.shtml} tool, a set of natural language analysis tools that integrates several NLP tools, including a part-of-speech tagger, a named entity (NE) recognizer, a parser, and a coreference resolution system, and provides model files for analysis of English. For our purpose, we included the NEs as tags that were input to the learning process. We also made use of correferences, including in each wordform the semantic tags (WordNet or clusters) corresponding to its antecedent. The main idea was to examine their usefulness for dependency parsing, as in \newcite{ciaramita-attardi:2007:IWPT2007}. 




\section{Results}\label{results}
In all the experiments we employed a baseline feature set using
word forms and parts of speech, and an enriched feature set (WordNet or clusters).
We firstly tested the addition of each individual semantic feature to each parser, evaluating its contribution to the parser's performance. For the combinations, instead of feature-engineering each parser with the wide array of different possibilities for features, as in \newcite{agirre-EtAl:2011:ACL-HLT2011}, we adopted the simpler approach of combining the outputs of the individual parsers by voting \cite{sagae2006}. We will use Labeled Attachment Score (LAS) as our main evaluation criteria. 
%However, as several related works have evaluated Unlabeled Attachment Score (UAS), we will also present this score for the sake of completeness.  
As in previous work, we exclude punctuation marks. For all the tests, we used a perceptron POS-tagger \cite{collins:2002:EMNLP02}, trained on WSJ sections 2--21, to assign POS tags automatically to both the training (using 10-way jackknifing) and test data, obtaining a POS tagging accuracy of 97.32\% on the test data. We will make use of Bikel's randomized parsing evaluation comparator to test the statistical signiÔ¨Åcance of the results. In all of the experiments the parsers were trained on sections 2-21 of the PTB and evaluated on the development set (section 22). Finally, the best performing system was evaluated on the test set (section 23).
% \footnote{For replicability, a complete description of all features can be found at http://lsi.bp.ehu.es/Semantics/.}.



\subsection{Single Parsers}\label{individual}
We run a series of experiments testing each individual semantic feature, also trying different
learning configurations for each one.  Regarding the WordNet information, there were 2 different features to experiment with (SF and SS). For the bit clusters, there are different possibilities, depending on the number of bits used. For Malt and MST, all the different lengths of bit strings were used. Given the computational requirements and the previous results on Malt and MST, we only tested all bits in ZPar.
%We also experimented using the Named Entities and correferences
Tables \ref{t1} and \ref{t2} show the results.

 %The tables do not show the impact of NEs and correferences, because they did not give significant improvements in our early experiments with the different parsers, so we decided not to include them in the rest of the experiments.
%We can observe significant improvements in several of the experiments:
\emph{Penn2Malt}. Table \ref{t1} shows that the only significant increase over the baseline is for ZPar with SS and for MST with clusters.

\emph{LTH}. Looking at table \ref{t2}, we can say that the differences in baseline parser performance are accentuated when using the LTH treebank conversion, as ZPar clearly outperforms the other two parsers by more than 4 absolute points. We can see that SF helps all parsers, although it is only significant for MST. Bit clusters improve significantly MST, with the highest increase across the table.  


\begin{table}[t]
\centering
\scriptsize
\begin{tabular}{ |c||l|c|c|c|c|c|c|c|} \hline 
%& \multicolumn{2}{c|}{{\bf Baseline}}  \\
%\hline
{\bf Parsers}&{\bf LAS} &{\bf UAS } \\ 
\hline \hline
Best baseline (ZPar) & 91.52 & 92.57  \\ \hline 
Best single parser (ZPar + Clusters) & 91.74 (+0.22) & 92.63  \\ \hline 
Best combination (3 baseline parsers) & 91.90 (+0.38) & 93.01   \\  \hline 
Best combination of 3 parsers:  &   &   \\  
 3 baselines + 3 SF extensions & 91.93 (+0.41)  &  92.95   \\ \hline 
Best combination of 3 parsers:  &   &   \\  
 3 baselines + 3 SS extensions & 91.87 (+0.35)  &  92.92   \\ \hline 
Best combination of 3 parsers:  &   &   \\  
 3 baselines + 3 cluster extensions & 91.90 (+0.38)  &  92.90   \\ \hline 
\end{tabular}
\\
\caption{\label{t3} Parser combinations on Penn2Malt.}
 \end{table}


\begin{table}[t]
\centering
\scriptsize
\begin{tabular}{ |c||l|c|c|c|c|c|c|c|} \hline 
%& \multicolumn{2}{c|}{{\bf Baseline}}  \\
%\hline
{\bf Parsers}&{\bf LAS} &{\bf UAS } \\ 
\hline \hline
Best baseline (ZPar) & 89.15 & 91.81  \\ \hline 
Best single parser (ZPar + SF) & 89.33 (+0.15) & 92.01  \\ \hline 
Best combination (3 baseline parsers) & 89.15 (+0.00) & 91.81   \\  \hline 
Best combination of 3 parsers:  &   &   \\ 
 3 baselines + 3 SF extensions & 89.56 (+0.41)\ddag  &  92.23   \\ \hline 
Best combination of 3 parsers:  &   &   \\  
 3 baselines + 3 SS extensions & 89.43 (+0.28)  &  93.12   \\ \hline 
Best combination of 3 parsers:  &   &   \\  
 3 baselines + 3 cluster extensions & 89.52 (+0.37)\dag  &  92.19   \\ \hline 
\end{tabular}
\\
\caption{\label{t4} Parser combinations on LTH (\dag: p \textless 0.05, \ddag: p \textless 0.005).}
 \end{table}


%
%Overall, we can say that, although most experiments give significant increases with respect to the baseline, it does not seem easy to find a clear pattern explaining the influence of each type of semantic information on each parser and treebank conversion. 
Overall, we see that the small improvements do not confirm the previous results on Penn2Malt, MaltParser and gold POS tags.
We can also conclude that automatically acquired clusters are specially effective with the MST parser in both treebank conversions, which suggests that the type of semantic information has a direct relation to the parsing algorithm. 
%On the other hand, MaltParser and ZPar do only get slight increases with clusters, compared to other types of semantic information coming from WordNet.
Section \ref{analysis} will look at the details by each knowledge type.




\begin{table*}[t]
\centering
\scriptsize
\begin{tabular}{ |l||l|l|l|l|c|c|c|c|} \hline 
& &  & \multicolumn{1}{c|}{{\bf LAS on sentences}}  & \multicolumn{1}{c|}{{\bf LAS on sentences}}  \\
%\hline
\bf{POS tags} & {\bf Parser} & {\bf LAS test set} & {\bf  without POS errors} & {\bf with POS errors}  \\ \hline \hline
Gold  & ZPar & 90.45 & 91.68 & 89.14 \\ \hline 
Automatic & ZPar  & 89.15 &  91.62 & 86.51   \\  \hline 
Automatic & Best combination of 3 parsers:  & 89.56 (+0.41) & 91.90 (+0.28) & 87.06 (+0.55)  \\  
&  3 baselines + 3 SF extensions  & & & \\ \hline 
Automatic& Best combination of 3 parsers:  & 89.43 (+0.28)  & 91.95 (+0.33) & 86.75 (+0.24) \\  
&  3 baselines + 3 SS extensions & &  & \\ \hline 
Automatic & Best combination of 3 parsers: & 89.52 (+0.37) & 91.92 (+0.30) & 86.96 (+0.45) \\  
 &  3 baselines + 3 cluster extensions & & & \\ \hline 
\end{tabular}
\\
\caption{\label{t9} Differences in LAS (LTH) for baseline and extended parsers with sentences having correct/incorrect POS tags (the  parentheses show the difference w.r.t ZPar with automatic POS tags).}
 \end{table*}

\subsection{Combinations}\label{Combinations}
Subsection \ref{individual} presented the results of the base algorithms and their extensions based on  semantic features. \newcite{sagae2006} report improvements over the best single
parser when combining three transition-based models and one graph-based model. The same
technique was also used by the winning team of the CoNLL 2007 Shared Task \cite{hal2007}, combining 
six transition-based parsers. 
We used MaltBlender\footnote{http://w3.msi.vxu.se/users/jni/blend/}, a tool for merging the output 
of several dependency parsers, using the Chu-Liu/Edmonds directed MST algorithm. 
%It provides several types of weighting schemes, based on POS, dependency relation or individual parser's score. 
%Although there are several types of weighting schemes, \newcite{surdeanu-manning:2010:EnsembleParsing} showed that the diversity of base parsers is more important than complex learning models. 
After several tests we noticed that weighted voting by each parser's labeled accuracy gave good 
results, using it in the rest of the experiments. We trained different types of combination:

\begin{compactitem}
\item Base algorithms. This set includes the 3 baseline algorithms, MaltParser, MST, and ZPar.
\item Extended parsers, adding semantic information to the baselines. We include the 
three base algorithms and their semantic extensions (SF, SS, and clusters). 
It is known \cite{surdeanu-manning:2010:EnsembleParsing} that adding more parsers to an ensemble usually 
improves accuracy, as long as they add to the diversity (and almost regardless of their accuracy level). So, for 
the comparison to be fair, we will compare ensembles of 3 parsers, taken from sets of 6 
parsers (3 baselines + 3 SF, SS, and cluster extensions, respectively).
%As the number of combinations is high, we tried the combination of the best performing instances of each parser.
\end{compactitem}

In each experiment, we took the best combination of individual parsers on the 
development set for the final test. Tables \ref{t3} and  \ref{t4} show the results.

\emph{Penn2Malt}. Table \ref{t3} shows that the combination of the baselines, without any semantic information, 
considerably improves the best baseline. Adding semantics does not give a noticeable increase with respect to 
combining the baselines. %We tested the individual effect of combining the baselines with extended parsers using synsets and clusters, but the results did not improve with respect to the best single system (ZPar + SF).

\emph{LTH} (table \ref{t4}). Combining the 3 baselines does not give an improvement over the best 
baseline, as ZPar clearly outperforms the other parsers. However, adding the semantic parsers gives 
an increase with respect to the best single parser (ZPar + SF), which is small but significant for 
SF and clusters.%, thus showing the beneficial effect of using additional semantic information.


%Although tables \ref{t3} and \ref{t4} present the results of combining the baselines with SF and SS, which gave the best combination, we also tried other types of combination using SF, SS and clusters, but we found that even when SS and cluster combinations gave significant increments with respect to the baselines, they did not reach the performance of the best combination. This fact seems to imply that the gain obtained from clusters is a subset of the overall improvement when using SF.
%
%As a validation, we repeated the experiments using gold POS tags directly from the treebank and found that, although there are slight gains using semantic information with respect to the baseline parsers, the results are not significant. This could mean that the semantic information can be used by the parsers to alleviate the POS tagging errors, while it is not so useful when having access to the correct POS tags.

%We also independently tested the combination of each parser's variants, e.g., combining the MaltParser baseline with the MaltParser's semantic extensions, and we found that this gave slight improvements for MaltParser and MST, which were not significant, but a significant improvement for the combination of ZPar's variants (obtaining 90.48 LAS, a 0.18 LAS increase wrt ZPar's baseline). This can suggest that, although none of the individual semantic features significantly helped ZPar, their combination does help the parser. 
%
%Table  \ref{t8} lists the accuracy of our parser together with results from related work. \newcite{koo-carreras-collins:2008:ACLMain} and \newcite{suzuki-EtAl:2009:EMNLP} use automatically acquired clusters, \newcite{koo-collins:2010:ACL} apply third order parsing, and \newcite{DBLP:conf/eacl/BohnetK12} use a graph-based completion model in a transition-based parser.
%
%\begin{table}[t]
%\centering
%\scriptsize
%\begin{tabular}{ |l||l|l|l|c|c|c|c|c|} \hline 
%%& \multicolumn{2}{c|}{{\bf Baseline}}  \\
%%\hline
%{\bf Parser}&{\bf LAS} &{\bf UAS } \\ 
%\hline \hline
%\cite{koo-carreras-collins:2008:ACLMain} & NA & 93.16  \\ \hline 
%\cite{suzuki-EtAl:2009:EMNLP} & NA & \textbf{93.79}   \\  \hline 
%\cite{koo-collins:2010:ACL} & NA & 93.04   \\  \hline 
%\cite{DBLP:conf/eacl/BohnetK12} & 92.38 & 93.39  \\ \hline 
%This paper. Best single system (ZPar + clusters)& 91.74  & 92.63  \\   \hline 
%This paper. Best combined system & \textbf{92.19}  & 93.26   \\  \hline 
%%Best combination (baselines + semantic (SF)) &  90.00(+0.85) & 92.57   \\  \hline 
%\end{tabular}
%\\
%\caption{\label{t8}  Comparison to the state of the art (Penn2Malt conversion, punctuation excluded).}
% \end{table}



%Comparison to \cite{koo-collins:2010:ACL}, which applies a third order dependency parser to English anc Czech. Our results give  state of the art UAS on the Penn2Malt conversion of the English Penn treebank. KOLDO WILL DO THIS!!! %HOWEVER, OUR RESULTS FOR MST-ORDER2 ARE HIGHER THAN THE ONES REPORTED IN THE LITERATURE (OUR UAS BASELINE IN TABLE 2 IS 93.01, WHILE KOO \& COLLINS 2010 REPORT 91.5 FROM MCDONALD AND PEREIRA 2006. ON THE OTHER HAND, OUR RESULTS ARE IN ACCORD WITH JOHANSSON AND NUGHES 2008, WHO ALSO APPLY MST TO THE PENN TREEBANK).

%THE PREVIOUS WORKS PRESENTED RESULTS ON UAS. WE COULD ALSO LOOK AT THE BEST LAS RESULTS AND COMPARE THEM TO OUR WORK.

%\emph{Koldo: The paper does not tell anything about using gold POS tags. We did not perform all the experiments with gold POS tags (but we did almost all). Perhaps we could add a short paragraph comparing the results. As the improvements were smaller or any at all with gold tags and some parsers, perhaps it could be hinted that semantic tags are specially useful for the parser to recover after POS tag errors, as they provide additional generalizations.}


\subsection{Analysis}\label{analysis}
%After looking at the improvements obtained by adding semantic information to the parsers,
In this section we analyze the data trying to understand where and how semantic information helps most.
%, which can shed light on a qualitative different parsers, eitheraspect of each type of semantic information. 
One of the obstacles of automatic parsers is the presence of incorrect POS tags due to automatic 
tagging. For example, ZPar's LAS score on the LTH conversion drops from 90.45\% with gold POS tags 
to 89.12\% with automatic POS tags. We will examine the influence of each type of semantic information 
on sentences that contain or not POS errors, and this will clarify whether the increments obtained when 
using semantic information are useful for correcting the negative influence of POS errors or they are 
orthogonal and constitute a source of new information independent of POS tags.
With this objective in mind, we analyzed the performance on the subset of the test corpus containing 
the sentences which had POS errors (1,025 sentences and 27,300 tokens) and the subset where the 
sentences had (automatically assigned) correct POS tags (1,391 sentences and 29,386 tokens). %The division gave us two almost equally sized sets of sentences, although the number of sentences with no tagging errors is higher, because probably this set contains shorter sentences, which are less prone to POS tagging errors.


Table \ref{t9} presents the results of the best single parser on the LTH conversion (ZPar) with 
gold and automatic POS tags in the first two rows. The LAS scores are particularized for sentences 
that contain or not POS errors. The following three rows present the enhanced (combined) parsers 
that make use of semantic information. As the combination of the three baseline parsers did not 
give any improvement over the best single parser (ZPar), we can hypothesize that the gain coming 
from the parser combinations comes mostly from the addition of semantic information.
Table \ref{t9} suggests that the improvements coming from WordNet's semantic file (SF) are 
unevenly distributed between the sentences that contain POS errors and those that 
do not (an increase of 0.28 for sentences without POS errors and 0.55 for those with errors). This 
could mean that a big part of the information contained  in SF helps to alleviate the errors 
performed by the automatic POS tagger. On the other hand, the increments are more evenly 
distributed for SS and clusters, and this can be due to the fact that the semantic 
information is orthogonal to the POS, giving similar improvements for sentences that 
contain or not POS errors. We independently tested this fact for the individual parsers. For 
example, with MST and SF the gains almost doubled  for sentences with incorrect 
POS tags (+0.37 with respect to +0.21 for sentences with correct POS tags) while 
the gains of adding clusters' information for sentences without and with POS errors 
were similar (0.91 and 1.33, repectively). This aspect deserves further 
investigation, as the improvements seem to be related to both the type of semantic 
information and the parsing algorithm.%, and the results should be examined in detail for each individual parser. 
We did an initial exploration but it did not give any clear indication of the types 
of improvements that could be expected using each parser and semantic data.

%%
%%\subsection{Semantic information and POS tagging errors}\label{analysis-POS-errors}
%%\textcolor{red}{Although all the experiments that have been presented were performed using automatically predicted POS tags, we independently tested the effectiveness of the different types of semantic information on gold POS tags. These experiments showed us that the gains coming from using semantics for improving parsing performance were very slight, if not detrimental, when applied to sentences with gold POS tags. This fact led us to investigate the relation between POS tagging errors and the gains coming from using semantic information. With this objective in mind, we separated the test corpus in two different sets of sentences: one with the sentences containing POS errors (1,025 sentences and 27,300 tokens) and another one with the sentences that had (automatically assigned) correct POS tags (1,391 sentences and 29,386 tokens). The division gave us two almost equally sized sets of sentences, taking tokens into accounts, although the number of sentences with no tagging errors is higher, because probably this set contains shorter sentences, which are less prone to POS tagging errors.}
%%
%%\textcolor{red}{We compared the effect of adding semantic information to the automatically POS tagged sentences, and compared the resulting LAS score to that of the same set when parsed with gold POS tags. The results are taken from the LTH converted version of the treebank, although similar results were obtained with the Penn2Malt conversion. Figures \ref{fig:auto-LTH-sentences-with-POS-errors} and \ref{fig:auto-LTH-sentences-without-POS-errors}\footnote{The reader may observe that the scores using gold POS tags (corresponding to the first column for each parser) are considerable higher in figure \ref{fig:auto-LTH-sentences-without-POS-errors} than in figure \ref{fig:auto-LTH-sentences-with-POS-errors}. This is presumably due to the fact that the sentences which are correctly POS tagged by an automatic tagger are probably also easier to parse.} present the differences in LAS for the sets of sentences with and without POS errors, respectively. In each figure and for each parser, there are three different values. The first one correspond to the results obtained when parsing the sentences containing the gold POS tags, which represents the best expected situation. The second column shows the results of parsing using the predicted POS tags, where the scores worsen considerably for sentences containing POS errors. The third column gives the increment produced by the addition of semantic information. Figure \ref{fig:auto-LTH-sentences-with-POS-errors} shows that adding  semantic information helps to alleviate an important proportion of POS errors, that represents a reduction of the parsing errors related to the incorrect POS tagging of \%34, \%59 and \%43 for MaltParser, MST and ZPar, respectively. Figure \ref{fig:auto-LTH-sentences-without-POS-errors}, however, shows that semantic information only gives slight increments on the set of sentences that are correctly POS tagged. The absolute LAS increments corresponding to Malt, MST and ZPar are (0.43, 0.29 and 0.05), compared to (0.93, 1.97 and 1.09) in figure \ref{fig:auto-LTH-sentences-with-POS-errors}. This fact implies that most of the improvement coming from the addition of semantic information is helping the parser to \emph{correct} tagging errors, while this semantic information has a less important effect in improving the sentences where no POS tagging errors were produced. The only exception to this is MaltParser, and the improvement can be explained by the fact that this parser obtains the lowest LAS results, and in this case semantic information can help to boost the parser's performance significantly, in accord with \cite{agirre-EtAl:2011:ACL-HLT2011}.}
%%
%%\begin{figure}
%%  \centering
%%  \includegraphics[trim = 80mm 60mm 70mm 70mm, width=0.5\textwidth]{Figure1bis-LTH-Sentences-with-POS-errors.pdf} %trim option's parameter order: left bottom right top
%%%  \includegraphics[width=0.5\textwidth]{auto-LTH-sentences-with-POS-errors.pdf}
%%  \caption{Effect of semantic information (SF) on sentences with POS taging errors. LTH conversion.}
%%  \label{fig:auto-LTH-sentences-with-POS-errors}
%%\end{figure}
%%
%%\begin{figure}
%%  \centering
%%  \includegraphics[trim = 80mm 70mm 70mm 70mm, width=0.5\textwidth]{Figure2-LTH-Sentences-without-POS-errors.pdf} %trim option's parameter order: left bottom right top
%%  \caption{Effect of semantic information (SF) on sentences without automatic POS taging errors. LTH conversion.}
%%  \label{fig:auto-LTH-sentences-without-POS-errors}
%%\end{figure}
%%
%%\textcolor{red}{KOLDO: AFTER LOOKING AT POS ERRORS OVER SENTENCES, I WILL SEE IF I CAN STABLISH A RELATIONSHIP BETWEEN POS ERRORS AND SEMANTIC INFO. THAT IS, GOING FROM SENTENCES WITH POS ERRORS DIRECTLY TO THE ERRORS THEMSELVES. PERHAPS I FIND SOMETHING INTERESTING. MY FIRST HYPOTHESIS IS THAT SEMANTIC INFO WILL CORRECT AN IMPORTANT PROPORTION OF DEPENDENCY ERRORS COMING FROM TAGGING ERRORS, BUT I WILL HAVE TO FIND THE DATA}
%%
%%\textcolor{red}{ENEKO: COULD YOU TRY TO FIND ONE OR TWO EXAMPLES???}
%%

\section{Conclusions}\label{conclusions}
This work has tried to shed light on the contribution of semantic information to 
dependency parsing. The experiments were thorough, testing two treebank conversions and three 
parsing paradigms on automatically predicted POS tags. Compared to \cite{agirre-EtAl:2011:ACL-HLT2011}, 
which used MaltParser on the LTH conversion and gold POS tags, our 
results can be seen as a negative outcome, as the improvements are very small and non-significant in most of 
the cases. For parser combination, WordNet semantic file information does give a small significant increment 
in the more fine-grained LTH representation. In addition we show that the improvement of automatic clusters 
is also weak. For the future, we think tdifferent parsers, eitherhat a more elaborate scheme is needed for word classes, requiring 
to explore different levels of generalization in the WordNet (or alternative) hierarchies. %Another interesting avenue is to extend the experiments to new languages.
%Overall, we can conclude that our results of WordNet information for dependency parsing has not been demonstrated. 
%This paper shows that semantic classes from WordNet do improve statistical dependency parsing for
%both graph-based dependency parsing and transition based dependency
%parsing,  for both global model and local models, and for both greedy
%search and non-greedy search, using automatically assigned POS tags. There have been increments with all the types of semantic information and parsers, although the coarse grained semantic file information from WordNet is the most effective,  compared to synsets and automatically acquired clusters. The improvements are general across parsers and dependency annotation schemes (linked to Penn2Malt and LTH treebank conversion), with a few exceptions. Finally, we have also shown that the increments of adding semantic information also hold when combining parsers.

%, and that automatic clusters are superfluous in the optimal mix. The results obtained in this work are competitive with the state of the art. 

%All the experiments using WordNet information made use of the first-sense heuristic, which is a crude yet simple and effective way of incorporating semantic information. As future work, we plan to use automatically disambiguated word senses, which could help to obtain a more fine grained semantic representation. 


\section*{Acknowledgments}
This research was supported by the the Basque Government (IT344-
10, S PE11UN114), the University of the
Basque Country (GIU09/19) and the Spanish
Ministry of Science and Innovation (MICINN,
TIN2010-20218).
%Do not number the acknowledgment section. Do not include this section when submitting your paper for review.
%\bibliographystyle{acl}
% you bib file should really go here 
%\bibliography{acl2013}



\bibliographystyle{acl}
{\bibliography{biblio-semantics}}





\end{document}
