SubmissionNumber#=%=#404
FinalPaperTitle#=%=#Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models
ShortPaperTitle#=%=#Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models
NumberOfPages#=%=#7
CopyrightSigned#=%=#Michael Auli
JobTitle#==#
Organization#==#
Abstract#==#Neural network language models are often trained by optimizing likelihood, but
we would prefer to optimize for a task specific metric, such as BLEU in machine
translation. We show how a recurrent neural network language model can be
optimized towards an expected BLEU loss instead of the usual cross-entropy
criterion. Furthermore, we tackle the issue of directly integrating a recurrent
network into first-pass decoding under an efficient approximation. Our best
results improve a phrase-based statistical machine translation system trained
on WMT 2012 French-English data by up to 2.0 BLEU, and the expected BLEU
objective improves over a cross-entropy trained model by up to 0.6 BLEU in a
single reference setup.
Author{1}{Firstname}#=%=#Michael
Author{1}{Lastname}#=%=#Auli
Author{1}{Email}#=%=#michael.auli@gmail.com
Author{1}{Affiliation}#=%=#Microsoft Research
Author{2}{Firstname}#=%=#Jianfeng
Author{2}{Lastname}#=%=#Gao
Author{2}{Email}#=%=#jfgao@microsoft.com
Author{2}{Affiliation}#=%=#Microsoft Research, Redmond

==========