% IDEAS FOR LATER
%---------------
% Use conversations for all documents. Requires getting more data.
% build into model a variable that indicates which topics are actually aligned and which are not
% run on multiple languages (not just two)
% Chinese LID system per token should be easy
% Group tweets in other ways, e.g. by hashtag, author, date, etc.

%
% File naaclhlt2013.tex
%

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{latexsym}
\usepackage{tikz}
\usepackage{url}
\usepackage{epstopdf}
\usepackage{CJKutf8} 
%\usepackage{subfig}
\usepackage{graphicx}
%\usepackage{multirow}
\usepackage[utf8]{inputenc}
%\usepackage[font=scriptsize,labelfont=bf]{subcaption}
%\usepackage[font=footnotesize,labelfont=bf]{caption}

%\setlength\titlebox{6.5cm}    % Expanding the titlebox

\usepackage{caption}
\usepackage{subcaption}
\usepackage{floatrow}
\usepackage{calc}% To calculate width for \FBwidth

\floatsetup{
  heightadjust=object,
  valign=t
}
\newcommand{\squishlist}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{3pt}
     \setlength{\topsep}{3pt}
     \setlength{\partopsep}{0pt}
     \setlength{\leftmargin}{1.5em}
     \setlength{\labelwidth}{1em}
     \setlength{\labelsep}{0.5em} } }

\newcommand{\squishlisttwo}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{0pt}
    \setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\leftmargin}{2em}
    \setlength{\labelwidth}{1.5em}
    \setlength{\labelsep}{0.5em} } }

\newcommand{\squishend}{
  \end{list}  }

\title{Learning Polylingual Topic Models from\\ Code-Switched Social Media Documents}

\author{Nanyun Peng ~~~~ Yiming Wang ~~~~ Mark Dredze\\
        Human Language Technology Center of Excellence \\
	    Center for Language and Speech Processing\\
	    Johns Hopkins University, Baltimore, MD USA\\
	    {\tt \{npeng1,freewym,mdredze\}@jhu.edu}}
%\author{}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Code-switched documents are common in social media, providing evidence for
polylingual topic models to infer aligned topics across languages.
We present Code-Switched LDA (csLDA), which infers language specific topic distributions
based on code-switched documents to facilitate multi-lingual corpus analysis.
We experiment on two code-switching corpora (English-Spanish Twitter data and English-Chinese Weibo data) and show that csLDA improves perplexity over LDA, and learns semantically coherent aligned topics as judged by human annotators.
\end{abstract}

\section{Introduction}
Topic models \cite{blei2003latent} have become standard tools for analyzing document collections, and 
topic analyses %have covered a wide range of data sets \cite{talley2011database} and in recent years,
are quite common for social media \cite{paul2011you,zhao2011comparing,hong2010empirical,ramage2010characterizing,eisenstein2010latent}.
Their popularity owes in part to their data driven nature, allowing them to adapt to new
corpora and languages.
In social media especially, there is a large diversity in terms of both the topic
and language,
necessitating the modeling of multiple languages simultaneously.
A good candidate for multi-lingual topic analyses are polylingual topic models \cite{mimno2009polylingual},
which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic.
Polylingual topic models enable cross language analysis by grouping documents by topic
regardless of language.

Training of polylingual topic models requires parallel or comparable corpora:
document tuples from multiple languages that discuss the same topic. While
additional non-aligned documents can be folded in during training, the ``glue'' documents
are required to aid in the alignment across languages.
However, the ever changing vocabulary and topics of social media \cite{Eisenstein:2013fk}
make finding suitable comparable corpora difficult. Standard techniques -- such
as relying on machine translation parallel corpora or comparable documents extracted
from Wikipedia in different languages -- fail to capture the specific terminology of social media.
Alternate methods that rely on bilingual
lexicons \cite{jagarlamudi2010extracting} similarly fail to adapt to shifting vocabularies. The result:
an inability to train polylingual models on social media.

In this paper, we offer a solution: utilize code-switched social media to discover correlations across languages.
Social media is filled with examples of code-switching, where users switch between two or more languages, both
in a conversation and even a single message \cite{wangling:acl2013}.
%Code-switching is prevalent in social media when topics span locations with different languages.
This mixture of languages in the same context suggests alignments between words across languages through
the common topics discussed in the context.



We learn from code-switched social media by extending the polylingual topic model framework
to infer the language of each token and then automatically processing the learned topics to identify 
aligned topics.
Our model improves both in terms of perplexity and a human evaluation, and we provide some example
analyses of social media that rely on our learned topics.
%Additionally,
%by inferring language the model can learn when token specific language identification (LID) is unavailable.


%One approach to cross language classification is to mine parallel corpora or bilingual dictionaries
%to obtain correspondences between languages \cite{Olsson:05}. A similar approach is taken by \cite{Wei:10} for the task of cross-language sentiment classification. They begin by automatically translating some of the data. Then, to reduce the noise introduced by the translations, they use only ``reliable'' parts of the translations to identify correspondences. Building on the domain adaptation algorithm of Structural Correspondence Learning (SCL) \cite{Blitzer:06}, they construct pivot features from these selected regions and adapt trained classifiers. \cite{Prettenhofer:2010fk} also use SCL for language adaptation. all these techniques are also rely heavily on bi-lingual dictionaries, which make the goal of our model to automatically align words from different languages much meaningful.

\begin{figure}[t]
\scriptsize
\center
\begin{tabular}{|l|}
\hline
User 1: !`Don Samuel es un crack! \#VamosMÃ©xico \#DaleTri\\
~~~~ RT @User4: Arriba! Viva Mexico! Advanced to GOLD. \\
~~~~ medal match in ``Football''!\\
User 2: @user1 rodo que tal el nuevo Mountain ?\\
User 3: @User1 @User4 wow this is something !! Ja ja ja \\
~~~~ Football well said\\
\hline
\end{tabular}
\caption{Three users discuss Mexico's football team advancing to the Gold medal game
in the 2012 Olympics in code-switched Spanish and English.\vspace{-.5cm}}
\label{fig:example}
\end{figure}



%One approach to cross language classification is to mine parallel corpora or bilingual dictionaries
%to obtain correspondences between languages \cite{Olsson:05}. A similar approach is taken by \cite{Wei:10} for the task of cross-language sentiment classification. They begin by automatically translating some of the data. Then, to reduce the noise introduced by the translations, they use only ``reliable'' parts of the translations to identify correspondences. Building on the domain adaptation algorithm of Structural Correspondence Learning (SCL) \cite{Blitzer:06}, they construct pivot features from these selected regions and adapt trained classifiers. \cite{Prettenhofer:2010fk} also use SCL for language adaptation. all these techniques are also rely heavily on bi-lingual dictionaries, which make the goal of our model to automatically align words from different languages much meaningful.


\section{Code-Switching}
Code-switched documents has received considerable attention in the NLP community.
Several tasks have focused on identification and analysis, including mining translations in code-switched documents 
\cite{wangling:acl2013}, predicting code-switched points \cite{solorio-liu:2008:EMNLP1},
identifying code-switched tokens  \cite{Lignos:13,yu-he-chien:2012:CLP2012,elfardy-diab:2012:POSTERS},
adding code-switched support to language models
\cite{li-fung:2012:PAPERS},
linguistic processing of code switched data
\cite{solorio-liu:2008:EMNLP2},  corpus creation
 \cite{LI12.964.L12-1573,diab-kamboj:2011:ALR9},
 and computational linguistic analyses and theories of code-switching \cite{sankofl:1998:ACLCOLING,joshi1982processing}.
 
Code-switching specifically in social media has also received some recent attention.
\newcite{Lignos:13} trained a supervised token level language identification system for 
Spanish and English code-switched social media to study code-switching behaviors.
\newcite{wangling:acl2013} mined translation spans for Chinese and English 
in code-switched documents to improve a translation system,
relying on an existing translation model to aid in the identification and extraction task.
In contrast to this work, we take an unsupervised approach, relying only on readily available document level
language ID systems to utilize code-switched data. Additionally, our focus is not on individual messages, rather
we aim to train a model that can be used to analyze entire corpora.

In this work we consider two types of code-switched documents: single messages and conversations, and two
language pairs: Chinese-English and Spanish-English. 
Figure \ref{fig:example} shows an example of a code-switched Spanish-English {\em conversation}, in which three users discuss
Mexico's football team advancing to the Gold medal game in the 2012 Summer Olympics.
In this conversation, some tweets are code-switched and some are in a single language. By collecting the entire
conversation into a single document we provide the topic model with additional content.
An example of a Chinese-English code-switched messages is given by \newcite{wangling:acl2013}:
\begin{quote}
\begin{CJK}{UTF8}{gbsn} 
\textit{watup Kenny Mayne!! - Kenny Mayne æè¿è¿ä¹æ ·å!!}
\end{CJK} 
\end{quote}
Here a user switches between languages in a single {\em message}. We empirically evaluate our
model on both conversations and messages. In the model presentation we will refer
to both as ``documents.''

%The code-switched documents provide a rich source of lexical knowledge about semantically aligned words across languages since speakers/authors remain topically relevant.
%Code-switching provides a rich source of lexical knowledge about semantically aligned words across languages since speakers/authors remain topically relevant.
% when topics span language communities. For example, fans of the World Cup and
%followers of Egyptian politics both communicate in a variety of languages using Twitter, and conversations often span languages,




\section{csLDA}
To train a polylingual topic model on social media, we
make two modifications to the model of \newcite{mimno2009polylingual}: add a token specific language variable, and a process for identifying aligned
topics.
%Each topic has a language specific distribution for each monolingual vocabulary.
%Polylingual topic modeling on code-switched data presents two challenges.
%We begin with a discussion of these challenges and then detail our model.

First, polylingual topic models require parallel or comparable corpora in which each document has an assigned 
language. In the case of code-switched social media data, we require a {\em per-token} language variable.
However, while document level language identification (LID) systems are common place, very few languages have
per-token LID systems \cite{king2013labeling,Lignos:13}.

%TODO add citation from emnlp for per-token LID system.
%Mark: I'm not sure what this comment refers to.

To address the lack of available LID systems, we add a per-token latent language variable to the polylingual topic model.
For documents that are not code-switched, we observe these variables
to be the output of a document level LID system. In the case of code-switched documents,
these variables are inferred during model inference.

Second, polylingual topic models assume the aligned topics are from parallel or comparable corpora, which implicitly
assumes that a topics popularity is balanced across languages. Topics that show up in one language necessarily
show up in another.
However, in the case of social media, we can make no such assumption. The topics discussed are influenced
by users, time, and location, all factors intertwined with choice of language.
For example, English speakers will more likely discuss Olympic basketball while Spanish speakers football.
There may be little or no documents on a given topic in one language, while they are plentiful
in another. In this case, a polylingual topic model, which necessarily infers a topic-specific word distribution for each topic
in each language, would learn two unrelated word distributions in two languages for a single topic.
Therefore, naively using the produced topics as ``aligned'' across languages is ill-advised.
% Naively applying polylingual topic model in this seting would align unrelated Spanish words to English basketball topic as well as unrelated English words to Spanish football topic, which makes the ``aligned'' topics meaningless.
%may find little support for Spanish basketball or English football in monolingual documents, and on these topics.
%As a result, the model may use topic-specific word distributions to learn %unrelated topics in each language. Examining the resulting ``aligned'' topics would be meaningless.

Our solution is to automatically identify aligned polylingual topics after learning by examining a topic's distribution across 
code-switched documents. Our metric relies on distributional properties of an inferred topic across the entire collection.

To summarize, based on the model of \newcite{mimno2009polylingual} we will learn:
\squishlist
\item For each topic, a language specific word distribution.
\item For each (code-switched) token, a language.
\item For each topic, an identification as to whether the topic captures an alignment across languages.
\squishend

The first two goals are achieved by incorporating new hidden variables in the traditional polylingual topic model. The third goal requires an automated post-processing step.
We call the resulting model Code-Switched LDA (csLDA). The generative process is as follows:


%\squishlisttwo
%\item For each document $d \in \mathcal{D}$:
%\item, draw a topic distribution from a Dirichlet Distribution: $\theta \sim Dir(\theta; \alpha_m)$;  \\
%draw a language distribution from a Dirichlet Distribution: $\psi \sim Dir(\psi; \alpha_l)$ (During the inference phase, we can utilize observations to guide gibbs sampling to sample within the languages observed)
%\item Then draw a language assignment for each token: $l \sim P(l|\psi)$;\\
%draw a topic assignment for each token: $z \sim P(z|\theta)$.
%\item For each language $l$, sample the topic-word distribution:$\Phi^l \sim Dir(\Phi^l;\beta^l)$.
%\item Finally, the observed words are generated given its topic assignment, language assignment and a language specific topic-word distribution: \\
%$w^l \sim P(w^l | z, \Phi^l)$.
%\squishend
\squishlisttwo
\item For each topic $z \in \mathcal{T}$
	\squishlisttwo
	\item For each language $l \in \mathcal{L}$
		\squishlisttwo
		\item Draw word distribution $\phi_z^{l}$$\sim$$Dir(\beta^l)$
		\squishend
	\squishend
\item For each document $d \in \mathcal{D}$:
	\squishlisttwo
	\item Draw a topic distribution $\theta_d \sim Dir(\alpha)$
	\item Draw a language distribution $\psi_d$$\sim$$Dir(\gamma)$
	\item For each token $i \in d$:
		\squishlisttwo
		\item Draw a topic $z_i \sim \theta_d$
		\item Draw a language $l_i \sim \psi_d$
		\item Draw a word $w_i \sim \phi_z^l$
		\squishend
	\squishend
\squishend
For monolingual documents, we fix $l_i$ to the LID tag for all tokens. Additionally, we use a single background
distribution for each language to capture stopwords; a control variable $\pi$, which follows a Dirichlet distribution with prior 
parameterized by $\delta$, is introduced to decide the choice between background words and topic words following \cite{chemudugunta2006modeling}\footnote{Omitted from the generative process but shown in Fig. \ref{fig:model}.}. 
We use asymmetric Dirichlet priors \cite{wallach2009rethinking}, and let the optimization process learn the 
hyperparameters. The graphical model is shown in Figure \ref{fig:model}.



\begin{figure}[t]
\begin{center}
  \scalebox{0.8}{
    \tikzstyle{var}=[circle,draw=black!100,fill=none,thick,minimum
size=7mm,inner sep=0mm]
    \tikzstyle{hidden}=[draw]
    \tikzstyle{obs}=[circle,draw=black!100,fill=none,thick,minimum
size=7mm,fill=gray,inner sep=0mm]
    \begin{tikzpicture}[style=thick]
     % \draw (8.0,1.5) rectangle (9.3,2.5); % z
      \draw (7.7,1.05) rectangle (11.2,3.95);  % N
      \draw (6.4,0.85) rectangle (11.7,4.15);  % D
     % \draw (6.5,3.1) rectangle (7.7,4.2);  % Ad
    %  \draw (6.4,-0.9) rectangle (7.8,0.6);  % b
      %\draw (4.8,1.3) rectangle (6.0,2.5);  % A
      \draw (9.6,-0.8) rectangle (11.7,0.65);  % phi
      \draw (9.7,-0.5) rectangle (10.7,0.5);
     % \draw (8.5,-0.9) rectangle (9.8,0.6);  % omega

    \node (gamma) at (5.3,3.5)[var] {$\gamma$};
      \node (alpha) at (5.3,2.5)[var] {$\alpha$};
      \node (li) at (8.6,3.5)[var] {$l_i$};
      \node (psid) at (7,3.5) [var] {$\psi_d$};
      \node (theta) at (7,2.5) [var] {$\theta_d$};
      \node (phi) at (10.2,0) [var] {$\phi_z^l$};
      \node (phib) at (11.2,0) [var] {$\phi_b^l$};
      \node (omega) at (10.5,-1.5) [var] {$\beta$};
      \node (B) at (8.6,0) [var] {$B$};
      \node (delta) at (7.0,0) [var] {$\delta$};
      %\node (b) at (7,0) [var] {$b$};
      \node (z) at (8.6,2.5) [var] {$z_i$};
      \node (b) at (8.6,1.5) [var] {$b_i$};
      \node (w) at (10.5,2.5) [obs] {$w_i$};

     \node (D) at (11.5,1.2) {$D$};
     \node (N) at (11.0,1.5) {$N$};
    % \node (K) at (9.1,1.7) {$K$};
    % \node (Ka) at (5.8,1.5) {$K$};
    % \node (Kad) at (7.5,3.3) {$K$};
     \node (L) at (11.5,-0.6) {$\mathcal{L}$};
     \node (T) at (10.6,-0.4) {$\mathcal{T}$};
    % \node (Zomega) at (9.2,-0.6) {$\sum_{k} Z_k$};

\draw [->] (alpha) -- (theta);
\draw [->] (psid) -- (li);
\draw [->] (li) -- (w);
%\draw [->] (b) -- (theta);
\draw [->] (gamma) -- (psid);
\draw [->] (theta) -- (z);
\draw [->] (z) -- (w);
\draw [->] (phi) -- (w);
\draw [->] (phib) -- (w);
\draw [->] (omega) -- (phi);
\draw [->] (omega) -- (phib);
\draw [->] (b) --(w);
\draw [->] (B) --(b);
\draw [->] (delta) -- (B);

    \end{tikzpicture}
  }
\end{center}
\vspace{-0.3cm}
\caption{The graphical model for csLDA.}
\label{fig:model}
\end{figure}

\subsection{Inference}
Inference for csLDA follows directly from LDA. A Gibbs sampler learns the
word distributions $\phi_z^l$ for each language and topic. We use a block Gibbs sampler
to jointly sample topic and language variables for each token.
%DONE: Is this correct?
% I added  w_i in the notation, please check it.
%Mark: Looks right to me.
As is customary, we collapse out $\phi$, $\theta$ and $\psi$.
The sampling posterior is:
\begin{small}
\begin{eqnarray}
P(z_i,l_i | \mathbf{w},\mathbf{z}_{-i},\mathbf{l}_{-i},  \alpha, \beta, \gamma) \propto   \nonumber \\
\frac{(n_{w_i}^{l,z})_{ -i} + \beta}{n_{ -i}^{l,z} + \mathcal{W}\beta} \times \frac{m^{z,d}_{ -i} + \alpha}{m^{d}_{ -i} + \mathcal{T}\alpha} \times \frac{o^{l,d}_{ -i} + \gamma}{o^{d}_{ -i} + \mathcal{L}\gamma}
\end{eqnarray}
\end{small}
\noindent where $(n_{w_i}^{l,z})_{ -i}$ is the number of times the type for word $w_i$ assigned to topic $z$ and language $l$ (excluding current word $w_i$),
$m^{z,d}_{ -i}$ is the number of tokens assigned to topic $z$ in document $d$ (excluding current word $w_i$),
$o^{l,d}_{ -i}$ is the number of tokens assigned to language $l$ in document $d$ (excluding current word $w_i$),
and these variables with superscripts or subscripts omitted are totals across all values for the variable. $\mathcal{W}$ is the number of words in the corpus.
All counts omit words assigned to the background.
During sampling, words are first assigned to the background/topic distribution and then topic and language are sampled for non-background words.

We optimize the hyperparameters $\alpha$, $\beta$, $\gamma$ and $\delta$ by interleaving sampling iterations with a Newton-Raphson update to obtain
the MLE estimate for the hyperparameters. Taking $\alpha$ as an example, one step of the Newton-Raphson update is: 
\begin{equation}
\alpha^{new} =\alpha^{old}-\mathbf{H}^{-1}\frac{\partial \mathcal{L}}{\partial \alpha} 
\end{equation}
%TODO (Mark): I modified the equation to use a partial derivative. Is this correct?
where $\mathbf{H}$ is the Hessian matrix and $\frac{\partial \mathcal{L}}{\partial \alpha} $ is the gradient of the likelihood function with respect to the optimizing hyperparameter. 
We interleave 200 sampling iterations with one Newton-Raphson update.
%TODO: Is this correct? Do we use a burnin before doing hyperparameter optimization?
% Correct. I filled in the number.

%The inference is nested in the learning procedure as the E step; It is also called in the test precedure where parameters $\Phi$ are fixed to compute the perplexity.
%what hyperparameters do we have to estimation
%We want to estimate the hyper-parameters using MLE estimates in the training procedure. However, the latent variables in the model make it impossible to obtain a closed-form of the likelihood function. We instead use an EM algorithm to iteratively learn the hyper-parameters. In the E step, we fix the current estimates of the hyper-parameters and use Gibbs sampling to infer the posterior of latent variables. The procedure is detailed below. In the M step, we use the posterior to compute the expected log-likelihood of the model and nest the Newton-Raphson algorithm to iteratively obtain the MLE of the hyper-parameters. Since in our model the hyper-parameters are all for Dirichlet distribution,
%The expected likelihood is
%\begin{equation}
%E[\sum_i \log p(\theta|\alpha)]=N \log \Gamma(\sum_k \alpha_k)-N \sum_k \log \Gamma(\alpha_k)+N\sum_k(\alpha_k-1)\log \bar{\theta}_k
%\end{equation}
%where $\log \bar{\theta}_k=\frac{1}{N}\sum_i \Psi(n_{ik}+\alpha_k^{old}-\Psi(n_i+\sum_k\alpha_k^{old}))$.
%the Newton-Raphson algorithm need the gradient and Hessian to compute the new parameters.
%The gradient $g_k=N\Psi(\sum_k \alpha_k)+N\Psi(\alpha_k)+N\log \bar{\theta}_k$; The Hessian $\mathbf{H}=\mathbf{Q}+\mathbf{1}\mathbf{1}^T z$ where $Q_{jk}=-N\Psi'(\alpha_k)\delta(j-k)$ and $z=N\Psi'(\sum_k \alpha_k)$.\\
%Then one Newton step is therefore:
%\begin{eqnarray}
%\alpha^{new}&=&\alpha^{old}-\mathbf{H}^{-1}\mathbf{g} \\\nonumber
%\mathbf{H}^{-1}&=& \mathbf{Q}^{-1}-\frac{\mathbf{Q}^{-1}\mathbf{1}\mathbf{1}^T\mathbf{Q}^{-1}}{1/z+\mathbf{1}^T\mathbf{Q}^{-1}\mathbf{1}} \\\nonumber
%(\mathbf{H}^{-1}\mathbf{g})_k&=&\frac{g_k-b}{q_{kk}} \\\nonumber
%\textrm{where}~~ b &=& \frac{\sum_j g_j/g_{jj}}{1/z+\sum_j1/q_{jj}}
%\end{eqnarray}

\subsection{Selecting Aligned Topics}
\label{sec:aligned_topics}
We next identify learned topics (a set of related word-distributions) that truly represent an aligned topic across
languages, as opposed to an unrelated set of distributions for which there is no supporting alignment
evidence in the corpus.
We begin by measuring how often each topic occurs in code-switched documents. If a topic never
occurs in a code-switched document, then there can be no evidence to support alignment across languages.
For the topics that appear at least once in a code-switched document,
we estimate their probability in the code-switched documents by a MAP estimate of $\theta$.
Topics appearing in at least one code-switched document with probability greater than a threshold $p$ are selected as 
candidates for true cross-language topics.

\section{Data}
We used two datasets: a Sina Weibo Chinese-English corpus
\cite{wangling:acl2013} and a Spanish-English Twitter corpus.

\paragraph{Weibo} \newcite{wangling:acl2013} extracted over 1m Chinese-English parallel segments from Sina 
Weibo, which are code-switched messages. We randomly sampled 29,705 code-switched messages along with 
42,116 Chinese and 42,116 English messages from the the same time frame. We used these data for training. We 
then sampled 
an additional 2475 code-switched messages, 4221 English and 4211 Chinese messages as test data.

\paragraph{Olympics} We collected tweets from July 27, 2012 to August 12, 2012, and identified 302,775 tweets
about the Olympics based on related hashtags and keywords (e.g. olympics, \#london2012, etc.) 
We identified code-switched tweets using the Chromium Language Detector\footnote{\scriptsize{https://code.google.com/p/chromium-compact-language-detector/}}. This system provides the top three possible
languages for a given document with confidence scores; we identify a tweet as code-switched if two predicted languages
each have confidence greater than 33\%.
We then used the tagger of \newcite{Lignos:13} to obtain token level LID tags, and only tweets with tokens in both Spanish 
and English are used as code-switched tweets. In total we identified 822 Spanish-English code-switched tweets.
We further expanded the mined tweets to full conversations, yielding 1055 Spanish-English code-switched documents (including both tweets and conversations), along with 4007 English and 4421 Spanish tweets composes our data set. We reserve 10\% of the data for testing.
%TODO: I'm confused. How did we go from 822 tweets to 1055 conversations? Should ``documents'' we replaced
% by "yielding 1055 Spanish-English code-switched tweets in 822 conversations (documents).''

%We experimented with creating documents based on single tweets or conversations (from crawling Twitter).
%While the Olympics data contains many code-switched tweets (822), the Asia data had few (22). Additionally, in our experiments, both
%settings performed equally well for Olympics, but conversations were better for Asia. Therefore, we used conversations as code-switched documents and tweets
%for monolingual data.
%We used a LID system to find Spanish, English and Spanish/English code-switched tweets/conversations, including token level LID. From this corpus,
%We used 4007 English, 4421 Spanish and 1055 code-switched tweets.

%Since code-switched conversations are not common, to obtain more data, we used both code-switched single tweets and code-switched conversations. In Olympics data set, there are 233 conversations and 822 single tweets are code-switched. In Asia data set, there are 391 conversations and 22 single tweets are code-switched.





\begin{figure*}
\ffigbox
{
\hspace{-4cm}
 \begin{subfigure}[bl]{0.35\textwidth}
      \includegraphics[width=\textwidth]{figs/olympics.eps}
 \end{subfigure}   
 \hspace{-1.3cm}
 \begin{subfigure}[bl]{0.35\textwidth}
     \includegraphics[width=\textwidth]{figs/weibo.eps}
 \end{subfigure}
 \hspace{-1.3cm}
 \begin{subfigure}[bl]{0.3\textwidth}
    \scriptsize
  \begin{tabular}{ |l|ccc|ccc|}
  \hline
 $\mathcal{T}$$=$$60/120$ &\multicolumn{3}{|c|}{{\bf Olympics}} & \multicolumn{3}{|c|}{{\bf Weibo}}\\
  \hline
  & En & Es & CS & En & Cn & CS \\
  \hline
  LDA & 11.32 & 9.44 & 6.97 & 29.19 & 23.06 & 11.69 \\
  LDA-bg &  11.35 & 9.51 & 6.79 & 40.87 & 27.56 & 10.91 \\
  \hline
  csLDA & 8.72 & 7.94 & 6.17 & 18.20 & 17.31 & 12.72 \\
  csLDA-bg &  8.72 & 7.73 & 6.04 & 18.25 & 17.74 & 12.46 \\
  \hline
  csLDA-bg &  8.73 & 7.93 & 4.91 & - & - & - \\
  with LID & & & & & & \\
 \hline
\end{tabular}
\end{subfigure}
}
{
  \caption{Plots show perplexity for different $\mathcal{T}$ (Olympics left, Weibo right). Perplexity in the table are in magnitude of $1\times 10^3$.
  }%csLDA always improves over LDA.
\label{fig:perplexity}
}
\end{figure*}
\section{Experiments}
We evaluated csLDA on the two datasets and evaluated each model using perplexity on held out data and human judgements.
While our goal is to learn polylingual topics, we cannot compare to previous polylingual models since they require comparable data, 
which we lack. Instead, we constructed a baseline from
LDA run on the entire dataset (no language information.)
For each model, we measured the document completion perplexity \cite{rosen2004author} on the held out data.
We experimented with different numbers of topics ($\mathcal{T}$). Since csLDA duplicates topic
distributions ($\mathcal{T}\times\mathcal{L}$)
we used twice as many topics for LDA.

Figure \ref{fig:perplexity} shows test perplexity for varying $\mathcal{T}$ and perplexity  for the best setting of csLDA ($\mathcal{T}$
$=$$60$) and LDA ($\mathcal{T}$$=$$120$). The table lists both monolingual and code-switched test data;
csLDA improves over LDA in almost every case, and across all values of $\mathcal{T}$.
The background distribution (-bg) has mixed results for LDA, whereas for
csLDA it shows consistent improvement.
Table \ref{tab:topics} shows some csLDA topics. While there are some mistakes, overall the topics are coherent and aligned.

We use the available per-token LID system \cite{Lignos:13} for Spanish/English
to justify csLDA's ability to infer the hidden language variables. 
We ran csLDA-bg with $l_i$ set
to the value provided by the LID system for code-switched documents (csLDA-bg with LID),
which gives csLDA high quality LID labels. 
While we see gains for the code-switched data,
overall the results for csLDA-bg and csLDA-bg with LID are similar, suggesting that the model can
operate effectively even without a supervised per-token LID system.


\begin{table*}%[t]
\ffigbox
{
\footnotesize
\center
\begin{subtable}{.5\textwidth}
\centering

\begin{tabular}{|c|c||c|c|}
\hline
\multicolumn{2}{|c||}{{\bf Football}} & \multicolumn{2}{|c|}{{\bf Basketball}} \\
English & Spanish & English & Spanish \\
\hline
mexico & mucho & game & espaÃ±a\\
brazil & argentina & basketball & baloncesto \\
soccer & mÃ©xico & year & basketball \\
vs & brasil & finals & bronce \\
womens & ganarÃ¡ & gonna & china \\
football & tri & nba & final \\
mens & yahel\_castillo & obama & rusia\\
final & delpo & lebron & espaÃ±ola\\
\hline
\end{tabular}
%\caption{Two aligned topics for English-Spanish pair.}
%\label{fig:topics}
\end{subtable}%
  %  }
\begin{subtable}{.5\textwidth}
\centering

\begin{tabular}{|c|c||c|c|}
\hline
\multicolumn{2}{|c||}{{\bf Social Media}} & \multicolumn{2}{|c|}{{\bf Transportation}} \\
English & Chinese & English & Chinese \\
\hline
twitter & \begin{CJK}{UTF8}{gbsn} ååå \end{CJK}  & car & \begin{CJK}{UTF8}{gbsn} æ±½è½¦ \end{CJK}\\
bitly & \begin{CJK}{UTF8}{gbsn} å¾®å \end{CJK} & drive & \begin{CJK}{UTF8}{gbsn} è¿ä¸ª \end{CJK} \\
facebook & \begin{CJK}{UTF8}{gbsn} æ´æ° \end{CJK} & road & \begin{CJK}{UTF8}{gbsn} çç \end{CJK} \\
check & \begin{CJK}{UTF8}{gbsn} ä¸è½½ \end{CJK} & line & \begin{CJK}{UTF8}{gbsn} æå¹´ \end{CJK} \\
use & \begin{CJK}{UTF8}{gbsn} è½¬å \end{CJK} & train & \begin{CJK}{UTF8}{gbsn} èªè¡è½¦ \end{CJK} \\
blog & \begin{CJK}{UTF8}{gbsn} è§é¢ \end{CJK} & harry & \begin{CJK}{UTF8}{gbsn} è½¦å \end{CJK} \\
free & \begin{CJK}{UTF8}{gbsn} pm \end{CJK} &  \begin{CJK}{UTF8}{gbsn} æ±½è½¦ \end{CJK}  & \begin{CJK}{UTF8}{gbsn} å¥é©° \end{CJK} \\
post & \begin{CJK}{UTF8}{gbsn} æ¨ç¹  \end{CJK} & bus & \begin{CJK}{UTF8}{gbsn} å¤§ä¼ \end{CJK} \\
\hline
\end{tabular}
%\caption{Two aligned topics for English-Chinese pair.}
%\label{fig:topics}
\end{subtable}%
}
{%
      \caption{Examples of aligned topics from Olympics (left) and Weibo (right).}%
      \label{tab:topics}
}
\end{table*}

%\begin{figure}
%\small
%\center
%\begin{tabular}{|c|c||c|c|}
%\hline
%\multicolumn{2}{|c||}{{\bf Football}} & \multicolumn{2}{|c|}{{\bf Basketball}} \\
%English & Spanish & English & Spanish \\
%\hline
%mexico & mucho & game & espaÃ±a\\
%brazil & argentina & basketball & baloncesto \\
%soccer & mÃ©xico & year & basketball \\
%vs & brasil & finals & bronce \\
%womens & ganarÃ¡ & gonna & china \\
%football & tri & nba & final \\
%mens & yahel\_castillo & obama & rusia\\
%final & delpo & lebron & espaÃ±ola\\
%\hline
%\end{tabular}
%\caption{Olympics topic distributions for each language. }
%\label{fig:topics}
%\end{figure}


%\begin{figure*}[t]
%\ffigbox{
%\begin{center}
%  \begin{subfloatrow}
%  \includegraphics[width=0.34\textwidth]{figs/olympics.eps}
%  \label{fig:results}
%\hspace{-1.5cm}
%\qquad
%  \includegraphics[width=0.34\textwidth]{figs/asia.eps}
%  \label{fig:sparse_results}
%\hspace{-1.5cm}
%\qquad
% \tiny
%\begin{tabular}{|c|c||c|c|}
%\hline
%\multicolumn{2}{|c||}{{\bf Football}} & \multicolumn{2}{|c|}{{\bf Basketball}} \\
%English & Spanish & English & Spanish \\
%\hline
%mexico & mucho & game & espaÃ±a\\
%brazil & argentina & basketball & baloncesto \\
%soccer & mÃ©xico & year & basketball \\
%vs & brasil & finals & bronce \\
%womens & ganarÃ¡ & gonna & china \\
%football & tri & nba & final \\
%mens & yahel\_castillo & obama & rusia\\
%final & delpo & lebron & espaÃ±ola\\
%\hline
%\end{tabular}
%\label{fig:topics}
%\end{center}
%}
%{
%\caption{caption}
%}
%\label{fig:perplexity}
%\end{figure*}



\subsection{Human Evaluation}
We evaluate topic alignment quality through a human judgements \cite{chang2009reading}.
For each aligned topic, we show an annotator the 20 most frequent words from the foreign language topic (Chinese or Spanish)
with the 20 most frequent words from the aligned English topic and two random English topics.
The annotators are asked to select the most related English topic among the three; the one with the most votes is 
considered the aligned topic. We count how often the model's alignments agree.

LDA may learn comparable topics in different languages
but gives no explicit alignments. 
We create alignments 
by classifying each LDA topic by language using the KL-divergence between the topic's words distribution and a word distribution
for the English/foreign language inferred from the monolingual documents.
Language is assigned to a topic by taking the minimum KL. For Weibo data, this was not effective since the vocabularies
of each language are highly unbalanced. Instead, we manually labeled the topics by language.
We then pair topics across languages using the cosine similarity of their co-occurrence statistics in code-switched documents. 
Topic pairs with similarity above $t$ are considered aligned topics. We also used a threshold $p$ (\S \ref{sec:aligned_topics})
to select aligned topics in csLDA.
To ensure a fair comparison, we select the same number of aligned topics for LDA and csLDA.\footnote{We used thresholds $p = 0.2$ and $t = 0.0001$. We limited the model with more alignments to match the one with less.}.
%Note that this penalizes csLDA, which learns many more aligned topics than LDA. Our evaluation will then focus only
%on the quality of the alignments (precision) and not on how many were discovered (recall.)
%When the thresholdings of csLDA and LDA return different number of aligned topics, we adopt the smaller one. We used thresholds $p = 0.2$ and $t = 0.0001$ applying to the best setting with $\mathcal{T}$$=$$60$ for csLDA (accordingly $\mathcal{T}$$=$$120$ for LDA) to select aligned topics. 
We used the best performing setting: csLDA $\mathcal{T}$$=$$60$, LDA $\mathcal{T}$$=$$120$, which produced
12 alignments from Olympics and 28 from Weibo.

%TODO: Move to text to save space.
%\begin{table}[h]
%\small
%\center
%\begin{tabular}{|c|c||c|c|}
%\hline
%\multicolumn{2}{|c||}{{\bf Olympics}} & \multicolumn{2}{|c|}{{\bf Weibo}} \\
%\hline
%LDA & csLDA & LDA & csLDA \\
%\hline
%0.25 & 0.5 & 0.7143 & 0.75 \\
%\hline
%\end{tabular}
%\caption{Quality of topic alignment using human judgments. }
%\label{tab:accuracy}
%\end{table}


Using Mechanical Turk we collected multiple judgements per alignment.
For Spanish, we removed workers who disagreed with the majority more than 50\% of the time (83 deletions),
leaving 6.5 annotations for each alignment (85.47\% inter-annotator agreement.)
For Chinese, since quality of general Chinese turkers is low \cite{Pavlick-EtAl-2014:TACL}
we invited specific workers and obtained 9.3 annotations per alignment (78.72\% inter-annotator agreement.)
For Olympics, LDA alignments matched the judgements 25\% of the time, while csLDA matched 50\% of the time.
While csLDA found 12 alignments and LDA 29, the 12 topics evaluated from both models show that csLDA's alignments
are higher quality.
For the Weibo data, LDA matched judgements 71.4\%, while csLDA matched 75\%. Both obtained high quality alignments --
likely due both to the fact that the code-switched data is curated to find translations and we hand labeled topic language --
but csLDA found many more alignments: 60 as compared to 28. These results confirm our automated results: csLDA finds higher 
quality topics that span both languages.



%10 annotations per alignment using Mechanical Turk. Since the quality of Chinese annotators on Mechanical Turk is awful(citation?), we actually get 10 graduate students whose native language is Mandarin to annotate the Weibo Data. To guarantee the quality of the annotation, we also removed workers who disagreed with the majority more than 50\% of the time, which results in 83 deletions for Olympics data and 0 deletion for Weibo data. After the removal, there are on average 6.5 annotations left for each aligned topics in Olympic data, and the finally inter-annotator agreement is 85.47\% on Olympics data and 78.57\% on Weibo data. The accuracy of the mined topic alignments in accordance with human judgement is shown in table \ref{tab:accuracy}. On the Olympics data, it is clear that csLDA infers better topic alignments across languages; on the Weibo data, although LDA achieves the same topic alignment accuracy as csLDA, we should aware that we favored LDA in the evaluation by choosing the best Topic number according to LDA, and we also did manual annotation for the topic languges for LDA, which ruled out a considerable amount of possible errors.




%Along with the prosperity of social networks, users from different countries contribute to build a large diverse virtual world. The use of miscellaneous languages and dialects provides a treasure trove for the study of linguistics. Moreover, it becomes a more and more prevalent phenomenon that people who know several languages tend to post ``code-switched tweets" on twitter. A ``code-switched tweet" refers to a post that written mainly in one language, but borrows some words, phrases or even sentences from other languages. Intuitively, some events, organizations, person names and set phases may be more natural to be mentioned in their original languages. Figure (1) shows an example of a code-switched tweet whose major language is Malay but also with many tokens in English.
%
%This interesting phenomenon provides us with a great resource to discover the connections between languages. Intuitively, different tokens in a single tweet should be about the same topics. If we can capture this character, the words from different languages can be automatically aligned according to common topics. This alignment can be beneficial for many applications, such as cross-language prediction(especially for semantics and sentiment) and machine translation.
%
%Topic model is a popular technique to discover semantically coherent ``topics" from
%text in an unsupervised fashion. However, the current topic models only apply within a single
%language or on aligned parallel corpus. In this paper, we propose a novel topic model that simultaneously model the language shifting and the topic coherence of a single tweet. We call it Code-Switch Topic Model(SCTM).
%
%
%
%\section{Proposed Model}
%The intuition behind our proposed model is that: although we have no knowledge about the correspondence between different languages, people usually talk about the same topic within a tweet. Therefore, words from different languages concurring in the same tweets tend to have similar semantic meanings. For example, if the English word ``medal'' appears frequently in some tweets with some Chinese words, one can infer with confidence that those Chinese words are semantically related to a sport event without knowing anything about Chinese. The classical LDA models the co-occurrence of some words as a cluster call ``topic'', and assume each document has a topic distribution characterizing that document. Here we also use the concept of ``topic'' for words clustering and the distribution of the topics for document representation; what is different is that each language has its own word clustering pattern for the same topic, which also implicitly aligns words from different languages through the topic. In order to learn the topics for different languages, each word in a tweet is also associated with a latent variable which indicates the language tag for it, and will be inferred automatically.
%\subsection{Generative Process}
%A code-switched tweet is generated by the following steps:\\
%\vspace{-0.5cm}
%\begin{enumerate}
%\item For each tweet, draw a topic distribution from a Dirichlet Distribution: $\theta \sim Dir(\theta; \alpha_m)$;  \\
%draw a language distribution from a Dirichlet Distribution: $\psi \sim Dir(\psi; \alpha_l)$ (During the inference phase, we can utilize observations to guide gibbs sampling to sample within the languages observed)
%\item Then draw a language assignment for each token: $l \sim P(l|\psi)$;\\
%draw a topic assignment for each token: $z \sim P(z|\theta)$.
%\item For each language $l$, sample the topic-word distribution:$\Phi^l \sim Dir(\Phi^l;\beta^l)$.
%\item Finally, the observed words are generated given its topic assignment, language assignment and a language specific topic-word distribution: \\
%$w^l \sim P(w^l | z, \Phi^l)$.
%\end{enumerate}the
%For the setting that language tags are given, the generative story just discards the steps of generating a language distribution and generating the language assignment for each token. Fig. \ref{fig:model} shows the graphical model.
%
%
%
%\subsection{Learning and Inference}
%
%\subsubsection{Hyper-parameter Learning}
%We want to estimate the hyper-parameters using MLE estimates in the training procedure. However, the latent variables in the model make it impossible to obtain a closed-form of the likelihood function. We instead use an EM algorithm to iteratively learn the hyper-parameters. In the E step, we fix the current estimates of the hyper-parameters and use Gibbs sampling to infer the posterior of latent variables. The procedure is detailed in \ref{sec:inf}. In the M step, we use the posterior to compute the expected log-likelihood of the model and nest the Newton-Raphson algorithm to iteratively obtain the MLE of the hyper-parameters. Since in our model the hyper-parameters are all for Dirichlet distribution, the expected likelihood is
%\begin{equation}
%E[\sum_i \log p(\theta|\alpha)]=N \log \Gamma(\sum_k \alpha_k)-N \sum_k \log \Gamma(\alpha_k)+N\sum_k(\alpha_k-1)\log \bar{\theta}_k
%\end{equation}
%where $\log \bar{\theta}_k=\frac{1}{N}\sum_i \Psi(n_{ik}+\alpha_k^{old}-\Psi(n_i+\sum_k\alpha_k^{old}))$.
%the Newton-Raphson algorithm need the gradient and Hessian to compute the new parameters.
%The gradient $g_k=N\Psi(\sum_k \alpha_k)+N\Psi(\alpha_k)+N\log \bar{\theta}_k$; The Hessian $\mathbf{H}=\mathbf{Q}+\mathbf{1}\mathbf{1}^T z$ where $Q_{jk}=-N\Psi'(\alpha_k)\delta(j-k)$ and $z=N\Psi'(\sum_k \alpha_k)$.\\
%Then one Newton step is therefore:
%\begin{eqnarray}
%\alpha^{new}&=&\alpha^{old}-\mathbf{H}^{-1}\mathbf{g} \\\nonumber
%\mathbf{H}^{-1}&=& \mathbf{Q}^{-1}-\frac{\mathbf{Q}^{-1}\mathbf{1}\mathbf{1}^T\mathbf{Q}^{-1}}{1/z+\mathbf{1}^T\mathbf{Q}^{-1}\mathbf{1}} \\\nonumber
%(\mathbf{H}^{-1}\mathbf{g})_k&=&\frac{g_k-b}{q_{kk}} \\\nonumber
%\textrm{where}~~ b &=& \frac{\sum_j g_j/g_{jj}}{1/z+\sum_j1/q_{jj}}
%\end{eqnarray}
%
%\subsubsection{Inference}
%\label{sec:inf}
%We use block Gibbs sampling to do the inference. The language specific topic-word distributions $\Phi^l$ are obtained by MAP estimation. Topic and language assignments for a test tweet $(w_1, ..., w_n)$ can be jointly inferred using Gibbs sampling, which involves sequentially resampling each token $t_i$ from its conditional posterior:\\
%
%\begin{eqnarray}
%&P(t_i = <z, l> | (w_1, ..., w_n), (z_1, ..., z_n)_{\backslash i}, (l_1, ..., l_n)_{\backslash i},  \alpha^m, \alpha^l, \beta^l)\\
%&= \frac{(N_{w_i}^{l,z})_{\backslash i} + \beta^l_{w_i}}{-1+\sum_w N_w^{l,z} + \beta^l_w} \times \frac{(N_z)_{\backslash i} + \alpha^m_z}{-1+\sum_z N_z + \alpha^m_z} \times \frac{(N_l)_{\backslash i} + \alpha^l_l}{-1+\sum_l N_l + \alpha^l_l}
%\end{eqnarray}
%where $(z_1, ..., z_n)_{\backslash i}$ and $(l_1, ..., l_n)_{\backslash i}$ are the current set of topic and language assignments for all other tokens in the tweet, respectively.
%
%The inference is nested in the learning procedure as the E step; It is also called in the test precedure where parameters $\Phi$ are fixed to compute the perplexity.
%
%\subsection{Evaluation}
%We evaluate our model in terms of document completion perplexity on some held-out test data and human judgment on its ability to mine topically aligned words from different languages. The performance of our model is compared to several baselines.
%\subsubsection{document completion perplexity}
%Perplexity is a measurement on a model's predictive power on held-out test data. Many topic models adopt perplexity as a standard evaluation technique. It is defined as:
%$$perplexity = 2^{-\sum_{i=1}^N \frac{1}{N} \log_2 q(x_i)}$$
%The intuition behind this measure is: given a proposed probability model q, one may evaluate q by asking how well it predicts a separate test sample $x_1, x_2, ..., x_N$ also drawn from p.
%
%We use a variation of perplexity measurement which is called document completion perplexity. The modification it makes to traditional perplexity is: instead of measuring the predictive power on a whole document, it compares predictive performance by estimating the probability of the second half of a document, given the first half.
%
%The results are compared with the classic LDA model with different setting:
%\begin{enumerate}
%\item A standard LDA is trained on the whole data set in which we do not distinguish tokens from different languages.
%\item Two standard LDAs are trained on pure language tweets, each for a language.
%\item Two standard LDAs are trained on pure language and code-switched tweets, each for a language.
%\end{enumerate}
%
%\subsubsection{word alignment}
%We also evaluate our model on its ability to align words in different languages via common topics. To obtain human judgements on the quality of topic alignment in different languages, we turn to Amazon Mechanical Turk(although we have not done this yet due to time limit, we have prepared data and will definitely do it sooner).
%
%The evaluate experiment is designed as follow: the judgers will be presented an English topic(in terms of the top 20 words) as well as 3 different Spanish topics(each topic is represented by 3 top words randomly sampled from the top 20 words in that topic). The aligned Spanish topic(returned by the model) must be in the 3 Spanish topics, and the judgers will be asked to choose which topic is aligned to the English topic. No alignment will always be a choice. An accuracy will thus be computed according to the human judgement.
%
%In this evaluation, our model will be compared with the aligned topics from two pure language LDAs. The topic alignment of two LDAs are done by finding the most similar topic pairs in different languages. The similarity is measured by the concurrence of their topic words in code-switched data.
%
%
%\section{Experiment}
%See the poster.
%
%\subsection{Setup and Data}
%The data we used is a collection of two-months' public tweets that are collected during 2012 London Olympic game period, and are filtered by some Olympiad related keywords. There are altogether 302775 tweets in which 276162 are pure English, 16094 are pure Spanish and 10519 are Code-Switched. In the code-switched tweets, there are 106138 English tokens and 54546 Spanish tokens.
%
%We held out 26755 pure English tweets and 1403 pure Spanish tweets as test data; the performance  of different models on different data will be compared and evaluated on the test data.
%
%\subsection{Results}
%See the poster.
%\subsubsection{Learning curve}
%See the poster.
%\subsubsection{Document Completion Perplexity}
%See the poster.
%\subsubsection{Topic Top-words Exhibition}
%See the poster.
%
%\section{Discussion and Future Work}
%See the poster.
%



\bibliographystyle{acl}
\bibliography{refs}


\end{document}
