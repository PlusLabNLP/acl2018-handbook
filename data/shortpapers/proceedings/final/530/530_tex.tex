\documentclass[11pt,letterpaper]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{comment}
\usepackage{multicol}
\usepackage{url}

% % prevents hyperref from distorting ACL bibfile style: see http://tex.stackexchange.com/questions/124410/hyperref-modifies-bibliography-style-of-acl-style-files 
% \makeatletter
% \newcommand{\@BIBLABEL}{\@emptybiblabel}
% \newcommand{\@emptybiblabel}[1]{}
% \makeatother
% \usepackage[colorlinks]{hyperref}

\usepackage{graphicx}
\usepackage{listings}
\usepackage{enumitem}
\usepackage[disable]{todonotes}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algnewcommand{\LineComment}[1]{\State \(\triangleright\) #1}
\usepackage[tone]{tipa}
\usepackage{xspace}

%\usepackage[compact]{titlesec}
\usepackage[small]{caption}
\usepackage{subcaption}
\setlength{\belowcaptionskip}{-7pt}

\setlength\titlebox{2.7cm}

\newcommand{\Sigmax}{\Sigma_{\text x}}
\newcommand{\Sigmay}{\Sigma_{\text y}}
\newcommand{\bos}{\textsc{bos}\xspace}
\newcommand{\eos}{\textsc{eos}\xspace}
\renewcommand{\th}{\raisebox{0.2\baselineskip}{\scriptsize th}\xspace}
\newcommand{\defeq}{\stackrel{\mbox{\tiny def}}{=}}
\newcommand{\ptheta}{{p_\theta}}

\newcommand{\ryan}[1]{\todo[color=green!40,author=Ryan]{#1}}
\newcommand{\jason}[1]{\todo[color=blue!40,author=Jason]{#1}}
\newcommand{\adi}[1]{\todo[color=orange!40,author=Adi]{#1}}
\newcommand{\violet}[1]{\todo[color=purple!40,author=Violet]{#1}}

\newcommand{\SkipForSpace}[1]{} 

\title{Stochastic Contextual Edit Distance and Probabilistic FSTs}
 \author{Ryan Cotterell \and Nanyun Peng \and Jason Eisner \\
   Department of Computer Science, Johns Hopkins University \\
  {\tt \{ryan.cotterell,npeng1,jason\}@cs.jhu.edu }
}
\begin{document}
\maketitle


\begin{abstract}
  String similarity is most often measured by weighted or unweighted edit distance $d(x,y)$.  \newcite{ristad1998learning} defined 
% the related notion of 
{\em stochastic} edit distance---a probability distribution $p(y \mid x)$ whose parameters can be trained from data.  We generalize this
%  notion 
so that the probability of choosing each edit operation can depend on contextual features.  We show how to construct and train a probabilistic finite-state transducer that computes our stochastic contextual edit distance.  
% OLD
% We propose a novel finite-state machine topology that we term
% \emph{contextual edit} machines.  \jason{what is important is that this is not just a measurement of edit distance (although the same topology could be used for that!), but computes a normalized probability distribution over output strings for each input string, which is not done by the Boltzmann distribution associated with edit distance.  Cite Ristad \& Yianilos.}  It generalizes the edit distance
% machine by allowing contextually-dependent edits. We provide an
% algorithm for the construction of the machine.  
% Additionally, we
% describe a log-linear parameterization of the machine and show how the
% weights on the machine can be trained efficiently using the
% expectation semiring. 
To illustrate the improvement from conditioning on context,
we model typos found in social media text. 
\end{abstract}

\section{Introduction}

Many problems in natural language processing can be viewed as stochastically mapping one string to another: e.g., transliteration, pronunciation modeling, phonology, morphology, spelling correction, and text normalization.  \newcite{ristad1998learning} describe how to train the parameters of a stochastic editing process that moves through the input string $x$ from left to right, transforming it into the output string $y$.  In this paper we generalize this process so that the edit probabilities are conditioned on input and output context. 

%EXAMPLE CUT - GOOD EXAMPLE THOUGH
\SkipForSpace{To give a simple example, consider the case of mapping an English word to 
its pronunication.  The difference between \emph{plan} and \emph{plane} 
lies in presence of a silent \emph{e}. We want \emph{a} $\mapsto$ 
/\textipa{\ae}/ in the former case, but \emph{a} $\mapsto$ /e\textipa{I}/
in the later. A finite-state transducer needs
access to right input context in order to correctly learn proper
mapping.}

%\jason{possibly put an example here.  But I don't like the German example because it could be handled by a preprocessing transducer that converts digraphs and trigraphs to single characters.  Also, why invoke German when you could do the same example in English using the ``th'' digraph?  You know your readers speak English, so going to German makes the example less accessible and makes it seem like you had to stretch to find a place where this work is useful.}
% Consider the case of mapping German orthography to pronunciation. In general, we would want to map \textbf{h} $\mapsto$ /h/, but in the context of $sc$, we would map \textbf{h} $\mapsto$ /\textipa{S}/ because \textit{sch} serves as a trigraph representing one phoneme \cite{hammer1977german}.  

We further show how to model the conditional distribution $p(y \mid x)$ as a probabilistic finite-state transducer (PFST), which can be easily combined with other transducers or grammars for particular applications.  We contrast our {\em probabilistic} transducers with the more general framework of {\em weighted} finite-state transducers (WFST), explaining why our restriction provides computational advantages when reasoning about unknown strings.

Constructing the finite-state transducer is tricky, so we give the explicit construction for use by others.  We describe how to train its parameters when the contextual edit probabilities are given by a log-linear model.  We provide a library
for training both PFSTs and WFSTs that works with OpenFST \cite{allauzen2007openfst}, and we illustrate its use with simple experiments on typos, which demonstrate the benefit of context.

% Finite-state methods provide a natural and flexible modeling approach for these problems.  
% However, potential users of finite-state techniques must choose among an infinite number of topologies. The choice of topology influences both the efficiency of the training and the richness of the relation described.  
% This work helps fill this gap with the introduction of a novel finite-state construction that we believe is general enough to aid in many common NLP tasks. In addition, we provide an algorithm for its construction and details on how to efficiently train the transducer with a log-linear parameterization using EM. 

% We also discuss general properties of locally normalized finite-state machines, many of which are non-obvious, and show the improved performance on modeling spelling correction.


% \ryan{citations here ?. Cite Markus. Why is this different than Markus's work?
% Markus did not discuss a topology, and in fact it's trivial in his case, but once you have local (per-state) normalization, you need a concept of which actions are competing given a past history, and therefore you are distinguishing past from future; you can't see future output, and to get future input you need lookahead, so it's more complicated. Defend why we are doing a locally normalized version. 
% What if you want to transduce an uncertain input -- e.g., input is the result of ASR or OCR or is missing diacritics or is an unknown underlying form?  You may want to marginalize over input, or reconstruct best input in a noisy-channel model.  You may also have directed graphical model over strings (not clear if we should discuss this), where the input is passed in by a message from a related string?  If there are exponentially or infinitely many inputs, you can't afford to compute a separate normalizing constant for each, and if you did, you would lose the benefit of dynamic programming and share
% Why normalized machines are important.
% Why locally normalized? Motivate what we are doing in the introduction. The topologies for globally 
% normalized machines are quite simple. Non trivial to find a topology that finds a . Uncertain
% input. Not endorsing locally normalized models. Probability distribution over input strings. 
% OCR strings. Reconstruct the uncertain input. Underlying form that we've reconstructed. Put in a footnote.
% Model of unbounded size. You can't normalize over that. I.i.d. samples from a Gaussian. You can't globally
% normalize that. Marginalize over some unknown number of unseen points. (for the paper
% where we talk about underlying forms; save that point for the underlying forms paper) 


% TWO Typoes of local normalization. Directed arc and per state in finite-state type machien type.
% Per state and per factor.

% Easier to train locally normalized machines. Faster to train. Suffer from label bias.
% Never have to compute global normalizing constant. A special case of this is 
% when there's no input. It can be used to generate strings (normalized per-staet).
% }

\section{Stochastic Contextual Edit Distance}\label{sec:csed}

Our goal is to define a family of probability distributions $\ptheta(y\mid x)$, where $x \in \Sigmax^*$ and $y \in \Sigmay^*$ are input and output strings over finite alphabets $\Sigmax$ and $\Sigmay$, and $\theta$ is a parameter vector.  

Let $x_i$ denote the $i$\th character of $x$.  If $i < 1$ or $i > |x|$,  
then $x_i$ is the distinguished symbol $\bos$ or $\eos$ (``beginning/end of string'').  Let $x_{i:j}$ denote the $(j-i)$-character substring $x_{i+1}x_{i+2}\cdots x_j$.

Consider a stochastic edit process that reads input string $x$ while writing output string $y$.  Having read the prefix $x_{0:i}$ and written the prefix $y_{0:j}$, the process must stochastically choose one of the following $2|\Sigmay|+1$ edit operations: 
\begin{itemize}[noitemsep]
\item {\sc delete}: Read $x_{i+1}$ but write nothing.
\item {\sc insert}$(t)$ for some $t \in \Sigmay$: Write $y_{j+1}=t$ without reading anything.
\item {\sc subst}$(t)$ for some $t \in \Sigmay$: Read $x_{i+1}$ and write $y_{j+1}=t$.  Note that the traditional {\sc copy} operation is obtained as
{\sc subst}$(x_{i+1})$.
\end{itemize}
In the special case where $x_{i+1}=\eos$, the choices are instead {\sc insert}$(t)$ and {\sc halt} (where the latter may be viewed as copying the \eos symbol).

The probability of each edit operation depends on $\theta$ and is conditioned on the left input context $C_1 = x_{(i-N_1):i}$, the right input context $C_2 = x_{i:(i+N_2)}$ %\violet{should this be $C_2 = x_{i+1:(i+N_2)}$?} \jason{no, read the definition of colon at the start of this section}
, and the left output context $C_3 = y_{(j-N_3):j}$, where the constants $N_1, N_2, N_3 \geq 0$ specify the model's context window sizes.\footnote{\label{fn:eosbit}If $N_2=0$, so that we do not condition on $x_{i+1}$, we must still condition on whether $x_{i+1}=\eos$ (a single bit).  We gloss over special handling for $N_2=0$; but it is in our code.}
%\jason{in future work, mention variable length}
Note that the probability cannot be conditioned on right output context because those characters have not yet been chosen.
%
Ordinary stochastic edit distance \cite{ristad1998learning} is simply the case $(N_1,N_2,N_3)=(0,1,0)$, while \newcite{bouchard2007probabilistic} used roughly $(1,2,0)$.

Now $\ptheta(y \mid x)$ is the probability that this process will write $y$ as it reads a given $x$.  This is the total probability (given $x$) of {\em all} latent edit operation sequences that write $y$.  In general there are exponentially many such sequences, each implying a different alignment of $y$ to $x$.   


This model is reminiscent of conditional models in MT that perform stepwise generation of one string or structure from another---e.g., string alignment models with contextual features \cite{cherry2003probability,liu2005log,dyer2013simple}, or tree transducers \cite{knight-graehl-2005}.


\section{Probabilistic FSTs}\label{sec:pfst}

We will construct a {\bf probabilistic finite-state transducer (PFST)} that compactly models $\ptheta(y\mid x)$ for all $(x,y)$ pairs.\footnote{Several authors have given recipes for finite-state transducers that perform a {\em single} contextual edit operation \cite{kaplan1994regular,mohri1996efficient,gerdemann-vannoord-1999}.  Such ``rewrite rules'' can be individually more expressive than our simple edit operations of section~\ref{sec:csed}; but it is unclear how to train a cascade of them to model $p(y \mid x)$.}  Then various computations with this distribution can be reduced to standard finite-state computations that efficiently employ dynamic programming over the structure of the PFST, and the PFST can be easily combined with other finite-state distributions and functions \cite{mohri1997finite,eisner2001expectation}.

A PFST is a two-tape generalization of the well-known nondeterministic finite-state acceptor.  It is a finite directed multigraph where each arc is labeled with an input in $\Sigmax \cup \{\epsilon\}$, an output in $\Sigmay \cup \{\epsilon\}$, and a probability in $[0,1]$.  ($\epsilon$ is the empty string.)  Each state (i.e., vertex) has a halt probability in $[0,1]$, and there is a single initial state $q_{\text{I}}$.\SkipForSpace{\footnote{To put PFSTs into the more general computational framework of weighted automata \cite{mohri2002semiring}, the probabilities should be regarded as weights in the $(+,\times)$ semiring.}} Each path from 
% CUT FOR SPACE
% the initial state 
$q_{\text{I}}$ to a final state $q_{\text{F}}$ has
\begin{itemize}[noitemsep]
\item an input string $x$, given by the concatenation of its arcs' input labels;
\item an output string $y$, given similarly;
\item a probability, given by the product of its arcs' probabilities and the halt probability of $q_{\text{F}}$.
\end{itemize}
We define $p(y \mid x)$ as the total probability of all paths having input $x$ and output $y$.
% CUT BECAUSE DISTRACTING
% \footnote{The ``Viterbi approximation'' uses the maximum probability instead of the total probability.  This can be handled in the same framework, combining the weights using the $(\max,\times)$ semiring rather than the $(+,\times)$ semiring.}
In our application, a PFST path corresponds to an edit sequence that reads $x$ and writes $y$.  The path's probability is the probability of that edit sequence given $x$.

% OLD
% A \textit{rational relation} is a relation between strings that can be expressed as a finite-state transducer, a two tape generalization of the well-studied finite-state acceptor.  A Weighted Finite-State Transducer (WFST) is a simple generalization where each arc is assigned a weight.  A WFST is said to be \textit{stochastic} when it satisfies the axioms of a probability measure.

% Formally, we are interested in a joint probability distribution
% $\ptheta  : \Sigmax^* \times \Sigmay^* \rightarrow
% \mathbb{R}_{\geq 0}$. We will refer to $\Sigmax$ as the \textbf{input language}
% and $\Sigmay$ as the \textbf{output language} throughout the
% paper. \jason{xerox terminology; change to input and output?}  This 
% stems from the convention of writing letters in $\Sigmax$ above the arc
% and letters from $\Sigmax$ below it. Both inference and decoding is tractable
% through the use of standard graph algorithms \cite{mohri1997finite}.
% WFSTs are also typically parameterized by a semiring, an algebraic
% structure that provides a framework for weights and operations over
% them \cite{mohri2002semiring}. This generalization provides a
% common framework for the various weights typically used. 

% It is known how to train such transducers using well-known algorithms \cite{eisner2001expectation}.
%One useful semiring for
%training the weights of a stochastic transducer is the \emph{expectation
%semiring} \cite{eisner2001expectation}. Running the well-known forward
%algorithm over a transducer with weights in the expectation semiring
%returns the gradient necessary for optimizing the weights.

%\ryan{discuss the deterministic triphone machine and the sproat machines for single contextual edits in unbounded context}

We must take care to ensure that for any $x \in \Sigmax^*$, the total probability of all paths accepting $x$ is 1, so that $\ptheta(y\mid x)$ is truly a conditional probability distribution.  This is guaranteed by the following sufficient conditions (we omit the proof for space), which do not seem to appear in previous literature:
\begin{itemize}
\item For each state $q$ and each symbol $b \in \Sigmax$, the arcs from $q$ with input label $b$ or $\epsilon$ must have total probability of 1.  (These are the available choices if the next input character is $x$.)
\item For each state $q$, the halt action and the arcs from $q$ with input label $\epsilon$ must have total probability of 1.  (These are the available choices if there is no next input character.)
\item Every state $q$ must be co-accessible, i.e., there must be a path of probability $> 0$ from $q$ to some $q_F$.  (Otherwise, the PFST could lose some probability mass to infinite paths.
%: you
 %  could get stuck in a loop reading $\epsilon$ forever without any
%   way to halt.  
The canonical case of this involves an loop $q \rightarrow q$ with input label $\epsilon$ and probability 1.)
\end{itemize}
We take the first two conditions to be part of the {\em definition} of a PFST.  The final condition requires our PFST to be ``tight'' in the same sense as a PCFG \cite{chi-geman-1998}, although the tightness conditions for a PCFG are more complex.  %\jason{in future work, include proof?  discuss $\phi$ and $\rho$ transitions?}
% When these conditions are satisfied, the PFST defines a probability
% distribution over \emph{finite} paths accepting the string $x$.  
% A \textbf{locally normalized conditional WFST} encodes a conditional
% probability distribution over a set of strings . To better understand
% the mechanics of the machine, first consider just \textbf{local
% normalization}. A WFST is said to be locally normalized
% (Markovian), if for each state in the machine, the sum of all
% out-going arcs is unity.
% \footnote{This is only a \textit{sufficient} condition for a WFST to
% represent a probability distribution. We could, for instance, train a
% globally normalized model.  See \cite{eisner2002parameter} for more
% details}
%\ryan{to our knowledge, these definitions have not been
%put down formally before.. include per state normalization.
%Should we prove this?. At every state, the totally probability of
%string starting from that state is one, $\Sigma, \epsilon, \phi (non consume), \rho$
%extend definition to deal with these}
% A WFST is said to be \textbf{conditional} if it represents a
% conditional probability distribution. We will now
% provide the formal conditions for a per-state normalized finite-state
% machine. To our knowledge these conditions have never appeared in the 
% literature. \ryan{should use a amsthm package?} A transducer $T$  with 
% weights in $(\mathbb{R}_{\geq 0},+,\times)$ is a locally
% normalized conditional WFST if it satisfies the following requirements:
 %This
 %is contrasted with an \textbf{improper conditional WFST} which
 %requires renormalization by a global partition function. Note that
 %this still requires each accepting path to have non-negative weight.
In section~\ref{sec:discussion}, we discuss the costs and benefits of 
\mbox{PFSTs} relative to other options.
% globally normalized weighted FSTs.

\section{The Contextual Edit PFST}

% CUT FOR SPACE
% \begin{figure}
%   \centering
%   \includegraphics[width=2.5in]{figs/local_config_diagram.pdf}
%   \caption{Context Configuration of a State}
%   \label{fig:local_config}
% \end{figure}

We now define a PFST topology that concisely captures the contextual edit process of section~\ref{sec:csed}.  We are given the alphabets $\Sigmax, \Sigmay$ and the context window sizes $N_1, N_2, N_3 \geq 0$.

% This is naturally viewed as
% a generalization of the stochastic edit distance transducer
% \cite{ristad1998learning} in that the probabilities of
% various operations (i.e., insertion, deletion, and substitution)
% are now \emph{dependent} on the local context. This is often exactly what we
% want when modeling string-to-string transductions. 

% A contextual edit machine $\mathcal{M}$ is parameterized by two finite
% alphabets $\Sigmax$ and $\Sigmay$, a left-input context window size
% $I \geq 0$, a right-input context window size $J \geq 0$  and a left-output context
% window size $K \geq 0$.  Each state represents
% a left input context, a right input context and a left output
% context.  since these are the unique contexts
% the machine needs to remember. Note that we cannot represent the
% right output context given the local normalization of the model.

For each possible context triple $C = (C_1, C_2, C_3)$ as defined in
section~\ref{sec:csed}, we construct an {\bf edit state} $q_C$ whose
outgoing arcs correspond to the possible edit operations in that
context.  

One might expect that the {\sc subst}$(t)$ edit operation that reads
$s=x_{i+1}$ and writes $t=y_{j+1}$ would correspond to an arc with
$s,t$ as its input and output labels.  However, we give a more
efficient design where in the course of reaching $q_C$, the PFST has
{\em already} read $s$ and indeed the entire right input context
$C_2=x_{i:(i+N_2)}$.  So our PFST's input and output actions are ``out
of sync'': its read head is $N_2$ characters ahead of its write head.
When the edit process of section~\ref{sec:csed} has read $x_{0:i}$ and
written $y_{0:j}$, our PFST implementation will actually have read
$x_{0:(i+N_2)}$ and written $y_{0:j}$.

This design eliminates the need for nondeterministic guessing (of the right context $x_{i:(i+N_2)}$) to determine the edit probability.  The PFST's state is fully determined by the characters that it has read and written so far\SkipForSpace{: it is deterministic when regarded as a joint acceptor of $x$ and $y$}.  This makes left-to-right composition in section~\ref{sec:complexity} efficient.%
\SkipForSpace{\footnote{\label{fn:nondet} A similar design is used to {\em deterministically} convert phones to triphones in speech recognition \cite{mohri2002weighted}. The alternative would be for the FST to have read only $x_{0:i}$ when entering $q_C$.  Then $C_2$ would be a nondeterministic guess of the right input context, with which future input arcs must be consistent.  While this design would keep the read and write heads in sync so that $x_{i+1}$ was read at the same time $y_{j+1}$ was written, it would result in a far more nondeterministic FST, which could be in any of $O(|\Sigmay|^{N_2})$ different states (corresponding to different guesses of the string $C_2$) given the characters it had read and written so far.  This nondeterminism would slow down the compositions in section~\ref{sec:complexity} that compute $\ptheta(y\mid x)$ or the most likely edit sequence given $x$.  (Also, this alternative design would not satisfy section~\ref{sec:pfst}'s formal PFST definition, although it would define the same distribution $\ptheta(y\mid x)$.)}}

\begin{figure} \centering
  \includegraphics[scale=.4]{figs/contextual_edit_machine.pdf}
  \caption{\label{fig:edit_distance}A fragment of a PFST with $N_1=1, N_2=2, N_3=1$.  
    Edit states are shaded.  A state $q_C$ is drawn with left and right input contexts $C_1,C_2$ in the left and right upper quadrants, and left output context $C_3$ in the left lower quadrant.  Each arc is labeled with input:output / probability. 
%
%    \protect\footnotemark
  }
\end{figure}
% \footnotetext{Certain arcs have been omitted for brevity.}

A fragment of our construction is illustrated in Figure~\ref{fig:edit_distance}. 
An edit state $q_C$ has the following outgoing {\bf edit arcs}, each of which corresponds to an edit operation that replaces some $s \in \Sigma_x \cup \{\epsilon\}$ with some $t \in \Sigma_y \cup \{\epsilon\}$:
\begin{itemize}[noitemsep]
\item A single arc with probability $p(\textsc{delete} \mid C)$ (here $s=(C_2)_1, t=\epsilon$)
\item For each $t \in \Sigmay$, an arc with probability $p(\textsc{insert}(t) \mid C)$ (here $s=\epsilon$)
\item For each $t \in \Sigmay$, an arc with probability $p(\textsc{subst}(t) \mid C)$ (here $s=(C_2)_1$)
\end{itemize}
Each edit arc is labeled with input $\epsilon$ (because $s$ has {\em already} been read) and output $t$.  The arc leads from $q_C$ to $q_{C'}$, a state that moves $s$ and $t$ into the left contexts: 
$C'_1=\text{suffix}(C_1s,N_1)$, 
$C'_2=\text{suffix}(C_2,N_2-|s|)$,
$C'_3=\text{suffix}(C_3t,N_3)$.

Section~\ref{sec:csed} mentions that the end of $x$ requires special handling.  An edit state $q_C$ whose $C_2=\eos^{N_2}$ only has outgoing $\textsc{insert}(t)$ arcs, and has a halt probability of $p(\textsc{halt} \mid C)$.  The halt probability at all other states is 0.
\SkipForSpace{\footnote{If $N_2=0$, we must tweak our construction to distinguish the end-of-string case: an edit state's $C_2$ cannot be empty, but consists of a nondeterministically guessed bit that indicates whether the next input character will be \eos (see footnotes~\ref{fn:eosbit} and~\ref{fn:nondet}).  
% CUT FOR SPACE
% We must nondeterministically guess this bit (see footnote~\ref{fn:nondet}) unless $N_1=0$ as well.  
We omit the details for space reasons, as $N_2=0$ is not very useful (it cannot even distinguish {\sc copy} from {\sc subst}).  However, our code does handle this case.}}

% OLD, INCORRECT (SEE JASON'S EMAIL OF 1/6/2014)
% \footnote{Our construction must be slightly tweaked if $N_2=0$.  We handle this as if $N_2=1$, except that $C_2$ does not store the exact identity of the lookahead character, but only a bit indicating whether that character is {\sc eos}---enough to tell whether $q_C$ matches this special case.  See footnote~\ref{fn:eosbit}.}

We must also build some {\bf non-edit states} of the form $q_C$ where $|C_2| < N_2$.  Such a state does not have the full $N_2$ characters of lookahead that are needed to determine the conditional probability of an edit.  Its outgoing arcs deterministically read a new character into the right input context.  For each $s \in \Sigmax$, we have an arc of probability 1 from $q_C$ to $q_{C'}$ where $C' = (C_1, C_2s, C_3)$, labeled with input $s$ and output $\epsilon$.  Following such arcs from $q_C$ will reach an edit state after $N_2-|C_2|$ steps.  

The initial state $q_I$ with $I = (\bos^{N_1},\epsilon,\bos^{N_3})$ is a non-edit state.  Other non-edit states are constructed only when they are reachable from another state.  In particular, a {\sc delete} or {\sc subst} arc always transitions to a non-edit state, since it consumes one of the lookahead characters.

% OLD
% Figure~\ref{fig:local_config} shows a schematic for how we draw the
% states.  This yields a set of states $\{ (c_i,c_j,c_k) \}$ where
% $c_i$ is in the set of all substrings of $\Sigmax^*$ of length at
% \emph{most} $I$, $c_j$ is in the set of all substrings of $\Sigmax^*$
% of length at \emph{most} $J$ and $c_k$ is in the set of all substrings
% of $\Sigmay^*$ of length at \emph{most} $K$. We additionally have to
% augment the alphabet to account for the end of sequence character \bos,
% which is not part of $\Sigmax$ or $\Sigmay$ but is part of $c_i$ and
% $c_j$. Similarly, \eos is not in $\Sigmax$ but is
% part of $c_j$.
% 
% For convenience, we will partition the states into two sets: the set of
% \emph{edit} states $\mathcal{E}$ and the set of \emph{non-edit} states
% $\mathcal{N}$. Roughly speaking, \emph{edit} states are those in which
% the machine has full context i.e., $|c_i| = I$, $|c_j| = J$, and
% $|c_k| = K$. We call such states \emph{edit} states because they are
% exactly the states where we allow the machine to make edits. All
% states where the full context is not reached are termed
% \emph{non-edit} states. This distinction is important for when we
% discuss the arcs in the machine.
% 
% There are three types of
% arcs that emanate from an \emph{edit} state: \textbf{insertion},
% \textbf{substitution} and \textbf{deletion}.
% 
% The arcs emanating from \emph{non-edit} states serve to read in
% characters on the input string to provide the proper context for the
% transductions. These arcs all have the form $c:\epsilon$ where $c \in
% \Sigmax$. The machine is deterministic at these states \ryan{can one
% say deterministic at a state?} in that given the next input character
% $c$, there is only one choice it can make. The machine reads in
% characters on the input string until it enters an \emph{edit} state,
% in which all of the context has been saturated. There are three types of
% arcs that emanate from an \emph{edit} state: \textbf{insertion},
% \textbf{substitution} and \textbf{deletion}.
% %\adi{It was not clear from the text the distinction between insertion
% %and substitution. That substitution is $\epsilon:c$ is counter-intuitive 
% %with respect to the regular edit distance machine and should be explained 
% %in more detail}
% 
% These correspond to the arcs in an edit distance machine. Deletion
% arcs have the form $\epsilon:\epsilon$ and connect the state to a
% state with with identical context with the exception that the first
% character of $c_j$ is removed. Deletion acs therefore always lead to a
% \emph{non-edit} since we are losing input context. This can be thought
% of popping a letter off the input string buffer without any modification
% to the output string. Substitution arcs have the form
% $\epsilon:c$ where $c$ is the new character substituted for the last
% character of $c_j$. Substitution arcs also always lead to a
% \emph{non-edit} state. In comparison to deletion, however, when we pop
% a letter off the input string buffer we simultaneously add a letter to
% the output string. A special case of substitution is \textbf{copy}, in
% which the substituted character matches the character already read
% in. Finally, \textbf{insertion} arcs have the form $\epsilon:c'$ where
% $c'$ is an additional character attached to the output string. This
% arc moves the machine into another \emph{edit} state, in which $c_k$
% reflects the insertion. This is different from substitution because we
% do \emph{not} pop a letter off the input string buffer. Figure
% ~\ref{fig:edit_distance} shows a contextual edit distance machine with
% arcs annotated with their function.
% 
% There are a few complications when handling the beginning and end of
% strings. One way to solve this is to augment $\Sigmax$ and $\Sigmay$,
% as mentioned above, with \bos and \eos characters. We
% can never read in, insert or substitute \bos character as they 
% only server as markers of word initial context in the first few states.
% On the other hand, \eos is a bit more tricky. If we read in 
% \eos at any time we have committed ourselves to ending
% the string the end. When \eos is never on the buffer our
% only available actions are insert and copy, which ends ends the 
% transduction.

%\jason{For convenience, include the formula for mapping each $C$ to an integer?}


% \section{Variable-Length Context} 

% THIS NEEDS A BIT OF WORK (LAST MINUTE ADD) \\

% Notice that $p(y \mid \epsilon)$ gives a $N_3$\th-order Markov model over $y$.  (The input left and right contexts are fixed in this case.)  So our transducer construction subsumes the usual finite-state construction of $n$-gram character language models, by taking $x=\epsilon$ and $N_3=n-1$.

% However, it is common to use backoff in $n$-gram language models \cite{allauzen2003generalized} ...

% The number of states of a contextual edit distance machine is exponential
% in the amount of context we choose to include. Much of the context,
% however, is unneeded and can in fact lead to overfitting. \jason{only leads to overfitting if you don't have backoff features etc.}  For this reason 
% one can construct a machine from a list of contexts deemed important. 
% This is analogous to backoff in a language model. The algorithm for
% this construction is provided in the appendix. Future work will focus
% on learning which of these contexts are most important to keep
% in the machine using advances in structured sparsity. 

% %\section{Model} 
% %\cite{mccallum2000maximum, lafferty2001conditional}.  While such a
% %locally normalized can potentially suffer from \emph{label bias}, it
% %can be trained more efficient as it is not necessary to sum over all
% %strings to compute the partition function. Globally normalized
% %log-linear parameterized WFST are particularly difficult given the
% %necessity to optimize a possibly divergent objective function
% %\cite{dreyer2008latent}. Locally normalized models obviate the need
% %for checks for divergence.

\section{Computational Complexity}\label{sec:complexity}

We summarize some useful facts without proof. For fixed alphabets $\Sigmax$ and $\Sigmay$, our final PFST, $T$, has $O(|\Sigmax|^{N_1+N_2}|\Sigmay|^{N_3})$ states and $O(|\Sigmax|^{N_1+N_2}|\Sigmay|^{N_3+1})$ arcs.  Composing this $T$ 
with deterministic FSAs takes time linear in the size of the {\em result}, \jason{before trimming: i.e., linear in the number of accessible states in the result} using a left-to-right, on-the-fly implementation of the composition operator $\circ$.

Given strings $x$ and $y$, we can compute $\ptheta(y\mid x)$ as the total probability of all paths in $x \circ T \circ y$.  This acyclic weighted FST has $O(|x|\cdot|y|)$ states and arcs.  It takes only $O(|x|\cdot|y|)$ time to construct it and sum up its paths by dynamic programming, just as in other edit distance algorithms.

Given only $x$, taking the output language of $x \circ T$ yields the full distribution $\ptheta(y \mid x)$ as a cyclic PFSA with $O(|x|\cdot \Sigmay^{N_3})$ states and $O(|x|\cdot \Sigmay^{N_3+1})$ arcs.  Finding its most probable path (i.e., most probable aligned $y$) takes time $O(|\text{arcs}| \log |\text{states}|)$, while computing every arc's expected number of traversals under $p(y \mid x)$ takes time $O(|\text{arcs}| \cdot |\text{states}|)$.%
\footnote{Speedups: In both runtimes, a factor of $|x|$ can be eliminated from $|\text{states}|$ by first decomposing $x \circ T$ into its $O(|x|)$ strongly connected components.  And the $|\text{states}|$ factor in the second runtime is unnecessary in practice, as just the first few iterations of conjugate gradient are enough to achieve good approximate convergence when solving the sparse linear system that defines the forward probabilities in the cyclic PFSA.}

$\ptheta(y\mid x)$ may be used as a noisy channel model.  Given a language model $p(x)$ represented as a PFSA $X$, $X \circ T$ gives $p(x,y)$ for all $x,y$.  In the case of an $n$-gram language model with $n \leq N_1+N_2$, this composition is efficient: it merely reweights the arcs of $T$.  We use Bayes' Theorem to reconstruct $x$ from observed $y$: $X \circ T \circ y$ gives $p(x,y)$ (proportional to $p(x \mid y)$) for each $x$.  This weighted FSA has \linebreak $O(\Sigmax^{N_1+N_2} \cdot |y|)$ states and arcs.

% OLD VERSION
% Reconstructing $x$ from an observed $y$ by Bayes' Theorem requires computing the input language of $T \circ y$.  This cyclic weighted FSA gives $\ptheta(y \mid x)$ for each $x$, and has $O(|y|\cdot \Sigmax^{N_1+N_2})$ states and arcs.  It can be intersected with a PFSA that represents a language model $p(x)$, to get a weighted FSA that gives $p(x,y)$ (proportional to $p(x \mid y)$) for each $x$.  

\section{Parameterization and Training}\label{sec:parameterization}\label{sec:training}

While the parameters $\theta$ could be trained via various objective functions, it is particularly efficient to compute the gradient of {\em conditional log-likelihood}, 
$\sum_k \log \ptheta(y_k \mid x_k)$, given a sample of pairs $(x_k,y_k)$.  This is a non-convex objective function because of the latent $x$-to-$y$ alignments: we do not observe {\em which} path transduced $x_k$ to $y_k$.  Recall from section~\ref{sec:complexity} that these possible paths are represented by the small weighted FSA $x_k \circ T \circ y_k$.

Now, a path's probability is defined by multiplying the contextual probabilities of edit operations $e$.  As suggested by
\newcite{berg2010painless}, we model these steps using a conditional log-linear model,
$\ptheta(e \mid C) \defeq \frac{1}{Z_C} \exp \left( \theta \cdot \vec{f}(C,e) \right)$.

To increase $\log \ptheta(y_k \mid x_k)$, we must raise the probability of the edits $e$ that were used to transduce $x_k$ to $y_k$, relative to competing edits from the same contexts $C$.  This means raising $\theta \cdot f(C,e)$ and/or lowering $Z_C$.  Thus, $\log \ptheta(y_k \mid x_k)$ depends only on the probabilities of edit arcs in $T$ that appear in $x_k \circ T \circ y_k$, and the competing edit arcs from the same edit states $q_C$.  

% OLD
% Using the contextual edit distance described above, we define a
% conditional distribution $\ptheta(x|y)$, which represents the
% stochastic edit distance between two strings. We use a log-linear
% parameterization similar to \newcite{dreyer2008latent}. Ours differs,
% however, in that the model is per-state normalized. In short, we
% have chosen to adopt the MEMM strategy instead of a CRF strategy.

% To train the model we use EM to perform maximum likelihood estimation \cite{dempster1977maximum}. Specifically, we seek to train
% $\ptheta$ from a set of ordered pairs of strings
% $(x_i,y_i)$ where $x_i$ is the surface form and $y_i$ is the
% underlying form. Formally, we aim to maximize the following objective
% \begin{equation}
% F(\theta) = \sum_x \sum_y \ptheta(x|y).
% \end{equation}
% Let $T$ be the WFST that represents the conditional probability
% distribution $\ptheta(x|y)$. We will use $\Pi_{x,y}$ to set of all
% paths in $T$ accepting $(x,y)$ (i.e., the set of all paths in $x \circ
% T \circ y$).  Each $\pi_i \in \Pi_{x,y}$ is therefore a sequence of
% arcs. Finally, let $q_{x,y}(\pi_i)$ be the distribution over all paths
% in $\Pi_{x,y}$.  In accordance with the log-linear parameterization,
% the transition function is defined as a conditional log-linear
% model. Formally, given an input symbol $s$, we take arc $i$ with
% probability $t_\theta(i|s)$ where
% \begin{equation} t_\theta(i|s) = \frac{1}{Z_i(s)}\exp(\theta^T f_i),
% \end{equation} where $f_i$ is feature vector for the $i$th arc and
% $Z_i(s)$ is the normalizing constant. First we define $a(i)$ to be the
% set of all arcs emanating from the same state as $i$ with the same
% input symbol \footnote{Arcs with $\epsilon$ as an input label are
% always part of this set.}
% \begin{equation}
% Z_i(s) = \sum_{j \in a(i)} \exp(\theta^T f_j).
% \end{equation}
% 
% This allows us to define the distribution as follows
% \begin{equation}
% \ptheta(x|y) = \sum_{\pi_i \in \Pi_{x,y}} \prod_{j \in \pi_i} t_\theta(j|s).
% \end{equation}

The gradient $\nabla_{\theta} \log \ptheta(y_k\mid x_k)$
% (full derivation shown in the appendix) 
takes the form
\begin{equation*}
\sum_{C,e} \text{c}(C,e) \left[ \vec{f}(C,e) - \sum_{e'} \ptheta(e'\mid C) \vec{f}(C,e') \right]
\end{equation*}
where $\text{c}(C,e)$ is the {\em expected} number of times that $e$ was chosen in context $C$ given $(x_k,y_k)$.  (That can be found by the forward-backward algorithm on $x_k \circ T \circ y_k$.)  So the gradient adds up the differences between observed and expected feature vectors at contexts $C$, where contexts are weighted by how many times they were likely encountered.

% \footnote{The observed counts are actually in expectation given the uncertainty
% over which path in the machine was actually traversed to generate the given string pair.}

In practice, it is efficient to hold the counts $\text{c}(C,e)$ constant over several gradient steps, since this amortizes the work of computing them.  This can be viewed as a generalized EM algorithm that imputes the hidden paths (giving $\text{c}$) at the ``E'' step 
% (obtaining $\text{c}(C,e)$) 
and improves their probability at the ``M'' step.

Algorithm~\ref{alg:training} provides the training pseudocode.  

\begin{algorithm}\begin{algorithmic}[1]
    \caption{Training a PFST $T_\theta$ by EM.}\label{alg:training}
    \small
    \While{not converged}
      \State reset all counts to 0 \Comment{begin the ``E step''}
      % $obsArcCounts = \vec{0}$ \\
      % $obsStateCounts = \vec{0}$ \\
      % $expArcCounts = \vec{0}$ \\
      \For{$k \gets 1\textbf{ to }K$}  \Comment{loop over training data}
        \State $M = x_k \circ T_\theta \circ y_k$ \Comment{small acyclic WFST} 
        \State $\vec{\alpha}$ = \textsc{Forward-Algorithm}($M$) 
        \State $\vec{\beta}$ = \textsc{Backward-Algorithm}($M$)
        \For{arc $A \in M$, from state $q \rightarrow q'$} 
          \If{$A$ was derived from an arc in $T_\theta$ \hspace*{\fill} \linebreak \hspace*{\fill} representing edit $e$, from edit state $q_C$,}
            \State $\text{c}(C,e)$ += $\alpha_q \cdot \text{prob}(A) \cdot \beta_{q'} / \beta_{q_{\textrm{I}}}$
          \EndIf
        \EndFor
      \EndFor
      \State $\theta \gets$ \textsc{l-bfgs}($\theta$, \textsc{Eval}, max\_iters=5) \Comment{the ``M step''}
    \EndWhile
    \Function{Eval}{$\theta$}\Comment{objective function \& its gradient}
         \State $F \gets 0$; $\nabla F \gets 0$ 
% TOO CONFUSING TO INCLUDE REGULARIZER
%         \State $F \gets -\lambda \cdot ||\theta||_2^2$; $\nabla F \gets -2 \lambda \cdot \theta$ \Comment{regularizer}
        \For{context $C$ such that $(\exists e) \text{c}(C,e) > 0$} % OLD: such that $text{c}(C) \defeq \sum_e \text{c}(C,e) > 0$
           \State $\textit{count} \gets 0$; $\textit{expected} \gets 0$; $Z_C \gets 0$
           \For{possible edits $e$ in context $C$}
             \State $F$ += $\text{c}(C,e) \cdot (\theta \cdot \vec{f}(C,e))$
             \State $\nabla F$ += $\text{c}(C,e) \cdot \vec{f}(C,e)$
             \State $\textit{count}$ += $\text{c}(C,e)$
             \State $\textit{expected}$ += $\exp ( \theta \cdot \vec{f}(C,e) ) \cdot \vec{f}(C,e)$
             \State $Z_C$ += $\exp ( \theta \cdot \vec{f}(C,e) )$
           \EndFor
           \State $F$ -= $\textit{count} \cdot \log Z_C$; $\nabla F$ -= $\textit{count} \cdot \textit{expected} / Z_C$
        \EndFor
        \State \textbf{return} $(F, \nabla F)$
    \EndFunction
\end{algorithmic}\end{algorithm}

\section{PFSTs versus WFSTs}\label{sec:discussion}
 
Our PFST model of $p(y\mid x)$ enforces a normalized probability distribution at each state.
% SHORTENED
% over each state's outgoing {\em arcs}, 
Dropping this requirement gives a {\bf weighted FST (WFST)}, whose path weights $w(x,y)$ can be globally normalized (divided by a constant $Z_x$) to obtain probabilities $p(y\mid x)$.  WFST models of contextual edits were studied by \newcite{dreyer2008latent}.  

PFSTs and WFSTs are respectively related to MEMMs \cite{mccallum00} and CRFs \cite{lafferty2001conditional}.  
They gain added power from hidden states and $\epsilon$ transitions
% , e.g. allowing $|y|\neq |x|$.
(although to permit a {\em finite}-state encoding, they condition on $x$ in a more restricted way than MEMMs and CRFs).
%  (e.g., our edit arcs depend on only a finite window of $N_1+N_2$ characters).  

WFSTs are likely to beat PFSTs as linguistic models,\footnote{WFSTs can also use a simpler topology \cite{dreyer2008latent} while retaining determinism, since edits can be scored ``in retrospect'' after they have passed into the left context.}  just as CRFs beat MEMMs \cite{klein-manning-2002-condit}.  A WFST's advantage is that the probability of an edit can be {\em indirectly} affected by the weights of other edits at a distance.  Also, one could construct WFSTs where an edit's weight {\em directly} considers local right output context $C_4$.

So why are we interested in PFSTs?  Because they do not require computing a separate normalizing contant $Z_x$ for every $x$.  This makes it computationally tractable to use them in settings where $x$ is uncertain because it is unobserved, partially observed (e.g., lacks syllable boundaries), or noisily observed.  E.g., at the end of section~\ref{sec:complexity}, $X$ represented an uncertain $x$.  So unlike WFSTs, PFSTs are usable as the conditional distributions in noisy channel models, channel cascades, and Bayesian networks.  In future we plan to measure their modeling disadvantage and 
attempt to mitigate it.

% JASON'S IDEAS ON FUTURE WORK 
% 1. Structure compilation from WFST to PFST.  This can be done by sampling or by taking an expectation over all possible futures via a cyclic backward algorithm.  It is related to a "lookahead" paper that I sent Ryan.
% 2. Training something like expected edit distance rather than conditional likelihood, for reasons that I sent Ryan.  This encourages training incorrect states to get back on track, like DAgger (at the cost of having to visit those states during training!)
% 3. Stacking two PFSTs as a probabilistic bimachine.

PFSTs are also more efficient to train under conditional likelihood.  It is faster to compute the gradient (and fewer steps seem to be required in practice), since we only have to raise the probabilities of {\em arcs} in $x_k \circ T \circ y_k$ relative to competing arcs in $x_k \circ T$.  We visit at most $|x_k| \cdot |y_k| \cdot |\Sigmay|$ arcs.  By contrast, training a WFST must raise the probability of the {\em paths} in $x_k \circ T \circ y_k$ relative to the {\em infinitely many} competing paths in $x_k \circ T$.  This requires summing around cycles in $x_k \circ T$, and requires visiting all of its $|x_k| \cdot |\Sigmay|^{N_3+1}$ arcs.
% CUT FOR SPACE; INSTEAD MENTIONED INFINITUDE ABOVE
% which can transduce $x_k$ to any $y \in \Sigmay^*$ of unbounded length

\jason{note that WFSTs are more closely related to the notion of edit distance, where each transition has a weight associated with the edit in that context (and compare this to Markus), but then to specialize to a stochastic process, forcing the right output context to $\epsilon$}

\jason{discuss HMMs?}

\section{Experiments}

\begin{figure*}[t!]
   \centering
   \includegraphics[width=3.1in]{figs/mean_ll_final.pdf}
   \includegraphics[width=3.1in]{figs/mean_expected_edit_final.pdf}
  \caption{(a) Mean $\log p(y \mid x)$ for held-out test examples.  (b) Mean expected edit distance (similarly).}
  \label{fig:experimental_results}
 \vspace{-2pt}
 \end{figure*}

% \begin{figure}
%   \centering
%   \includegraphics[width=3.1in]{figs/likelihood_final.pdf}
%  % \includegraphics[width=3.1in]{figs/expected_edit_final.pdf}
%  \caption{Mean $\log p(y \mid x)$ for held-out test examples. \\
%  Ryan Will merge both plots together}
%  \label{fig:experimental_results}
% %\vspace{-2pt}
% \end{figure}

To demonstrate the utility of contextual edit transducers, we
examine spelling errors in social media data. Models
of spelling errors are useful in a variety of settings including
spelling correction itself and phylogenetic models of string variation
\cite{mays1991context,church1991probability,kukich1992techniques,andrews2014robust}. 

To eliminate experimental confounds, we use no dictionary or language model as one would in practice, but directly evaluate our ability to model $p(\text{correct} \mid \text{misspelled})$.  
% OLD VERSION
% we use no dictionary or language model, but just directly model the transduction of the misspelled variant to the corrected form, e.g., $p(the \mid teh)$.  
% CUT
% This task is a subset of the text normalization problem \cite{sproat2001normalization}.
Consider $(x_k,y_k)=(\textit{feeel, feel})$.  Our model defines $p(y \mid x_k)$ for all $y$. Our training objective (section~\ref{sec:training}) tries to make this large for $y=y_k$.  
%\jason{is this a real example where we beat the baseline?} \ryan{yes it is}
A {\em contextual} edit model 
learns here that $\textit{e} \mapsto \epsilon$ is more likely in the context of \textit{ee}.

% PROBABLY THIS COMMENT IS OBJECTING TO OLDER EXAMPLE SUCH AS (teh,the)
%\jason{This may be a bad example because our current model can't capture the real generalization, namely that frequent or short words may swap letters that are typed by different hands.  But we can make a point of the fact that we can still fit the data better with our model thanks to having more parameters.}

We report on test data how much probability mass lands on the true $y_k$.  We also report
how much mass lands ``near'' $y_k$, by measuring the expected edit distance of the predicted $y$ to the truth.  Expected edit distance is defined as $\sum_y \ptheta(y\mid x_k) d(y,y_k)$ where  $d(y,y_k)$ is the Levenshtein distance between two strings.  It can be computed using standard finite-state algorithms \cite{mohri2003edit}.  

\vspace{-3pt}
\subsection{Data}
\vspace{-1pt}

We use an annotated corpus \cite{typocorpus} of 50000 misspelled words $x$ from tweets along with their corrections $y$.
All examples have $d(x,y)=1$ though we do not exploit this fact.
% The dataset contains about 50000 misspelled words and their correctly spelled counterparts. 
We randomly selected 6000 training pairs and 100 test pairs. We regularized the objective by adding $\lambda \cdot ||\theta||_2^2$, where for each training condition, we chose $\lambda$ by coarse grid search to maximize the conditional likelihood of 100 additional development pairs.

\vspace{-3pt}
\subsection{Context Windows and Edit Features}
\vspace{-1pt}

We considered four different settings for the context window sizes $(N_1,N_2,N_3)$: (0,1,0)=stochastic edit distance, (1,1,0), (0,2,0), and (1,1,1).  
% Note that (0,1,0) is ordinary stochastic edit distance.  

Our log-linear edit model (section~\ref{sec:parameterization}) includes a dedicated indicator feature for each contextual edit $(C,e)$, allowing us to fit {\em any} conditional distribution $p(e \mid C)$.  In our ``backoff'' setting, each $(C,e)$ also has 13 binary backoff features that it shares with other $(C',e')$.  So we have a total of 14 feature templates, which generate over a million features in our largest model.  The shared features let us learn that certain {\em properties} of a contextual edit tend to raise or lower its probability (and the regularizer encourages such generalization).  

Each contextual edit $(C,e)$ can be characterized as a 5-tuple $(s,t,C_1,C_2',C_3)$: it replaces $s \in \Sigma_x \cup \{\epsilon\}$ with $t \in \Sigma_y \cup \{\epsilon\}$ when $s$ falls between $C_1$ and $C_2'$ (so $C_2 = sC_2'$) and $t$ is preceded by $C_3$.  Then each of the 14 features of $(C,e)$ indicates that a particular subset of this 5-tuple has a particular value.  The subset always includes $s$, $t$, or both.  It never includes $C_1$ or $C_2'$ without $s$, and never includes $C_3$ without $t$.

\jason{We should really do character-by-character backoff.  The above backoff scheme has a lot of redundant features when some of the tuple elements are always empty anyway!}

% We consider 3 sets of features: upper language features, lower language features and transduction features. The upper language features will always include the current editing character s, along with possible context $C_1$ and/or $C_2$; the lower language features will always include the current output character t, along with possible context $C_3$; the transduction features will have transduction s-t, along with possilbe upper and lower context $C_1$, $C_2$ and $C_3$. This strategy results in 14 feature templates with each feature a binary indicator representing whether the feature fires on a given arc. \SkipForSpace{We use sparse representation for the feature vector. }
% %each arc emanating from an edit state had a feature vector associated with. 
% %For a context $(C_1,C_2,C_3)$ there is a binary indicator features that fire on every  
% %$(C_{1_{0:i}},C_{2_{0:j}},C_{3_{i:k}})$ for all $i \leq N_1$, $j \leq N_2$ and $k \leq N_3$.
% %\footnote{A strict subset of these features was used in the experimental results report. We are rerunning
% %the experiments with the full feature set and expect to see improved results}  
% In effect, this parameterization encodes a form of backoff\SkipForSpace{, which in theory should allow us to generalize better to contexts that were not observed in training}. 

\subsection{Results}

Figures~\ref{fig:experimental_results}a 
% shows learning curves for the log-probability of test data,
and~\ref{fig:experimental_results}b 
% for the expected edit distance per test example. 
show the learning curves.
\jason{sort the legend to match order of curves, and match colors of backoff/non-backoff models} We see that both metrics improve with more training data; with more context; and with backoff.  With backoff, all of the contextual edit models substantially beat ordinary stochastic edit distance, and their advantage grows with training size.

\jason{The best models achieve an average log-probability around -0.4, which means that they assign 67\% probability to the correct answer (without a dictionary),
  a bit better than -0.53 or so for weighted Levenshtein (= 59\%).  We should report similar numbers for expected edit distance.  That improvement seems more dramatic, suggesting that weighted Levenshtein is assigning mass to more things that are badly wrong?  Give some analysis in final version.}

% \ryan{table about features and topologies}
% \begin{figure}
%   \centering
%   \begin{tabular}{|c|c|}
%     \hline 
%     \# & Description \\ \hline
%     1 & (1,1,0) topology with features (backoff) \\ \hline
%     2 & Weighted Edit Distance with features \\ \hline
%     3 & (1,1,0) topology without features \\ \hline
%     4 & Weighted edit distance without features  \\ \hline
%     \end{tabular}
%   \caption{Description of Experiments}
%   \label{fig:experiment_description}
% \end{figure}

% \begin{figure}
%   \centering
%   \includegraphics[width=3.0in]{figs/likelihood.pdf}
%   \caption{Log-Likelihood of the Test Data}
%   \label{fig:log_loss}
% \end{figure}

% \begin{figure}
%   \centering
%   \includegraphics[width=3.0in]{figs/expected_edit_distance.pdf}
%   \caption{Expected Edit Distance of Test Data}
%   \label{fig:expected_edit_distance}
% \end{figure}


\section{Conclusion}

We have presented a trainable, featurizable model of {\em contextual} edit distance\SkipForSpace{and 
tested it in an example domain}. Our main contribution is an efficient encoding of such a model as a tight PFST---that is, a WFST that is guaranteed to directly define conditional string probabilities without need for further normalization.  We are releasing OpenFST-compatible code that can train both PFSTs and WFSTs \cite{cotterell2014brezel}.  We formally defined PFSTs, described their speed advantage at training time, and noted that they are crucial in settings where the input string is unknown.  In future, we plan to deploy our PFSTs in such settings. %to extend their modeling capacity.\SkipForSpace{ compensate for their limiations as models (e.g., the label bias problem).  We also plan to improve our PFSTs to consider variable-length contexts and phonological features.}
 
% The topology given in this paper is designed to facilitate
% the training of a per-state normalized model. One could, 
% however, easily train a globally normalized model.
% \SkipForSpace{While per-state normalized models can potentially
% suffer from label bias, they are much faster to train
% as there is no global partition function to compute.} 
% Empirically benchmarking both speed and accuracy for
% per-state and normally contextual edit transducers would
% be useful. \SkipForSpace{Additionally, treating expected edit distance
% as the loss function and directly minimizing it may also
% lead to better results, especially in the per-state normalized
% case \ryan{see Jason's email} \jason{but note that this makes it slow!}.} Another line of work we are
% actively pursing is the application of such models to the 
% full text normalization problem. The log-linear parameterization
% allows for user encoded features, which could capture edits
% that are common in social media. We hope to experiment with 
% this as an error model in a larger finite-state text normalization 
% system.

% \subsection{EM}
% \subsubsection{E-step} Computation of the expected feature counts is
% accomplished with standard finite state algorithm.  If the weights of
% the transducers are taken from the expectation semiring, the E-step
% can be performed by shortest path algorithms \cite
% {eisner2002parameter}. We briefly review the expectation semiring as
% it is key for computing expected counts in a log-linear
% parameterization.  Weights in the expectation semiring take the form
% of a pair $(p,v)$.  In our case, $p$ is a real number representing a
% probability and $v$ is a vector feature counts. The weight on every
% arc in the machine includes which features fire upong the crossing of
% that arc. We define
% \begin{equation} \label{eq:expectation_plus} (p_1,v_1) \oplus
% (p_2,v_2) = (p_1 + p_2,v_1 + v_2)
% \end{equation}
% \begin{equation} \label{eq:expectation_times} (p_1,v_1) \otimes
% (p_2,v_2) = (p_1p_2,p_2v_1 + p_1v_2)
% \end{equation} This terms the computation of expected counts into a
% shortest-distance problem \cite{morhi2002semiring}.

% \subsubsection{M-step} We can think of each state in the machine as
% having its own conditional log-linear model, from which it samples to
% determine its next state. The goal is the M-step is then to optimize
% the weights using a gradient-based procedure. A similar technique has
% been applied to HMMs with locally normalized log-linear emission
% distributions \cite{bert2010painless}. To prevent overfitting we use
% $\ell_2$ regularization.  

% \begin{figure} \label{fig:em_algorithm} \centering
%   \begin{algorithmic} \Repeat \\ Run Forward-Backward Algorithm
% Compute expected feature counts \textbf{c} \Repeat \\ Compute the
% expected expected counts \textbf{e} \\ $\nabla\ell(\theta)$=
% \textbf{c} - \textbf{e} \\ $\theta \leftarrow \eta \nabla
% \ell(\theta)$ \Until{convergence}

%     \Until{convergence}
% \end{algorithmic}
% \caption{EM}
% \end{figure} \ryan{regularization?, feature-enhanced EM?}



% \section{Latent Attributes} Finite-state machines allow for simple
% encoding of latent attributes as features. This idea was adduced in
% \cite{dreyer2008latent}. In this section we discuss a relatively
% simple technique to acheive such models through the use of composition
% of different finite-state machines. In effect, this is a simple
% technique for simulating a $n$-tape finite-state machine with a simple
% FST.  This has a natural application in computational phonology when
% one seeks to express autosegmental representations in a computational
% model \cite{goldsmith1976autosegmental}. One such encoding involves
% modeling each individual tier as a separate tape in a FSM
% \cite{koskenniemi1984general}.

% A useful example of this would an autosegmental syllable tier. In
% phonology, syllables are not observed and there is often empirical
% disagreement where syllables lie. For example NETtalk and and
% Merriam-Webster's syllabification only agree in 54\% of the instances
% \cite{bartlett2008automatic}.  While syllables are nevertheless
% important, it makes the most sense to treat them as a latent variable.
% In a log-linear parameterization, we can define features that fire
% this latent structure.  Now consider the syllable case. $\Sigma_1 =
% {O,R}$ where $O$ and $R$ stand for onset and rime respectively and
% $\Sigma_2$ is the alphabet of the sequence we wish to add a latent
% alignment to. We create a toy example where $\Sigma_2 = {a,b}$. This
% yields the following machine

% \begin{figure}[H] \centering
%   \includegraphics[width=2.5in]{figs/figure1.png} % better name!
%   \caption{Syllable Latent Variables}
%   \label{fig:syllables}
% \end{figure}

% If we are modeling a transduction from $\Sigma_3 \mapsto \Sigma_2$
% with some transducer $T$ for some $\Sigma_3$, say a grapheme to
% phoneme conversion, we can simply add our label structure machine $S$
% into the composition and marginalize out the hidden latent (this can
% be done efficiently with projection). 

% \section{Efficient Training}
% Want to discuss the direct computational of $x \circ T \circ y$
% We now discusison efficient training of locally normalized WFST.  The
% major obstacle to fast training is the composition of relatively
% larger machines. The more context and the more latent variables we
% seekt to include in our model, the more arcs the the machine will have
% and thus the soutput the composition. We can avoid ever building the
% machine through a few tricks. Let $T$ be a stochastic edit transducer
% with latent attributes. Thus each arc has features that fire on the 
% context of the state, the input and output symbol on the arc itself
% and the set of latent attributes. The  


% and $L$ be a latent variable FST like above. We seek to train the
% weights in $T' = T \circ L $ (better notation).  For training
% data $\mathcal{D} = \{(x_i,y_i)\}$, this involves computing the
% composition of $x_i \circ T \circ y_i \circ L$. However, we 
% can directly contruct 


% Just as in an MEMM, a general locally normalized WFST can be viewed as
% a set of $|\mathcal{Q}|$ log-linear models where $\mathcal{Q}$ is the
% set of states.  We therefore



% In terms of effiency, it can be more effient to separate latent
% structure into a separate machine. Consider the case of a contextual
% edit distance machine that remembers a large number of context
% (specify number).  This machine will be very large, millions of
% arcs. For a training example $x_i$, we can exploit the associativty of
% the composition operator since although $T \circ S$ will lead to a a
% bigger machine, building the whole machine is unnecessary. We can
% simple compute $(x_i \circ T) \circ S$.

% \section{Loss Aware Training}


% \section{Alignment}
% Discuss alignment alphabet $A_{xy}$ and how we marginalize over
% all alignments. 

% \section{Phonological Attributes}
% We make a distinction between \emph{features}
% and \emph{attributes}. 

% % \section{Feature Induction} To address the prohibitive size of
% % contextual edit machines we attempt to control the size of the
% % machine. The number of the states in the machine is directly dependent
% % on the active features. We propose to reign in the size using recent
% % the developments in structured sparsity. If we want to consider
% % arbitrary context, we have an infinite set of features. Nevertheless,
% % these features are nested. We can organized them into a DAG called a
% % Hasse diagram. We can deal with the infinite number by ensuring that
% % only a finite number are active at any time. For a finite set of
% % active features, there exists a minimal machine such that no two arcs
% % have the same set of active features.  The procedure works roughly as
% % follows:
% % \begin{itemize}
% %   \item We start with no features in the model
% %   \item we construct a DAG out of our features
% %   \item LOOP
% %   \item We activate all features all of whose parents are active (or
% % have no parents)
% %   \item We construct the minimal finite state machine such that no two
% % arcs have the same set of active features
% %   \item We optimize with group lasso
% %   \item deactive all features whose weights did not budge from 0
% % \end{itemize}


% % \section{The Group Lasso} We divide our set of features $\mathcal{F}$
% % into $M$ groups $G_1,\ldots,G_M$ where each $G_m \subset
% % \{1,\ldots,D\}$.  Define $\theta_m$ to be the subvector of those
% % weights that correspond to the features in the $m$-th group and let
% % $d_1,\ldots,d_m$ be nonnegative scalars. The following is the
% % \emph{group-Lasso} regularizer:
% % $$
% % \Omega_d^{GL} = \sum_{m-1}^M d_m||\theta_m||_2
% % $$.
% \section{Implementation} The implementation was done using OpenFST
% \cite{allauzen2007openfst} \ryan{how should one better incorporate
% this into the paper?}

% \section{Phonological Features} Here we give a brief explanation of
% phonological features and show how to organize them into a Hasse
% diagram.

% % \section{Backing Off} failure arcs \cite{allauzen2003generalized}

% \section{Experiments} We performs experiments on three tasks 1)
% grapheme to phoneme conversion, 2) transliteration and 3) pair-wise
% morphological conversion. We use the CELEX data set for G2P and
% pair-wise morphological transudction. We use datafrom NEWS 2009 Shared
% Task for the transliteration.

% phonology?  comparison to openfst ngram


% \section{Evaluation}

% % There are three reasonable metrics to minimize: % 1. cross-entropy =
% - ∑i log p(yi | xi) % 2. edit dist of decode = ∑i d(y*i, yi) where d
% is unweighted Levenshtein distance and y*i is a Viterbi decode or MBR
% decode given xi % 3. expected edit dist = ∑i ∑y p(y | xi) d(y, yi) We
% evaluate our model on three metrics 1) cross-entropy $\sum_i
% \log(p(y_i| x_i)$ 2) edit distance of the one-best decoding
% \ryan{k-best???} $\sum_i d(y_i^*,y_i)$ where $d$ is the unweighted
% Levenshtein distance and $y_i^*$ is the result from Viterbi decoding
% 3) expected edit distance, which we define was $\sum_i \sum_y p(y |
% x_i) d(y,y_i)$.

\bibliographystyle{acl} 
\bibliography{context_edit_dist}

\end{document}
