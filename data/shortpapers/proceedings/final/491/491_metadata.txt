SubmissionNumber#=%=#491
FinalPaperTitle#=%=#Tailoring Continuous Word Representations for Dependency Parsing
ShortPaperTitle#=%=#Tailoring Continuous Word Representations for Dependency Parsing
NumberOfPages#=%=#7
CopyrightSigned#=%=#MOHIT BANSAL
JobTitle#==#Research Assistant Professor
Organization#==#Toyota Technological Institute at Chicago
Abstract#==#Word representations have proven useful for many NLP tasks, e.g., Brown
clusters as features in dependency parsing (Koo et al., 2008). In this paper,
we investigate the use of continuous word representations as features for
dependency parsing. We compare several popular embeddings to Brown clusters,
via multiple types of features, in both news and web domains. We find that all
embeddings yield significant parsing gains, including some recent ones that can
be trained in a fraction of the time of others. Explicitly tailoring the
representations for the task leads to further improvements. Moreover, an
ensemble of all representations achieves the best results, suggesting
their complementarity.
Author{1}{Firstname}#=%=#Mohit
Author{1}{Lastname}#=%=#Bansal
Author{1}{Email}#=%=#mbansal@ttic.edu
Author{1}{Affiliation}#=%=#Toyota Technological Institute at Chicago
Author{2}{Firstname}#=%=#Kevin
Author{2}{Lastname}#=%=#Gimpel
Author{2}{Email}#=%=#kgimpel@ttic.edu
Author{2}{Affiliation}#=%=#Toyota Technological Institute at Chicago
Author{3}{Firstname}#=%=#Karen
Author{3}{Lastname}#=%=#Livescu
Author{3}{Email}#=%=#klivescu@ttic.edu
Author{3}{Affiliation}#=%=#TTI-Chicago

==========