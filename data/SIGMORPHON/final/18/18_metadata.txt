SubmissionNumber#=%=#18
FinalPaperTitle#=%=#One-Size-Fits-All Multilingual Models
ShortPaperTitle#=%=#
NumberOfPages#=%=#7
CopyrightSigned#=%=#Ben Peters
JobTitle#==#
Organization#==#Instituto de Telecomunicações, Av. Rovisco Pais 1, 1049-001 Lisboa, Portugal
Abstract#==#This paper presents DeepSPIN's submissions to Tasks 0 and 1 of the SIGMORPHON 2020 Shared Task. For both tasks, we present multilingual models, training jointly on data in all languages. We perform no language-specific hyperparameter tuning -- each of our submissions uses the same model for all languages. Our basic architecture is the sparse sequence-to-sequence model with entmax attention and loss, which allows our models to learn sparse, local alignments while still being trainable with gradient-based techniques. For Task 1, we achieve strong performance with both RNN- and transformer-based sparse models. For Task 0, we extend our RNN-based model to a multi-encoder set-up in which separate modules encode the lemma and inflection sequences. Despite our models' lack of language-specific tuning, they tie for first in Task 0 and place third in Task 1.
Author{1}{Firstname}#=%=#Ben
Author{1}{Lastname}#=%=#Peters
Author{1}{Username}#=%=#bpop
Author{1}{Email}#=%=#benzurdopeters@gmail.com
Author{1}{Affiliation}#=%=#Instituto de Telecomunicações
Author{2}{Firstname}#=%=#André F. T.
Author{2}{Lastname}#=%=#Martins
Author{2}{Username}#=%=#afm
Author{2}{Email}#=%=#andre.t.martins@tecnico.ulisboa.pt
Author{2}{Affiliation}#=%=#Unbabel, Instituto de Telecomunicacoes

==========