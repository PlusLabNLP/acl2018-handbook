SubmissionNumber#=%=#17
FinalPaperTitle#=%=#Similarity-Based Non-Scorable Response Detection for Automated Speech Scoring
ShortPaperTitle#=%=#Similarity-Based Non-Scorable Response Detection for Automated Speech Scoring
NumberOfPages#=%=#8
CopyrightSigned#=%=#Su-Youn Yoon
JobTitle#==#
Organization#==#Educational Testing Service
660 Rosedale Road, Princeton, NJ
Abstract#==#This study provides a method that identifies problematic responses which make
automated speech scoring difficult. When automated scoring is used in the
context of a high stakes language proficiency assessment, for which the scores
are used to make consequential decisions, some test takers may have an
incentive to try to game the system in order to artificially inflate their
scores. Since many automated proficiency scoring systems use fluency features
such as speaking rate as one of the important features, students may engage in
strategies designed to manipulate their speaking rate as measured by the
system.

In order to address this issue, we developed a method which filters out
non-scorable responses based on text similarity measures. Given a test
response, the method generated a set of features which calculated the topic
similarity with the prompt question or the sample responses including relevant
content. Next, an automated filter which identified these problematic responses
was implemented using the similarity features. This filter improved the
performance of the baseline filter in identifying responses with topic
problems.
Author{1}{Firstname}#=%=#Su-Youn
Author{1}{Lastname}#=%=#Yoon
Author{1}{Email}#=%=#syoon9@gmail.com
Author{1}{Affiliation}#=%=#Educational Testing Service
Author{2}{Firstname}#=%=#Shasha
Author{2}{Lastname}#=%=#Xie
Author{2}{Email}#=%=#shxie@microsoft.com
Author{2}{Affiliation}#=%=#Microsoft

==========