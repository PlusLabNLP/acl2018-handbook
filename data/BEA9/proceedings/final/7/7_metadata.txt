SubmissionNumber#=%=#7
FinalPaperTitle#=%=#Automatic evaluation of spoken summaries: the case of language assessment
ShortPaperTitle#=%=#Automatic evaluation of spoken summaries
NumberOfPages#=%=#11
CopyrightSigned#=%=#Anastassia Loukina
JobTitle#==#
Organization#==#
Abstract#==#This paper investigates whether ROUGE, a popular metric for the evaluation of
automated written summaries, can be applied to the assessment of spoken
summaries produced by non-native speakers of English. We demonstrate that
ROUGE, with its emphasis on the recall of information, is particularly suited
to the assessment of the summarization quality of non-native speakers'
responses. A standard baseline implementation of ROUGE-1 computed over the
output of the automated speech recognizer has a Spearman correlation of 
rho = 0.55 with experts' scores of speakers’ proficiency  rho = 0.51 for a
content-vector baseline). Further increases in agreement with experts’ scores
can be achieved by using types instead of tokens for the computation of word
frequencies for both candidate and reference summaries, as well as by using
multiple reference summaries instead of a single one. These modifications
increase the correlation with experts’ scores to a Spearman correlation of
rho = 0.65. Furthermore, we found that the choice of reference summaries does
not
have any impact on performance, and that the adjusted metric is also robust to
errors introduced by automated speech recognition (rho = 0.67 for human
transcriptions vs. rho = 0.65 for speech recognition output).
Author{1}{Firstname}#=%=#Anastassia
Author{1}{Lastname}#=%=#Loukina
Author{1}{Email}#=%=#aloukina@ets.org
Author{1}{Affiliation}#=%=#Educational Testing Service
Author{2}{Firstname}#=%=#Klaus
Author{2}{Lastname}#=%=#Zechner
Author{2}{Email}#=%=#kzechner@ets.org
Author{2}{Affiliation}#=%=#Educational Testing Service
Author{3}{Firstname}#=%=#Lei
Author{3}{Lastname}#=%=#Chen
Author{3}{Email}#=%=#lchen@ets.org
Author{3}{Affiliation}#=%=#Educational Testing Service

==========