SubmissionNumber#=%=#19
FinalPaperTitle#=%=#Automatic Assessment of the Speech of Young English Learners
ShortPaperTitle#=%=#Automatic Assessment of the Speech of Young English Learners
NumberOfPages#=%=#10
CopyrightSigned#=%=#Jian Cheng
JobTitle#==#
Organization#==#Knowledge Technologies, Pearson, 4040 Campbell Ave., Menlo Park, California 94025, USA
Abstract#==#This paper introduces some of the research behind automatic scoring of the
speaking part of the Arizona English Language Learner Assessment, a large-scale
test now operational for students in Arizona.  Approximately 70% of the
students tested are in the range 4-11 years old. We cover the methods used to
assess spoken responses automatically, considering both what the student says
and the way in which the student speaks.  We also provide evidence for the
validity of machine scores.  The assessments include 10 open-ended item types. 
For 9 of the 10 open item types, machine scoring performed at a similar level
or better than human scoring at the item-type level.  At the participant level,
correlation coefficients between machine overall scores and average human
overall scores were:  Kindergarten: 0.88; Grades 1-2: 0.90; Grades 3-5: 0.94;
Grades 6-8: 0.95; Grades 9-12: 0.93.  The average correlation coefficient was
0.92.  We include a note on implementing a detector to catch problematic test
performances.
Author{1}{Firstname}#=%=#Jian
Author{1}{Lastname}#=%=#Cheng
Author{1}{Email}#=%=#jian.cheng@pearson.com
Author{1}{Affiliation}#=%=#Pearson
Author{2}{Firstname}#=%=#Yuan
Author{2}{Lastname}#=%=#Zhao D'Antilio
Author{2}{Email}#=%=#yuan.dantilio@pearson.com
Author{2}{Affiliation}#=%=#Pearson
Author{3}{Firstname}#=%=#Xin
Author{3}{Lastname}#=%=#Chen
Author{3}{Email}#=%=#xin.chen@pearson.com
Author{3}{Affiliation}#=%=#Pearson
Author{4}{Firstname}#=%=#Jared
Author{4}{Lastname}#=%=#Bernstein
Author{4}{Email}#=%=#jared@erosenfeld.com
Author{4}{Affiliation}#=%=#Tasso Partners LLC

==========