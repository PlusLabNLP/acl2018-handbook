SubmissionNumber#=%=#15
FinalPaperTitle#=%=#[TACL] Parallel Algorithms for Unsupervised Tagging
ShortPaperTitle#=%=#Parallel Algorithms for Unsupervised Tagging
NumberOfPages#=%=#14
CopyrightSigned#=%=#
JobTitle#==#
Organization#==#
Abstract#==#We propose a new method for unsupervised tagging that finds minimal models,
that are further improved by Expectation Minimization training. In contrast to
previous approaches that rely on manually specified and multi-step heuristics
for model minimization, our approach is a simple greedy approximation algorithm
DMLC (Distributed Minimum Label Cover) that solves this objective in a single
step.

We extend the method and show how to efficiently parallelize the algorithm on
modern parallel computing platforms while preserving approximation guarantees. 
The new method easily scales to large data and grammar sizes, overcoming the
memory bottleneck in previous approaches. We demonstrate the power of the new
algorithm by evaluating on various sequence labeling tasks---Part-of-Speech
tagging for multiple languages (including low-resource languages), with
complete and incomplete dictionaries, and supertagging, a complex sequence
labeling task, where the grammar size alone can grow to millions of entries.
Our results show that for all of these settings, our method achieves
state-of-the-art scalable performance that yields high quality tagging outputs.
Author{1}{Firstname}#=%=#Sujith
Author{1}{Lastname}#=%=#Ravi
Author{1}{Email}#=%=#sravi@google.com
Author{1}{Affiliation}#=%=#Google
Author{2}{Firstname}#=%=#Sergei
Author{2}{Lastname}#=%=#Vassilivitskii
Author{2}{Email}#=%=#sergeiv@google.com
Author{2}{Affiliation}#=%=#Google
Author{3}{Firstname}#=%=#Vibhor
Author{3}{Lastname}#=%=#Rastogi
Author{3}{Email}#=%=#vibhor.rastogi@gmail.com
Author{3}{Affiliation}#=%=#Twitter

==========
