SubmissionNumber#=%=#9
FinalPaperTitle#=%=#Combining Subword Representations into Word-level Representations in the Transformer Architecture
ShortPaperTitle#=%=#
NumberOfPages#=%=#6
CopyrightSigned#=%=#Noe Casas
JobTitle#==#
Organization#==#Universitat Politècnica de Catalunya. Carrer de Jordi Girona 1-3 . 08034 Barcelona. Spain
Abstract#==#In Neural Machine Translation, using word-level tokens leads to degradation in translation quality. The dominant approaches use subword-level tokens, but this increases the length of the sequences and makes it difficult to profit from word-level information such as POS tags or semantic dependencies.

We propose a modification to the Transformer model to combine subword-level representations into word-level ones in the first layers of the encoder, reducing the effective length of the sequences in the following layers and providing a natural point to incorporate extra word-level information.

Our experiments show that this approach maintains the translation quality with respect to the normal Transformer model when no extra word-level information is injected and that it is superior to the currently dominant method for incorporating word-level source language information to models based on subword-level vocabularies.
Author{1}{Firstname}#=%=#Noe
Author{1}{Lastname}#=%=#Casas
Author{1}{Username}#=%=#noe
Author{1}{Email}#=%=#noe.casas@upc.edu
Author{1}{Affiliation}#=%=#Polytechnic University of Catalonia
Author{2}{Firstname}#=%=#Marta R.
Author{2}{Lastname}#=%=#Costa-jussà
Author{2}{Username}#=%=#marta.ruiz
Author{2}{Email}#=%=#marta.ruiz@upc.edu
Author{2}{Affiliation}#=%=#Universitat Politècnica de Catalunya
Author{3}{Firstname}#=%=#José A. R.
Author{3}{Lastname}#=%=#Fonollosa
Author{3}{Username}#=%=#jose.fonollosa
Author{3}{Email}#=%=#jose.fonollosa@upc.edu
Author{3}{Affiliation}#=%=#Universitat Politècnica de Catalunya

==========