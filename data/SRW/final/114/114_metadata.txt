SubmissionNumber#=%=#114
FinalPaperTitle#=%=#Inducing Grammar from Long Short-Term Memory Networks by Shapley Decomposition
ShortPaperTitle#=%=#
NumberOfPages#=%=#7
CopyrightSigned#=%=#Yuhui Zhang
JobTitle#==#
Organization#==#Stanford University, Stanford, CA 94305
Abstract#==#The principle of compositionality has deep roots in linguistics: the meaning of an expression is determined by its structure and the meanings of its constituents. However, modern neural network models such as long short-term memory network process expressions in a linear fashion and do not seem to incorporate more complex compositional patterns. In this work, we show that we can explicitly induce grammar by tracing the computational process of a long short-term memory network. We show: (i) the multiplicative nature of long short-term memory network allows complex interaction beyond sequential linear combination; (ii) we can generate compositional trees from the network without external linguistic knowledge; (iii) we evaluate the syntactic difference between the generated trees, randomly generated trees and gold reference trees produced by constituency parsers; (iv) we evaluate whether the generated trees contain the rich semantic information.
Author{1}{Firstname}#=%=#Yuhui
Author{1}{Lastname}#=%=#Zhang
Author{1}{Username}#=%=#yuhuiz
Author{1}{Email}#=%=#yuhuiz@stanford.edu
Author{1}{Affiliation}#=%=#Stanford University
Author{2}{Firstname}#=%=#Allen
Author{2}{Lastname}#=%=#Nie
Author{2}{Username}#=%=#nlsdnm
Author{2}{Email}#=%=#anie@stanford.edu
Author{2}{Affiliation}#=%=#Stanford University

==========