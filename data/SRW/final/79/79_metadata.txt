SubmissionNumber#=%=#79
FinalPaperTitle#=%=#Transferring Monolingual Model to Low-Resource Language: The Case of Tigrinya
ShortPaperTitle#=%=#
NumberOfPages#=%=#
CopyrightSigned#=%=#
JobTitle#==#
Organization#==#
Abstract#==#In recent years, transformer models have achieved great success in natural language processing tasks. Most of the current state-of-the-art NLP results are achieved by using monolingual transformer models, where the model is pre-trained using a single language unlabelled text corpus. Then, the model is fine-tuned to the specific downstream task. However, the cost of pre-training a new transformer model is high for most languages. In this work, we propose a novel transfer learning method to adopt a strong source language model, trained from a large monolingual corpus to a low-resource language. Thus, using XLNet language model, we demonstrate competitive performance with mBERT and a pre-trained target language model on the Cross-lingual Sentiment (CLS) dataset and on a new sentiment analysis dataset for low-resourced language Tigrinya. With only 10k examples of the given Tigrinya sentiment analysis dataset, English XLNet has achieved 78.88% F1-Score outperforming BERT and mBERT by 10% and 7%, respectively. More interestingly, fine-tuning (English) XLNet model on the CLS dataset has promising results compared to mBERT and even outperformed mBERT for one dataset of the Japanese language.
Author{1}{Firstname}#=%=#Abrhalei Frezghi
Author{1}{Lastname}#=%=#Tela
Author{2}{Firstname}#=%=#Abraham Woubie
Author{2}{Lastname}#=%=#Zewoudie
Author{3}{Firstname}#=%=#Ville
Author{3}{Lastname}#=%=#Hautam√§ki

==========