Understanding and identifying humor has been increasingly popular, as seen by the number of datasets created to study humor.  However, one area of humor research, humor generation, has remained a difficult task, with machine generated jokes failing to match human-created humor.  As many humor prediction datasets claim to aid in generative tasks, we examine whether these claims are true.  We focus our experiments on the most popular dataset, included in the 2020 SemEval's Task 7, and teach our model to take normal text and ``translate'' it into humorous text.  We evaluate our model compared to humorous human generated headlines, finding that our model is preferred equally in A/B testing with the human edited versions, a strong success for humor generation, and is preferred over an intelligent random baseline 72\% of the time.  We also show that our model is assumed to be human written comparable with that of the human edited headlines and is significantly better than random, indicating that this dataset does indeed provide potential for future humor generation systems.
