Generative training of energy-based models has recently achieved promising results in image modeling tasks. Their success is due to efficient gradient-based MCMC sampling. However, gradient-based inference has not yet successfully applied to high-dimensional discrete tasks such as machine translation. Nevertheless, we can still benefit from the power of energy-based models to improve the performance of autoregressive neural machine translation (NMT). Building on \citeauthor{grover\&al19}{\textasciitilde}(\citeyear{grover\&al19}) and \citeauthor{deng\&al20}{\textasciitilde}(\citeyear{deng\&al20}) we introduce energy-reweighted NMT (ERNMT) as a joint model combining an autoregressive NMT and an energy-based model that is trained to discriminate samples of the autoregressive NMT from the data distribution. The energy model re-weighs the samples from the autoregressive model such that the joint model is a better fit for the data distribution. The energy model is defined over the sentences of the target language; thus it can be shared among all language pairs that have a common target language to create a multi-source ERNMT. Our multi-source ERNMT consistently outperforms Transformer-based autoregressive NMT on several translation tasks. In particular, it improves the Sinhala-English translation task by +3 BLEU points. This is a submission to the extended abstract track.
