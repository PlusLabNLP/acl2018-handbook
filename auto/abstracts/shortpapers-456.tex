How might a parser benefit from real-valued word embeddings? We isolate three ways in which word embeddings could be used to extend a state-of-the-art statistical parser: by relating out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon. On small training sets, we show small but significant gains over the baseline parser; as the training data grows, we find that these gains diminish. Our results support a hypothesis that word embeddings import syntactic information that is useful but redundant with (rather than complementary to) distinctions learned from larger treebanks in other ways.
