Transformer-based models have made great progress in open-domain natural language generation. These models are commonly fine-tuned for controlled text generation, e.g., for generating domain-specific text. Dathathri et al. (2019) recently proposed the Plug and Play Language Model (PPLM) which allows standard transformer-based models to perform controlled text generation with the help of simple topic bag-of-words classifier. However, it turns out that text generated in this fashion does not use all of the words in the bag-of-words with equal frequency. Some words from the topic are strongly over-represented in the generated text, while others rarely or never get generated. In this work, we analyze the lexical diversity of texts generated by PPLM for four different topic models and show that the generated topic word frequencies are disproportionately skewed toward common words. We then propose and compare three different methods for encouraging PPLM to generate a greater diversity of topic words. We show that these approaches are effective in alleviating the imbalance issue. This is a submission to the extended abstract track.
