we propose new automatic evaluation metric for machine translation. Our metric is based on chunk between reference and candidate translation. Moreover, we apply a prize based on sentence-length to the metric, not like a penalty in BLEU and NIST. We call this metric Automatic Evaluation of Machine Translation that Prize is Applied to Chunk-based metric (APAC). Through meta-evaluation experiments, we confirmed that our metric is obtained the stable correlation with human judgment among some metrics.
