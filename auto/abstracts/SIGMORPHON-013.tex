This paper  investigates  the  ability  of  neural  network  architectures  to  effectively  learn diachronic  phonological  generalizations  in  amultilingual  setting.   We  employ  models  using three  different types  of language embedding  (dense,  sigmoid,  and  straight-through). We find that the Straight-Through model out-performs the other two in terms of accuracy, but the Sigmoid model's language embeddings show the strongest agreement with the traditional  subgrouping  of  the  Slavic  languages. We find that the Straight-Through model has learned coherent,  semi-interpretable information  about  sound  change,  and  outline  directions for future research.
