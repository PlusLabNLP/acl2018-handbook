We introduce an iterative text refinement model to reduce the decoding space of non-autoregressive models by disentangling the token prediction and relative position prediction. Our model not only allows for more flexible text generation and refinement but can be directly applied to the lexically constrained text generation task.
