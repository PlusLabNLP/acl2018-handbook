The impressive results achieved in recent years by neural machine translation comes together with high computational and memory requirements, both of which could hinder its adoption, for instance, in mobile computing environments. In this paper we focus on the memory requirement of neural machine translation and investigate how various strategies of parameter sharing within a network impact translation quality and memory requirements. We consider two classes of models: RNN-- and transformer--based systems. We progressively tie more parameters and observe the change in the overall translation quality. We conduct our investigation using four bilingual (En\$\leftrightarrow\$De and En\$\leftrightarrow\$Fi) and one multilingual systems. With the RNN-based models, we were often able to halve the number of parameters while losing only 1 BLEU. We however observed significant drop in translation quality with the transformer-based models. This is a submission to the extended abstract track.
