This paper introduces some of the research behind automatic scoring of the speaking part of the Arizona English Language Learner Assessment, a large-scale test now operational for students in Arizona.  Approximately 70\% of the students tested are in the range 4-11 years old. We cover the methods used to assess spoken responses automatically, considering both what the student says and the way in which the student speaks.  We also provide evidence for the validity of machine scores.  The assessments include 10 open-ended item types. For 9 of the 10 open item types, machine scoring performed at a similar level or better than human scoring at the item-type level.  At the participant level, correlation coefficients between machine overall scores and average human overall scores were:  Kindergarten: 0.88; Grades 1-2: 0.90; Grades 3-5: 0.94; Grades 6-8: 0.95; Grades 9-12: 0.93.  The average correlation coefficient was 0.92.  We include a note on implementing a detector to catch problematic test performances.
