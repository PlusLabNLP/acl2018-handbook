Recently, researchers have begun exploring methods of scoring student essays with respect to particular dimensions of quality such as coherence, technical errors, and prompt adherence. The work on modeling prompt adherence, however, has been focused on whether individual sentences adhere to the prompt. We present a new annotated corpus of essay-level prompt adherence scores and propose a learning-based approach to scoring essays along the prompt adherence dimension. Our approach makes use of novel features and significantly outperforms a baseline prompt adherence scoring system yielding relative error reductions of up to 21.4\%.
