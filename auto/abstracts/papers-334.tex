We aim to improve spoken term detection performance by incorporating contextual information beyond traditional N-gram language models.              Instead of taking a broad view of topic context in spoken documents, variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents.              We show that given the detection of one instance of a term we are more likely to detect additional instances of that term in the same document. We leverage term 'burstiness' for each keyword by taking the most confident hypothesis in each document and interpolating with lower scoring hits.              We then develop a principled approach to select interpolation weights using only the ASR training data.  Using our re-weighting approach we demonstrate up to 1.5\% absolute performance improvement on the term detection task in all Babel program languages.
