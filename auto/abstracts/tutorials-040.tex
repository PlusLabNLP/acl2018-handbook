Natural language process (NLP) has undergone tremendous changes in recent years. In the past two years, breakthroughs have been happening at an unprecedented pace.  However, while the performance of the NLP models gets better, models and techniques become more complex. As a result, these new developments have become less accessible to practitioners, including software engineers and data scientists, to understand and interpret. This tutorial attempts to review the recent developments in NLP models and make them accessible to practitioners. At the center of the recent developments in NLP is the transformer architecture. The BERT (Bidirectional Encoder Representations from Transformers) paper published in late 2018 has spurred a wave of transformer-based and pre-train-and-fine-tuning NLP techniques, such as XLNet, RoBERTa, among others. These new techniques have set the new state-of-the-art (SOTA) performance in many NLP tasks. In this tutorial, we will have a close study of these transformer-based approaches and apply them in a few standard NLP tasks and across multiple languages.  We will use the open source repository (https://github.com/microsoft/nlp) to illustrate the use of transformers across different scenarios and languages. Special attention will be paid to applying these advances to non-English (and non left-to-right) languages.
