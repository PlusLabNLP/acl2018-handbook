Self-supervised deep learning (SSDL) methods emerge as a promising learning paradigm in the field of Computer Vision recently. The approach cleverly formulates supervised learning problems using dense learning signals, without the need of external human annotations. Beyond Vision, it is a general framework that enables a variety of learning models, including deep reinforcement learning and the success of AlphaGo Zero. In NLP, SSDL has also achieved promising results in representation learning, including masked language models, such as BERT and XLNet. In this tutorial, we provide a gentle introduction to the foundation of self-supervised deep learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in self-supervised deep learning for NLP, with a special focus on generation and language models. We provide an overview of the research area, categorize different types of self-supervised learning models, and discuss pros and cons, aiming to provide some interpretations and practical perspectives on the future of self-supervised learning for solving real-world NLP problems.
