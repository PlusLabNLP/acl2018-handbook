We present an approach for automatically learning to solve algebra word problems given only text.  Successfully solving these problems requires handling complex semantic relations which cross sentences, such as determining that the distinct entities mentioned in various sentences are all a part of the same set.  Our algorithm first determines the relation between entities in the text, and then translates these into a system of equations.  The algorithm is trained in a weak supervision setting, where we are only given the text and the resulting numerical answers, without information on the entity relations or the underlying equations.  Despite this relatively weak supervision, we are able to successfully generate the correct answer set for 77\% of the problems on a dataset of actual grade-school algebra problems, compared to the 26\% produced by a baseline which is agnostic to semantic relations.
