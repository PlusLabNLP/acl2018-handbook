The ability of children to generalize over the linguistic input they receive is key to acquiring productive knowledge of verbs.  Such generalizations help children extend their learned knowledge of constructions to a novel verb, and use it appropriately in syntactic patterns previously unobserved for that verb---a key factor in language productivity.  Computational models can help shed light on the gradual development of more abstract knowledge during verb acquisition. We present an incremental Bayesian model that simultaneously and incrementally learns argument structure constructions and verb classes given naturalistic language input.  We show how the distributional properties in the input language influence the formation of generalizations over the constructions and classes.
