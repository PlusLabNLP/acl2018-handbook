We present novel automatic metrics for machine translation evaluation, which use discourse structure and convolution kernels to compare the discourse tree of an automatic translation with that of the human reference. We experiment with five different transformations and augmentations of a base discourse tree representation based on the rhetorical structure theory, and we combine the kernel scores for each of them into a single score. Finally, we add to the combination other metrics from the Asiya MT evaluation toolkit, and we tune the weights of the combination on actual human judgments. Experiments on the WMT12 and WMT13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved, both at the segment and at the system level.
