A key challenge for computational models of conversation is to discover latent structure in dialogue. This is particularly true for task-oriented dialogues, where the latent structure provides a basis for analysing, evaluating, and building conversational systems. In this paper, we propose three new unsupervised dialogue models to discover latent structures in task-oriented dialogues. Our methods synthesize hidden Markov models (for underlying state) and topic models (to connect words to states). We apply our models to two corpora of real, non-trivial dialogues: human-computer spoken dialogues in the bus timetable domain, and human-human text-based web chats from a live technical support service. Qualitatively, we show that our models extract meaningful state representations and dialogue structures consistent with human annotations. Quantitatively, we evaluate using log likelihood and an ordering task on a held-out test set. We show that our models advance the state-of-the-art on both metrics, on both datasets.
