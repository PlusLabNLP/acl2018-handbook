
\begin{tutorial}
  {}
  {tutorials-023}
  % {\daydateyear, \tutorialtimeb \\\tutorialtimezone}
  {}
  {\TutLocD}
  {\TutLevelD}
\end{tutorial}


\tutorialabstract{}{}{}{}{tutorials-023}

\vspace{2ex}\centerline{\rule{.5\linewidth}{.5pt}}\vspace{2ex}
\setlength{\parskip}{1ex}\setlength{\parindent}{0ex}

{\small
{\bfseries Lili Mou}

{\tt doublepower.mou@gmail.com}

{\tt https://lili-mou.github.io}

Dr. Lili Mou is an Assistant Professor at the
Department of Computing Science, University of
Alberta. He is also an Amii Fellow and a Canadian CIFAR AI Chair. Lili received his BS and
PhD degrees in 2012 and 2017, respectively, from
School of EECS, Peking University. After that,
he worked as a postdoctoral fellow at the University of Waterloo and a research scientist at Adeptmind (a startup in Toronto, Canada). His research interests include deep learning applied to natural
language processing as well as programming language processing. Recently, he has been focusing
more on text generation, from both continuous latent space and discrete word space. He has more
than 30 papers published at top-tier conferences
and journals, including AAAI, ACL, CIKM, COLING, EMNLP, ICASSP, ICLR, ICML, IJCAI, INTERSPEECH, NAACL-HLT, and TACL. He presented a tutorial “Discreteness in Neural Natural
Language Processing” at EMNLP-IJCNLP’19.

{\bfseries Olga Vechtomova}

{\tt ovechtomova@uwaterloo.ca}

{\tt https://ov-research.uwaterloo.ca}

Dr. Olga Vechtomova is an Associate Professor
in the Department of Management Sciences, Faculty of Engineering, cross-appointed in the School
of Computer Science at the University of Waterloo. Olga leads the Natural Language Processing
Lab, affiliated with the Waterloo.AI Institute. Her
research has been supported by a number of industry and government grants, including Amazon Research Award and Natural Sciences and Engineering Research Council (NSERC). The research in
her Lab is mainly focused on designing deep neural networks for natural language generation tasks.
Her current and recent projects include controlled
text generation, text style transfer, and designing
text generative models for creative applications.
She has over 50 publications in NLP and Information Retrieval conferences and journals, including
NAACL-HLT, COLING, ACL, ACM SIGIR, and
CIKM. She and her colleagues recently received
the ACM SIGIR 2019 Test of Time Award.

}