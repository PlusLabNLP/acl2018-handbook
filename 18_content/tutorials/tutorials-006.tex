\begin{tutorial}
  {Neural Semantic Parsing}
  {tutorial-028}
  {\daydateyear, \tutorialafternoontime}
  {\TutLocF}
\end{tutorial} 

Semantic parsing, the study of translating natural language utterances into machine-executable programs, is a well-established research area and has applications in question answering, instruction following, voice assistants, and code generation. In the last two years, the models used for semantic parsing have changed dramatically with the introduction of neural encoder-decoder methods that allow us to rethink many of the previous assumptions underlying semantic parsing. We aim to inform those already interested in semantic parsing research of these new developments in the field, as well as introduce the topic as an exciting research area to those who are unfamiliar with it.

Current approaches for neural semantic parsing share several similarities with neural machine translation, but the key difference between the two fields is that semantic parsing translates natural language into a formal language, while machine translation translates it into a different natural language. The formal language used in semantic parsing allows for constrained decoding, where the model is constrained to only produce outputs that are valid formal statements. We will describe the various approaches researchers have taken to do this. We will also discuss the choice of formal languages used by semantic parsers, and describe why much recent work has chosen to use standard programming languages instead of more linguistically-motivated representations. We will then describe a particularly challenging setting for semantic parsing, where there is additional context or interaction that the parser must take into account when translating natural language to formal language, and give an overview of recent work in this direction. Finally, we will introduce some tools available in AllenNLP for doing semantic parsing research.

\vspace{2ex}\centerline{\rule{.5\linewidth}{.5pt}}\vspace{2ex}
\setlength{\parskip}{1ex}\setlength{\parindent}{0ex}


  {\bfseries Matt Gardner} is a research scientist at the Allen
Institute for Artificial Intelligence. His research
focuses on question answering and semantic parsing.
He is the lead maintainer of the AllenNLP
toolkit and a host of the NLP Highlights podcast.
 
  {\bfseries Pradeep Dasigi} is a PhD student at the Language
Technologies Institute in Carnegie Mellon
University. His research interest lies in building
knowledge-aware language understanding systems,
with a recent focus on neural semantic parsing.

  {\bfseries Srinivasan Iyer} is a graduate student in the Natural
Language Processing group at the University
of Washington, Seattle. His main research area is
context dependent semantic parsing directly from
natural language to general purpose programming
source code. Other aspects of his research are
learning semantic parsers from massive online resources
and incorporating user feedback for model
improvement.

  {\bfseries Alane Suhr} is a PhD student in Computer Science
at Cornell University. Alane’s research interests
include developing machine learning methods for
understanding natural language grounded in complex
environments and interactions. She is a recipient
of an NSF Graduate Research Fellowship, the
Best Resource Paper award at ACL 2017, and an
Outstanding Paper Award at NAACL 2018.

  {\bfseries Luke Zettlemoyer} is an Associate
Professor in the Paul G. Allen School of
Computer Science \& Engineering at the University
of Washington. He has a been doing research
in semantic parsing for many years, and recently
shifted to studying neural models for this problem.
Luke’s honors include multiple best paper awards,
a PECASE award, and an Allen Distinguished Investigator
award.




