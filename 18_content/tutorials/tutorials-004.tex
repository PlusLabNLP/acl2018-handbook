\begin{tutorial}
  {Connecting Language and Vision to Actions}
  {tutorial-020}
  {\daydateyear, \tutorialmorningtime}
  {\TutLocD}
\end{tutorial}


A long-term goal of AI research is to build intelligent agents that can see the
rich visual environment around us, communicate this understanding in natural
language to humans and other agents, and act in a physical or embodied
environment. To this end, recent advances at the intersection of language and
vision have made incredible progress -- from being able to generate natural
language descriptions of images/videos, to answering questions about them, to
even holding free-form conversations about visual content! However, while these
agents can passively describe images or answer (a sequence of) questions about
them, they cannot act in the world (what if I cannot answer a question from my
current view, or I am asked to move or manipulate something?). Thus, the
challenge now is to extend this progress in language and vision to embodied
agents that take actions and actively interact with their visual environments.
To reduce the entry barrier for new researchers, this tutorial will provide an
overview of the growing number of multimodal tasks and datasets that combine
textual and visual understanding. We will comprehensively review existing
state-of-the-art approaches to selected tasks such as image captioning, visual
question answering (VQA) and visual dialog, presenting the key architectural
building blocks (such as co-attention) and novel algorithms (such as
cooperative/adversarial games) used to train models for these tasks. We will
then discuss some of the current and upcoming challenges of combining language,
vision and actions, and introduce some recently-released interactive 3D
simulation environments designed for this purpose.


\vspace{2ex}\centerline{\rule{.5\linewidth}{.5pt}}\vspace{2ex}
\setlength{\parskip}{1ex}\setlength{\parindent}{0ex}



{\bfseries Peter Anderson} is a final year PhD candidate in
Computer Science at the Australian National University,
supervised by Dr Stephen Gould, and a researcher
within the Australian Centre for Robotic
Vision (ACRV). His PhD focuses on deep learning
for visual understanding in natural language.
He was an integral member of the team that won
first place in the 2017 Visual Question Answering
(VQA) challenge at CVPR, and he has made
several contributions in image captioning, including
achieving first place on the COCO leaderboard
in July 2017. He has published at CVPR,
ECCV, EMNLP and ICRA, and spent time at numerous
universities and research labs including
Adelaide University, Macquarie University, and
Microsoft Research. His research is currently
focused on vision-and-language understanding in
complex 3D environments.

{\bfseries Abhishek Das} is a Computer Science PhD student
at Georgia Institute of Technology, advised
by Dhruv Batra, and working closely with Devi
Parikh. He is interested in deep learning and its
applications in building agents that can see (computer
vision), think (reasoning and interpretability),
talk (language modeling) and act (reinforcement
learning). He is a recipient of an Adobe Research
Fellowship and a Snap Research Fellowship.
He has published at CVPR, ICCV, EMNLP,
HCOMP and CVIU, co-organized the NIPS 2017
workshop on Visually-Grounded Interaction and
Language, and has held visiting positions at Virginia
Tech, Queensland Brain Institute and Facebook
AI Research. He graduated from Indian
Institute of Technology Roorkee in 2015 with a
Bachelorâ€™s in Electrical Engineering.

{\bfseries Qi Wu} is a research fellow in the Australia
Centre for Robotic Vision (ACRV) in the University
of Adelaide. Before that, he was a postdoc
researcher in the Australia Centre for Visual Technologies
(ACVT) in the University of Adelaide.
He obtained his PhD degree in 2015 and MSc degree
in 2011, in Computer Science from University
of Bath, United Kingdom. His research interests
are mainly in Computer Vision and Machine
Learning. Currently, he is working on the vision to
language problem and he is especially an expert
in the area of Image Captioning and Visual Question
Answering (VQA). His attributes-based image
captioning model got first place on the COCO
Image Captioning Challenge Leader Board in the
October of 2015. He has published several papers
in prestigious conferences and journals, such as
TPAMI, CVPR, ICCV, ECCV, IJCAI and AAAI.


