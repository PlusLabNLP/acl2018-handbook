Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or can be attributed to individual neurons. We develop a simple unsupervised method to discover important neurons in NMT and show experimentally that translation quality depends on the discovered neurons. We visualize these neurons and find that many of them capture common linguistic phenomena. We also demonstrate how to control the behavior of NMT systems by modifying individual neurons, providing a powerful tool for analyzing and improving NMT. This is a submission to the extended abstract track
