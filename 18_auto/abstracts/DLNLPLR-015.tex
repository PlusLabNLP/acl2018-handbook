The deep learning approach to representing word meaning exploits statistically observable relationships between words. But when resources are too low to find these relationships, a neural network can do worse than fail; it can return harmful garbage. The main aim of this work is to explore this threshold of rareness. First, we  analyze frequency distributions in text and in common word similarity tasks used to evaluate meaning representations. Then, we pinpoint the benefits of the low resource tool SwordSS (Singh et al., 2016), which uses surface morphology to enhance neural embeddings.
